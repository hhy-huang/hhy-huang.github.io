<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>三层感知机-step by step | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="实现内容： 实现一个三层感知机 对手写数字数据集进行分类 绘制损失值变化曲线 完成kaggle MNIST手写数字分类任务，根据给定的超参数训练模型，完成表格的填写  实现数据集使用手写数字集。并且40%作测试集，60%做训练集。 12345678import matplotlib.pyplot as plt%matplotlib inlinefrom time import timeimport">
<meta property="og:type" content="article">
<meta property="og:title" content="三层感知机-step by step">
<meta property="og:url" content="http://example.com/2021/11/09/3-layer-MLP-md/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="实现内容： 实现一个三层感知机 对手写数字数据集进行分类 绘制损失值变化曲线 完成kaggle MNIST手写数字分类任务，根据给定的超参数训练模型，完成表格的填写  实现数据集使用手写数字集。并且40%作测试集，60%做训练集。 12345678import matplotlib.pyplot as plt%matplotlib inlinefrom time import timeimport">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\mathrm{loss}&space;=&space;-&space;\frac{1}{n}&space;\sum_n&space;\sum^{K}_{k=1}&space;y_k&space;\log{(\hat{y_k})}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\log{\frac{\exp{(O_i)}}{\sum_K&space;\exp{(O_k)}}}&space;&=&space;\log{\frac{\exp{(O_i&space;-&space;\mathrm{max}(O))}}{\sum_K&space;\exp{(O_k&space;-&space;\mathrm{max}(O))}}}\\&space;&=&space;O_i&space;-&space;\mathrm{max}(O)&space;-&space;\log{\sum_K&space;\exp{(O_k&space;-&space;\mathrm{max}(O))}}&space;\end{aligned}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?Z&space;=&space;XW_1&space;&plus;&space;b_1\\&space;H_1&space;=&space;\mathrm{ReLU}(Z)\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;W_2}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;W_2}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;W_2}\\&space;&&space;=&space;\frac{1}{n}&space;(\hat{y}&space;-&space;y)&space;\frac{\partial&space;O}{\partial&space;W_2}\\&space;&&space;=&space;\frac{1}{n}&space;[{H_1}^\mathrm{T}&space;(\hat{y}&space;-&space;y)]&space;\end{aligned}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;b_2}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;b_2}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;b_2}\\&space;&&space;=&space;\frac{1}{n}&space;(\hat{y}&space;-&space;y)&space;\frac{\partial&space;O}{\partial&space;b_2}\\&space;&&space;=&space;\frac{1}{n}&space;\sum^n_{i=1}&space;(\hat{y_i}&space;-&space;y_i)&space;\end{aligned}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;W_1}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;W_1}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;W_1}\\&space;&&space;=&space;\frac{1}{n}&space;{X}^\mathrm{T}&space;[(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;\frac{\partial&space;H_1}{\partial&space;Z}]\\&space;\end{aligned}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\frac{\partial&space;\mathrm{ReLU(x)}}{\partial&space;x}&space;=&space;\begin{cases}&space;0&space;&&space;\text{if&space;}&space;x&space;<&space;0\\&space;1&space;&&space;\text{if&space;}&space;x&space;\geq&space;0&space;\end{cases}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\frac{\partial&space;loss}{\partial&space;{W_1}_{ij}}&space;=&space;\begin{cases}&space;0&space;&&space;\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\&space;\frac{1}{n}&space;{X}^\mathrm{T}&space;(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;&&space;\text{if&space;}&space;{Z}_{ij}&space;\geq&space;0&space;\end{cases}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;b_1}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;b_1}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;b_1}\\&space;&&space;=&space;\frac{1}{n}&space;(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;\frac{\partial&space;H_1}{\partial&space;Z}\\&space;&&space;=&space;\begin{cases}&space;0&space;&\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\&space;\frac{1}{n}&space;\sum_n&space;(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;&\text{if&space;}&space;{Z}_{ij}&space;\geq&space;0&space;\end{cases}&space;\end{aligned}">
<meta property="article:published_time" content="2021-11-09T15:12:33.000Z">
<meta property="article:modified_time" content="2021-11-09T16:32:36.164Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://latex.codecogs.com/gif.latex?\mathrm{loss}&space;=&space;-&space;\frac{1}{n}&space;\sum_n&space;\sum^{K}_{k=1}&space;y_k&space;\log{(\hat{y_k})}">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-3-layer-MLP-md" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/09/3-layer-MLP-md/" class="article-date">
  <time datetime="2021-11-09T15:12:33.000Z" itemprop="datePublished">2021-11-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      三层感知机-step by step
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="实现内容："><a href="#实现内容：" class="headerlink" title="实现内容："></a>实现内容：</h2><ol>
<li>实现一个三层感知机</li>
<li>对手写数字数据集进行分类</li>
<li>绘制损失值变化曲线</li>
<li>完成kaggle MNIST手写数字分类任务，根据给定的超参数训练模型，完成表格的填写</li>
</ol>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>数据集使用手写数字集。并且40%作测试集，60%做训练集。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">trainX, testX, trainY, testY = train_test_split(load_digits()[<span class="string">&#x27;data&#x27;</span>], load_digits()[<span class="string">&#x27;target&#x27;</span>], test_size = <span class="number">0.4</span>, random_state = <span class="number">32</span>)</span><br></pre></td></tr></table></figure>

<p>接下来是数据预处理，神经网络的训练方法一般是基于梯度的优化算法，如梯度下降，为了让这类算法能更好的优化神经网络，我们往往需要对数据集进行归一化，这里我们选择对数据进行标准化。</p>
<p>减去均值可以让数据以0为中心，除以标准差可以让数据缩放到一个较小的范围内。这样可以使得梯度的下降方向更多样，同时缩小梯度的数量级，让学习变得稳定。  </p>
<p>首先需要对训练集进行标准化，针对每个特征求出其均值和标准差，然后用训练集的每个样本减去均值除以标准差，就得到了新的训练集。然后用测试集的每个样本，减去训练集的均值，除以训练集的标准差，完成对测试集的标准化。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trainY_mat = np.zeros((<span class="built_in">len</span>(trainY), <span class="number">10</span>))</span><br><span class="line">trainY_mat[np.arange(<span class="number">0</span>, <span class="built_in">len</span>(trainY), <span class="number">1</span>), trainY] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">testY_mat = np.zeros((<span class="built_in">len</span>(testY), <span class="number">10</span>))</span><br><span class="line">testY_mat[np.arange(<span class="number">0</span>, <span class="built_in">len</span>(testY), <span class="number">1</span>), testY] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>下面是参数的初始化。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span>(<span class="params">h, K</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数初始化</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    h: int: 隐藏层单元个数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    K: int: 输出层单元个数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    parameters: dict，参数，键是&quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    np.random.seed(<span class="number">32</span>)</span><br><span class="line">    W_1 = np.random.normal(size = (trainX.shape[<span class="number">1</span>], h)) * <span class="number">0.01</span></span><br><span class="line">    b_1 = np.zeros((<span class="number">1</span>, h))</span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">32</span>)</span><br><span class="line">    W_2 = np.random.normal(size = (h, K)) * <span class="number">0.01</span></span><br><span class="line">    b_2 = np.zeros((<span class="number">1</span>, K))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">&#x27;W1&#x27;</span>: W_1, <span class="string">&#x27;b1&#x27;</span>: b_1, <span class="string">&#x27;W2&#x27;</span>: W_2, <span class="string">&#x27;b2&#x27;</span>: b_2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>向前传播，这里具体指的就是依据公式向前计算值。</p>
<p>这里有一点要注意，矩阵的点乘是使用<code>np.dot()</code>进行的，否则py会默认为元素乘。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_combination</span>(<span class="params">X, W, b</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    计算Z，Z = XW + b</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    W: np.ndarray, shape = (m, h)，权重</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    b: np.ndarray, shape = (1, h)，偏置</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    Z: np.ndarray, shape = (n, h)，线性组合后的值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Z = XW + b</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    Z = np.dot(X,W) + b</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure>
<p>每一线性层的输出都要经过一个activate，隐藏层的activate为ReLu。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReLU</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    ReLU激活函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X: np.ndarray，待激活的矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    activations: np.ndarray, 激活后的矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    X[X &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    activations = X</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> activations</span><br></pre></td></tr></table></figure>
<p>输出层要经过softmax找到每一个label的概率大小。这里值得注意的是，O矩阵的求和是对每一行的各个元素求和，而不是对所有元素求和，所以要有<code>axis=1</code>，对行进行sum操作，并保持维度。</p>
<p>前一个<code>my_softmax(O)</code>会导致对于较小值的output，会导致分母为0的情况，所以要对其进行一些处理，让O的每一个元素减去该行的最大值，这样能保证取exp后至少一个元素为1，所以不会出现NaN的情况。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_softmax</span>(<span class="params">O</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    softmax激活</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(O) / np.<span class="built_in">sum</span>(np.exp(O), axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">O</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    softmax激活函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    O: np.ndarray，待激活的矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    activations: np.ndarray, 激活后的矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE HEER</span></span><br><span class="line">    O = O - np.<span class="built_in">max</span>(O, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    activations = my_softmax(O)</span><br><span class="line">    <span class="keyword">return</span> activations</span><br></pre></td></tr></table></figure>
<p>接下来是实现损失函数，交叉熵损失函数：<br><a href="https://www.codecogs.com/eqnedit.php?latex=\mathrm{loss}&space;=&space;-&space;\frac{1}{n}&space;\sum_n&space;\sum^{K}_{k=1}&space;y_k&space;\log{(\hat{y_k})}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathrm{loss}&space;=&space;-&space;\frac{1}{n}&space;\sum_n&space;\sum^{K}_{k=1}&space;y_k&space;\log{(\hat{y_k})}" title="\mathrm{loss} = - \frac{1}{n} \sum_n \sum^{K}_{k=1} y_k \log{(\hat{y_k})}" /></a><br>这里又会出一个问题，交叉熵损失函数中，我们需要对softmax的激活值取对数，也就是log\haty，这就要求我们的激活值全都是大于0的数，不能等于0，但是我们实现的softmax在有些时候确实会输出0。这就使得在计算loss的时候会出现问题，解决这个问题的方法是log softmax。所谓log softmax，就是将交叉熵中的对数运算与softmax结合起来，避开为0的情况。</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\log{\frac{\exp{(O_i)}}{\sum_K&space;\exp{(O_k)}}}&space;&=&space;\log{\frac{\exp{(O_i&space;-&space;\mathrm{max}(O))}}{\sum_K&space;\exp{(O_k&space;-&space;\mathrm{max}(O))}}}\\&space;&=&space;O_i&space;-&space;\mathrm{max}(O)&space;-&space;\log{\sum_K&space;\exp{(O_k&space;-&space;\mathrm{max}(O))}}&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\log{\frac{\exp{(O_i)}}{\sum_K&space;\exp{(O_k)}}}&space;&=&space;\log{\frac{\exp{(O_i&space;-&space;\mathrm{max}(O))}}{\sum_K&space;\exp{(O_k&space;-&space;\mathrm{max}(O))}}}\\&space;&=&space;O_i&space;-&space;\mathrm{max}(O)&space;-&space;\log{\sum_K&space;\exp{(O_k&space;-&space;\mathrm{max}(O))}}&space;\end{aligned}" title="\begin{aligned} \log{\frac{\exp{(O_i)}}{\sum_K \exp{(O_k)}}} &= \log{\frac{\exp{(O_i - \mathrm{max}(O))}}{\sum_K \exp{(O_k - \mathrm{max}(O))}}}\\ &= O_i - \mathrm{max}(O) - \log{\sum_K \exp{(O_k - \mathrm{max}(O))}} \end{aligned}" /></a></p>
<p>这样我们再计算loss的时候就可以把输出层的输出直接放到log softmax中计算，不用先激活，再取对数了。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_softmax</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    log softmax</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    x: np.ndarray，待激活的矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    log_activations: np.ndarray, 激活后取了对数的矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    log_activations = x - np.<span class="built_in">max</span>(x) - np.log( np.<span class="built_in">sum</span>(np.exp(x - np.<span class="built_in">max</span>(x)), axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>) )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> log_activations</span><br></pre></td></tr></table></figure>
<p>然后编写<code>cross_entropy_with_softmax</code>。函数内容不再赘述。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_with_softmax</span>(<span class="params">y_true, O</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    求解交叉熵损失函数，这里需要使用log softmax，所以参数分别是真值和未经softmax激活的输出值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    loss: float, 平均的交叉熵损失值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 平均交叉熵损失</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    loss = - <span class="number">1</span>/<span class="built_in">len</span>(y_true) * np.<span class="built_in">sum</span>(np.<span class="built_in">sum</span>(y_true * log_softmax(O)))    <span class="comment"># 这里是元素乘</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>正是因为softmax激活与交叉熵损失会有这样的问题，所以在很多深度学习框架中，交叉熵损失函数就直接带有了激活的功能，所以我们在实现前向传播计算的时候，就不要加softmax激活函数了。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">X, parameters</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    前向传播，从输入一直到输出层softmax激活前的值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    parameters: dict，参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    Z = np.dot(X, parameters[<span class="string">&#x27;W1&#x27;</span>]) + parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 隐藏层的激活</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    H = ReLU(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 隐藏层到输出层</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    O = np.dot(H, parameters[<span class="string">&#x27;W2&#x27;</span>]) + parameters[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> O</span><br></pre></td></tr></table></figure>
<p>下面是反向传播，也是本篇blog的重点。首先是偏导的推导，细节不再赘述，使用链式求导法则认真推导即可。</p>
<p>forward公式：最后一层的输出，使用softmax函数激活，得到神经网络计算出的各类的概率值。<br><a href="https://www.codecogs.com/eqnedit.php?latex=Z&space;=&space;XW_1&space;&plus;&space;b_1\\&space;H_1&space;=&space;\mathrm{ReLU}(Z)\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Z&space;=&space;XW_1&space;&plus;&space;b_1\\&space;H_1&space;=&space;\mathrm{ReLU}(Z)\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2" title="Z = XW_1 + b_1\\ H_1 = \mathrm{ReLU}(Z)\\ O = H_1 W_2 + b_2" /></a></p>
<p>损失函数对参数W_2和b_2的偏导数：<br><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;W_2}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;W_2}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;W_2}\\&space;&&space;=&space;\frac{1}{n}&space;(\hat{y}&space;-&space;y)&space;\frac{\partial&space;O}{\partial&space;W_2}\\&space;&&space;=&space;\frac{1}{n}&space;[{H_1}^\mathrm{T}&space;(\hat{y}&space;-&space;y)]&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;W_2}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;W_2}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;W_2}\\&space;&&space;=&space;\frac{1}{n}&space;(\hat{y}&space;-&space;y)&space;\frac{\partial&space;O}{\partial&space;W_2}\\&space;&&space;=&space;\frac{1}{n}&space;[{H_1}^\mathrm{T}&space;(\hat{y}&space;-&space;y)]&space;\end{aligned}" title="\begin{aligned} \frac{\partial loss}{\partial W_2} & = \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial O} \frac{\partial O}{\partial W_2}\\ & = \frac{\partial loss}{\partial O} \frac{\partial O}{\partial W_2}\\ & = \frac{1}{n} (\hat{y} - y) \frac{\partial O}{\partial W_2}\\ & = \frac{1}{n} [{H_1}^\mathrm{T} (\hat{y} - y)] \end{aligned}" /></a></p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;b_2}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;b_2}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;b_2}\\&space;&&space;=&space;\frac{1}{n}&space;(\hat{y}&space;-&space;y)&space;\frac{\partial&space;O}{\partial&space;b_2}\\&space;&&space;=&space;\frac{1}{n}&space;\sum^n_{i=1}&space;(\hat{y_i}&space;-&space;y_i)&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;b_2}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;b_2}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;b_2}\\&space;&&space;=&space;\frac{1}{n}&space;(\hat{y}&space;-&space;y)&space;\frac{\partial&space;O}{\partial&space;b_2}\\&space;&&space;=&space;\frac{1}{n}&space;\sum^n_{i=1}&space;(\hat{y_i}&space;-&space;y_i)&space;\end{aligned}" title="\begin{aligned} \frac{\partial loss}{\partial b_2} & = \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial O} \frac{\partial O}{\partial b_2}\\ & = \frac{\partial loss}{\partial O} \frac{\partial O}{\partial b_2}\\ & = \frac{1}{n} (\hat{y} - y) \frac{\partial O}{\partial b_2}\\ & = \frac{1}{n} \sum^n_{i=1} (\hat{y_i} - y_i) \end{aligned}" /></a></p>
<p>求得loss对W_1和b_1的偏导数：<br><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;W_1}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;W_1}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;W_1}\\&space;&&space;=&space;\frac{1}{n}&space;{X}^\mathrm{T}&space;[(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;\frac{\partial&space;H_1}{\partial&space;Z}]\\&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;W_1}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;W_1}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;W_1}\\&space;&&space;=&space;\frac{1}{n}&space;{X}^\mathrm{T}&space;[(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;\frac{\partial&space;H_1}{\partial&space;Z}]\\&space;\end{aligned}" title="\begin{aligned} \frac{\partial loss}{\partial W_1} & = \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial O} \frac{\partial O}{\partial H_1} \frac{\partial H_1}{\partial Z} \frac{\partial Z}{\partial W_1}\\ & = \frac{\partial loss}{\partial O} \frac{\partial O}{\partial H_1} \frac{\partial H_1}{\partial Z} \frac{\partial Z}{\partial W_1}\\ & = \frac{1}{n} {X}^\mathrm{T} [(\hat{y} - y) {W_2}^\mathrm{T} \frac{\partial H_1}{\partial Z}]\\ \end{aligned}" /></a></p>
<p>ReLu的偏导数：</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;\mathrm{ReLU(x)}}{\partial&space;x}&space;=&space;\begin{cases}&space;0&space;&&space;\text{if&space;}&space;x&space;<&space;0\\&space;1&space;&&space;\text{if&space;}&space;x&space;\geq&space;0&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;\mathrm{ReLU(x)}}{\partial&space;x}&space;=&space;\begin{cases}&space;0&space;&&space;\text{if&space;}&space;x&space;<&space;0\\&space;1&space;&&space;\text{if&space;}&space;x&space;\geq&space;0&space;\end{cases}" title="\frac{\partial \mathrm{ReLU(x)}}{\partial x} = \begin{cases} 0 & \text{if } x < 0\\ 1 & \text{if } x \geq 0 \end{cases}" /></a></p>
<p>从而：</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;loss}{\partial&space;{W_1}_{ij}}&space;=&space;\begin{cases}&space;0&space;&&space;\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\&space;\frac{1}{n}&space;{X}^\mathrm{T}&space;(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;&&space;\text{if&space;}&space;{Z}_{ij}&space;\geq&space;0&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;loss}{\partial&space;{W_1}_{ij}}&space;=&space;\begin{cases}&space;0&space;&&space;\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\&space;\frac{1}{n}&space;{X}^\mathrm{T}&space;(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;&&space;\text{if&space;}&space;{Z}_{ij}&space;\geq&space;0&space;\end{cases}" title="\frac{\partial loss}{\partial {W_1}_{ij}} = \begin{cases} 0 & \text{if } {Z}_{ij} < 0\\ \frac{1}{n} {X}^\mathrm{T} (\hat{y} - y) {W_2}^\mathrm{T} & \text{if } {Z}_{ij} \geq 0 \end{cases}" /></a></p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;b_1}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;b_1}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;b_1}\\&space;&&space;=&space;\frac{1}{n}&space;(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;\frac{\partial&space;H_1}{\partial&space;Z}\\&space;&&space;=&space;\begin{cases}&space;0&space;&\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\&space;\frac{1}{n}&space;\sum_n&space;(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;&\text{if&space;}&space;{Z}_{ij}&space;\geq&space;0&space;\end{cases}&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\frac{\partial&space;loss}{\partial&space;b_1}&space;&&space;=&space;\frac{\partial&space;\mathrm{loss}}{\partial&space;\hat{y}}&space;\frac{\partial&space;\hat{y}}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;b_1}\\&space;&&space;=&space;\frac{\partial&space;loss}{\partial&space;O}&space;\frac{\partial&space;O}{\partial&space;H_1}&space;\frac{\partial&space;H_1}{\partial&space;Z}&space;\frac{\partial&space;Z}{\partial&space;b_1}\\&space;&&space;=&space;\frac{1}{n}&space;(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;\frac{\partial&space;H_1}{\partial&space;Z}\\&space;&&space;=&space;\begin{cases}&space;0&space;&\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\&space;\frac{1}{n}&space;\sum_n&space;(\hat{y}&space;-&space;y)&space;{W_2}^\mathrm{T}&space;&\text{if&space;}&space;{Z}_{ij}&space;\geq&space;0&space;\end{cases}&space;\end{aligned}" title="\begin{aligned} \frac{\partial loss}{\partial b_1} & = \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial O} \frac{\partial O}{\partial H_1} \frac{\partial H_1}{\partial Z} \frac{\partial Z}{\partial b_1}\\ & = \frac{\partial loss}{\partial O} \frac{\partial O}{\partial H_1} \frac{\partial H_1}{\partial Z} \frac{\partial Z}{\partial b_1}\\ & = \frac{1}{n} (\hat{y} - y) {W_2}^\mathrm{T} \frac{\partial H_1}{\partial Z}\\ & = \begin{cases} 0 &\text{if } {Z}_{ij} < 0\\ \frac{1}{n} \sum_n (\hat{y} - y) {W_2}^\mathrm{T} &\text{if } {Z}_{ij} \geq 0 \end{cases} \end{aligned}" /></a></p>
<p>描述完公式后下面来用代码实现，首先dW2和db2的代码是很显然的。对于dW1，这里涉及到的ReLu的偏导数，很显然如果hidden层的值小于零对应ReLu为0时，定义其偏导为0，那么如何确定dW1中的那些值是由该定义得到的呢。如果我们眼光狭窄只分析dW2公式的最后结果必然很难分析出来，因为最终的dW2是(hidden, output)维度的，而relu_regard是(n, hidden)维度的，直接对它们进行关联显然不现实。那么需要追根溯源，深入了解这个dW2的来由。</p>
<p>在dW2分段函数的前一步，它的结果是XT点积后面的一堆，其中H对Z的偏导其实就是ReLu的偏导，是在这里决定了dW2的值，再来分析一下维度1/n可以broadcast不用管，后面是<code>(n, input)T · [(n, output) · (hidden, output)T * (n, hidden)]</code>这样的维度关系。这里尤其要注意最后一个运算，我一开始卡在这里好久，因为这里涉及到了元素乘，<code>(n, hidden) * (n, hidden)</code>，这里决定了哪个计算位置的值来自于ReLu的0，元素乘后再与X的转置计算。</p>
<p>db2同理。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_gradient</span>(<span class="params">y_true, y_pred, H, Z, X, parameters</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    计算梯度</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    H: np.ndarray, shape = (n, h)，隐藏层激活后的值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    X: np.ndarray, shape = (n, m)，输入的原始数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    parameters: dict，参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    grads: dict, 梯度</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算W2的梯度</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    dW2 = (<span class="number">1</span>/<span class="built_in">len</span>(y_true)) * np.dot(np.transpose(H), (y_pred - y_true))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算b2的梯度</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    db2 = (<span class="number">1</span>/<span class="built_in">len</span>(y_true)) * np.<span class="built_in">sum</span>(y_pred - y_true, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算ReLU的梯度</span></span><br><span class="line">    relu_grad = Z.copy()</span><br><span class="line">    relu_grad[relu_grad &gt;= <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    relu_grad[relu_grad &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算W1的梯度</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    dW1 = <span class="number">1</span>/<span class="built_in">len</span>(y_true) * np.dot(np.transpose(X), np.dot((y_pred - y_true), np.transpose(parameters[<span class="string">&#x27;W2&#x27;</span>])) * relu_grad  )</span><br><span class="line">    <span class="comment"># 计算b1的梯度</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    db1 = <span class="number">1</span>/<span class="built_in">len</span>(y_true) * np.<span class="built_in">sum</span>(np.dot((y_pred - y_true), np.transpose(parameters[<span class="string">&#x27;W2&#x27;</span>])) * relu_grad, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">&#x27;dW2&#x27;</span>: dW2, <span class="string">&#x27;db2&#x27;</span>: db2, <span class="string">&#x27;dW1&#x27;</span>: dW1, <span class="string">&#x27;db1&#x27;</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>梯度下降，反向传播，参数更新。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">parameters, grads, learning_rate</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数更新</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    parameters: dict，参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    grads: dict, 梯度</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    learning_rate: float, 学习率</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    parameters[<span class="string">&#x27;W2&#x27;</span>] -= learning_rate * grads[<span class="string">&#x27;dW2&#x27;</span>]</span><br><span class="line">    parameters[<span class="string">&#x27;b2&#x27;</span>] -= learning_rate * grads[<span class="string">&#x27;db2&#x27;</span>]</span><br><span class="line">    parameters[<span class="string">&#x27;W1&#x27;</span>] -= learning_rate * grads[<span class="string">&#x27;dW1&#x27;</span>]</span><br><span class="line">    parameters[<span class="string">&#x27;b1&#x27;</span>] -= learning_rate * grads[<span class="string">&#x27;db1&#x27;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">y_true, y_pred, H, Z, X, parameters, learning_rate</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    计算梯度，参数更新</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    H: np.ndarray, shape = (n, h)，隐藏层激活后的值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    X: np.ndarray, shape = (n, m)，输入的原始数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    parameters: dict，参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    learning_rate: float, 学习率</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    grads = compute_gradient(y_true, y_pred, H, Z, X, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    update(parameters, grads, learning_rate)</span><br></pre></td></tr></table></figure>
<p>训练。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">trainX, trainY, testX, testY, parameters, epochs, learning_rate = <span class="number">0.01</span>, verbose = <span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    训练</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    trainX: np.ndarray, shape = (n, m), 训练集</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    trainY: np.ndarray, shape = (n, K), 训练集标记</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    testX: np.ndarray, shape = (n_test, m)，测试集</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    testY: np.ndarray, shape = (n_test, K)，测试集的标记</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    parameters: dict，参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    epochs: int, 要迭代的轮数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    learning_rate: float, default 0.01，学习率</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    verbose: boolean, default False，是否打印损失值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 存储损失值</span></span><br><span class="line">    training_loss_list = []</span><br><span class="line">    testing_loss_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 这里要计算出Z和H，因为后面反向传播计算梯度的时候需要这两个矩阵</span></span><br><span class="line">        Z = linear_combination(trainX, parameters[<span class="string">&#x27;W1&#x27;</span>], parameters[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        H = ReLU(Z)</span><br><span class="line">        train_O = linear_combination(H, parameters[<span class="string">&#x27;W2&#x27;</span>], parameters[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        train_y_pred = softmax(train_O)</span><br><span class="line">        training_loss = cross_entropy_with_softmax(trainY, train_O)</span><br><span class="line">        </span><br><span class="line">        test_O = forward(testX, parameters)</span><br><span class="line">        testing_loss = cross_entropy_with_softmax(testY, test_O)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> verbose == <span class="literal">True</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;epoch %s, training loss:%s&#x27;</span>%(i + <span class="number">1</span>, training_loss))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;epoch %s, testing loss:%s&#x27;</span>%(i + <span class="number">1</span>, testing_loss))</span><br><span class="line">            <span class="built_in">print</span>()</span><br><span class="line">        </span><br><span class="line">        training_loss_list.append(training_loss)</span><br><span class="line">        testing_loss_list.append(testing_loss)</span><br><span class="line">        </span><br><span class="line">        backward(trainY, train_y_pred, H, Z, trainX, parameters, learning_rate)</span><br><span class="line">    <span class="keyword">return</span> training_loss_list, testing_loss_list</span><br></pre></td></tr></table></figure>
<p>绘制loss随epoch的变化曲线。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_loss_curve</span>(<span class="params">training_loss_list, testing_loss_list</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    绘制损失值变化曲线</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    plt.figure(figsize = (<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">    plt.plot(training_loss_list, label = <span class="string">&#x27;training loss&#x27;</span>)</span><br><span class="line">    plt.plot(testing_loss_list, label = <span class="string">&#x27;testing loss&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<p>预测</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">X, parameters</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，取每行最大的那个数的下标作为标记</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X: np.ndarray, shape = (n, m), 训练集</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    parameters: dict，参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    prediction: np.ndarray, shape = (n, 1)，预测的标记</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 用forward函数得到softmax激活前的值</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    O = forward(X, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算softmax激活后的值</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    y_pred = softmax(O)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取每行最大的元素对应的下标</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    prediction = np.argmax(y_pred, axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> prediction</span><br></pre></td></tr></table></figure>
<p>训练一个不算特别优秀的3-layer-perceptron。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">h = <span class="number">50</span></span><br><span class="line">K = <span class="number">10</span></span><br><span class="line">parameters = initialize(h, K)</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters, <span class="number">1000</span>, <span class="number">0.03</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">end_time = time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training time: %s s&#x27;</span>%(end_time - start_time))</span><br><span class="line">prediction = predict(testX, parameters)</span><br><span class="line"><span class="built_in">print</span>(accuracy_score(prediction, testY))</span><br><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br></pre></td></tr></table></figure>
<p>到这里就结束了，其实不算复杂，数值计算的细节比较重要，以前经常用pytorch来写BP、RNN之类的，但是很少从底层去实现过，这还是一个简单的感知机模型，较复杂的基础模型涉及到的内容可能更复杂。只能说我企图学会吧。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/11/09/3-layer-MLP-md/" data-id="ckvsbawxx0000r5oe89koez8m" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/12/21/Adaboost-md/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Adaboost
        
      </div>
    </a>
  
  
    <a href="/2021/11/08/acm-zhengshumicifang-md/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">acm-zhengshumicifang.md</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACM/" rel="tag">ACM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI-Course/" rel="tag">AI Course</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CV/" rel="tag">CV</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HPC/" rel="tag">HPC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP%E7%9A%84%E4%B8%80%E4%BA%9B%E6%94%B6%E8%8E%B7/" rel="tag">NLP的一些收获</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/QT/" rel="tag">QT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/" rel="tag">数值计算</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" rel="tag">高性能计算</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ACM/" style="font-size: 20px;">ACM</a> <a href="/tags/AI-Course/" style="font-size: 10px;">AI Course</a> <a href="/tags/CV/" style="font-size: 12.5px;">CV</a> <a href="/tags/HPC/" style="font-size: 10px;">HPC</a> <a href="/tags/NLP%E7%9A%84%E4%B8%80%E4%BA%9B%E6%94%B6%E8%8E%B7/" style="font-size: 17.5px;">NLP的一些收获</a> <a href="/tags/QT/" style="font-size: 10px;">QT</a> <a href="/tags/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">数值计算</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">高性能计算</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/04/26/ARNN-md/">ARNN复现反思</a>
          </li>
        
          <li>
            <a href="/2022/03/06/linux-command-md/">linux_command.md</a>
          </li>
        
          <li>
            <a href="/2022/02/13/MPbasedRandomWalk-md/">Meta Path Based Random Walk</a>
          </li>
        
          <li>
            <a href="/2022/01/19/HPC-OpenCL-md/">OpenCL并行编程框架【高性能计算导论课程作业】</a>
          </li>
        
          <li>
            <a href="/2022/01/13/HPC-mandelbrot-md/">Mandelbrot Set</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2024 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>