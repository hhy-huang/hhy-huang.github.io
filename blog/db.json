{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"191244f31a4fad9efbb09395f3cc2e7f846f8e66","modified":1627099928626},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1626193759789},{"_id":"themes/landscape/README.md","hash":"37fae88639ef60d63bd0de22314d7cc4c5d94b07","modified":1626193759789},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1626193759790},{"_id":"themes/landscape/.npmignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1626193759789},{"_id":"themes/landscape/_config.yml","hash":"79ac6b9ed6a4de5a21ea53fc3f5a3de92e2475ff","modified":1626193759789},{"_id":"themes/landscape/package.json","hash":"544f21a0b2c7034998b36ae94dba6e3e0f39f228","modified":1626193759790},{"_id":"themes/landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1626193759791},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1626193759792},{"_id":"themes/landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1626193759791},{"_id":"themes/landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1626193759792},{"_id":"themes/landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1626193759792},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1626193759792},{"_id":"themes/landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1626193759792},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1626193759792},{"_id":"themes/landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1626193759792},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1626193759792},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1626193759792},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1626193759792},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1626193759791},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1626193759791},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1626193759791},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1626193759791},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1626193759791},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1626193759791},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1626193759791},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1626193759791},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"d0d753d39038284d52b10e5075979cc97db9cd20","modified":1626193759793},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1626193759793},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"950ddd91db8718153b329b96dc14439ab8463ba5","modified":1626193759793},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1626193759793},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1626193759793},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1626193759794},{"_id":"themes/landscape/layout/_partial/gauges-analytics.ejs","hash":"aad6312ac197d6c5aaf2104ac863d7eba46b772a","modified":1626193759794},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"7e749050be126eadbc42decfbea75124ae430413","modified":1626193759794},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"5abf77aec957d9445fc71a8310252f0013c84578","modified":1626193759794},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1626193759794},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1626193759794},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1626193759793},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1626193759794},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1626193759795},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1626193759794},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1626193759795},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1626193759798},{"_id":"themes/landscape/source/css/_variables.styl","hash":"628e307579ea46b5928424313993f17b8d729e92","modified":1626193759798},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1626193759798},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1626193759796},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1626193759797},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1626193759797},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1626193759797},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1626193759797},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1626193759797},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1626193759797},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1626193759798},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1626193759797},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1626193759796},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1626193759795},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1626193759795},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1626193759796},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1626193759795},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1626193759795},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1626193759795},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1626193759800},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1626193759801},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1626193759798},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1626193759801},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1626193759801},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1626193759801},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1626193759801},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1626193759801},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1626193759801},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1626193759801},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1626193759796},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1626193759799},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1626193759800},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1626193759801},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1626193759799},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1626193759798},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1626193759799},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1626193759799},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1626193759799},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1626193759799},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1626193759799},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1626193759800},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"58f58145f2a483de7d4aa9ee47147e62ddfb832d","modified":1604714008503},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1626193759801},{"_id":"public/archives/2021/07/index.html","hash":"24fb45d790981a4070bcbfb20b8d897ade884dd9","modified":1726335261521},{"_id":"public/archives/2021/index.html","hash":"1bb689e719dc441be0ab5befa01e00b147cdc379","modified":1726335261521},{"_id":"public/archives/index.html","hash":"880a02f9a7d2a7178fdac1751b7e0170e2bca8b6","modified":1726335261521},{"_id":"public/index.html","hash":"199cda484bf30d9f684b83f11c850d73033836d9","modified":1726335261521},{"_id":"public/2021/07/14/hello-world/index.html","hash":"1d4ebe311c3cfa23217650a913fd72437b967cf7","modified":1726335261521},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1626194739167},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1626194739167},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1626194739167},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1626194739167},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1626194739167},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1626194739167},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1626194739167},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1626194739167},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1626194739167},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1626194739167},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1626194739167},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1626194739167},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1626194739167},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1626194739167},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1626194739167},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1626194739167},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1626194739167},{"_id":"public/css/style.css","hash":"5f8dadd37d0052c557061018fe6f568f64fced9b","modified":1626194739167},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1626194739167},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1626194739167},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1626194739167},{"_id":"public/css/images/banner.jpg","hash":"58f58145f2a483de7d4aa9ee47147e62ddfb832d","modified":1626197033446},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1626194739167},{"_id":"source/_posts/firstPage-md.md","hash":"1b1f7715e64205557e9ad35f006f69a805c40e82","modified":1626196397606},{"_id":"public/2021/07/14/firstPage-md/index.html","hash":"92109c32fe4475a3941490ec92f28363db5c0a8c","modified":1726335261521},{"_id":"source/.DS_Store","hash":"40970008550621432b604220add1c05fed30e1cf","modified":1726335225178},{"_id":"public/tags/A/index.html","hash":"132c1fa61a3d3870de55b47d7ddbd2893d7eeeff","modified":1626195509791},{"_id":"themes/landscape/layout/.DS_Store","hash":"7c8a85c6f12c1edce45ceab2c03529bde5b2f79c","modified":1626711665406},{"_id":"themes/landscape/source/css/.DS_Store","hash":"f1db6587630f244af7214e9599341309466bf8ce","modified":1636101995205},{"_id":"themes/landscape/.DS_Store","hash":"49c7ce93b1ffd2d463c6098751f25d612a42671d","modified":1726335225226},{"_id":"themes/landscape/source/.DS_Store","hash":"60173f846c78f146ae9bdda542da1f50127d2b2a","modified":1636101937211},{"_id":"public/tags/NLP的一些收获/index.html","hash":"32475c35387224f1a23a90cc5cc66b5dd629a611","modified":1726335261521},{"_id":"themes/landscape/source/css/images/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1626197209354},{"_id":"themes/landscape/layout/_partial/.DS_Store","hash":"88ce5d618f5bf6c5dfdd973fac0965ba2091852c","modified":1626711665401},{"_id":"source/_posts/acm1-md.md","hash":"ed72b44b379bb5dcced432784eacffc53a47ff0d","modified":1626240579811},{"_id":"public/tags/ACM/index.html","hash":"18c6d0dcea9fff4403a154f5f9309d09d1ca9590","modified":1726335261521},{"_id":"public/2021/07/14/acm1-md/index.html","hash":"759fafb96a102729e752a0e52b55406c1397a651","modified":1726335261521},{"_id":"source/_posts/acm2-md.md","hash":"c5fa170a85e8cb36dd12ec156de35dc5babf0bb5","modified":1626278530962},{"_id":"public/2021/07/14/acm2-md/index.html","hash":"aa474a4a0de37d2e15a6725b1c500a8af73b87bb","modified":1726335261521},{"_id":"source/_posts/acm3-md.md","hash":"489350939ef69c499b357a27e98d068f2a7ff341","modified":1627099383661},{"_id":"public/2021/07/16/acm3-md/index.html","hash":"fa3390f0f78b5e52fcae1d561d385e58e53a5d8f","modified":1726335261521},{"_id":"public/archives/page/2/index.html","hash":"816f112ff71df46df6833feeb0fe567a6efafa0f","modified":1726335261521},{"_id":"public/archives/2021/page/2/index.html","hash":"2ee9029e2cef62d3e1bf0c72ef2b95f0cb4e37f7","modified":1726335261521},{"_id":"public/archives/2021/07/page/2/index.html","hash":"70fceea3d106aee0348548124bc13211b0e3f2bf","modified":1626369296484},{"_id":"source/_posts/acm4-md.md","hash":"19cd08789e9771a27880eedc3e5f64435ef2d462","modified":1626541883278},{"_id":"public/2021/07/18/acm4-md/index.html","hash":"ddf3428eee7cc6b164c3f420c3fb8a56e0084d9f","modified":1726335261521},{"_id":"public/page/2/index.html","hash":"0f320e4d4e8e59a8ec9c3679b437eb003e947890","modified":1726335261521},{"_id":"source/_posts/qtsignalsAndSlots-md.md","hash":"3de9d3f3972bcb9fb31223ebffc9e2fcb8db9ae8","modified":1626769861785},{"_id":"source/_posts/game1-md.md","hash":"3c64f891d6bb54e357afc8b6db4fa966e25d666c","modified":1626616074698},{"_id":"public/2021/07/20/qtsignalsAndSlots-md/index.html","hash":"ab0d7c78ffbe5e38f063ca5a531ba77c24a906a8","modified":1726335261521},{"_id":"public/tags/QT/index.html","hash":"a53fb55ce6edc4163811f1bc62770506bef6bcc0","modified":1726335261521},{"_id":"public/2021/07/18/game1-md/index.html","hash":"9fb92443165c5f8ea615329a208a8f49890cb191","modified":1626714515996},{"_id":"source/_posts/juanjihe-md.md","hash":"b74ccc830b4f049946275e056fb17708a9414267","modified":1627099894032},{"_id":"source/_posts/.DS_Store","hash":"26e91d36d33b6a34299113bb8a44abbcc756ddda","modified":1642499592320},{"_id":"public/2021/04/10/juanjihe-md/index.html","hash":"dcf8264168c9d4bdfe49e327a1e125b352e8bed0","modified":1726335261521},{"_id":"public/archives/2021/04/index.html","hash":"9d0e99ff12810d44f8f7d86b242c56e26ac75c1a","modified":1726335261521},{"_id":"public/tags/NLP/index.html","hash":"edc16df3e8a356003345e40370597f648eb665c2","modified":1629207582307},{"_id":"source/_posts/jisuanfangfa1-md.md","hash":"a73fd1cc332a2e11a9d5d37523c08554fb1b1a80","modified":1627100435793},{"_id":"public/2021/06/12/jisuanfangfa1-md/index.html","hash":"99a506a1e2715eb6dae965aee3d4f192526c3763","modified":1726335261521},{"_id":"public/archives/2021/06/index.html","hash":"c9015ce6cb3f88bf119c10f4cae180c9c7ddaf7d","modified":1726335261521},{"_id":"public/tags/数值计算/index.html","hash":"920480557fe35bfea3a5d78b90fd3f2ee6fbb92a","modified":1726335261521},{"_id":"source/_posts/IOU-md.md","hash":"d0833081f4791fab3b2c89a596b2544dae327601","modified":1627376526626},{"_id":"public/2021/07/27/IOU-md/index.html","hash":"4d2114d9b182a9eb389ae486654ad169fae24eb8","modified":1726335261521},{"_id":"public/tags/CV/index.html","hash":"b968885f86e2d122ba861f4cc63b66cca0cb43d0","modified":1726335261521},{"_id":"source/_posts/BUTD-md.md","hash":"71f06deecc67d08e935395e736782a4f7525bc7d","modified":1627386760470},{"_id":"public/2021/07/27/BUTD-md/index.html","hash":"b2faf663912573872fa571085f5b8f6db9a49632","modified":1726335261521},{"_id":"public/page/3/index.html","hash":"4ae66cb2987c3d3bba88a1337a7972a3acebee10","modified":1726335261521},{"_id":"source/_posts/STGN-process-md.md","hash":"905f22c3b74686fee6cc4ef4be5d967f19a89322","modified":1628509006971},{"_id":"source/_posts/NDCG-md.md","hash":"ae7916a107548467b4c9257447f7e88d938ed02c","modified":1627817360158},{"_id":"public/2021/08/06/STGN-process-md/index.html","hash":"159ca9f84364078cb0c1006f6b34567cf36f93f5","modified":1726335261521},{"_id":"public/2021/08/01/NDCG-md/index.html","hash":"f89caeeba2c6d909f5dc82ebba1f799404307e86","modified":1726335261521},{"_id":"public/archives/2021/08/index.html","hash":"6d397c68089d776b9c7b669711c7b89828c4550f","modified":1726335261521},{"_id":"source/_posts/BLEU-md.md","hash":"4606015a79ac1fd6c6162cfe73ebe46c78f37e33","modified":1629207650712},{"_id":"public/2021/08/17/BLEU-md/index.html","hash":"56ef9660f506e26e4161ad809e18ea370bfb3e07","modified":1726335261521},{"_id":"source/_posts/model-eval-md.md","hash":"94dbff259fd109d657894141ca58bb00bae0e5df","modified":1632069657325},{"_id":"source/_posts/1.png","hash":"d58aeae97527dda81ff223387770552fd1390f0d","modified":1631755355129},{"_id":"public/2021/09/16/model-eval-md/index.html","hash":"00a51babd32a0ffd2892ed7aef20cd2e807e0f94","modified":1726335261521},{"_id":"public/tags/机器学习/index.html","hash":"e86dae7f23957bdbe9d7e0f3a031e6f4e6453184","modified":1726335261521},{"_id":"public/archives/2021/09/index.html","hash":"71a17cee4d680caeee8d02b4560f4956926fc889","modified":1726335261521},{"_id":"source/_posts/linear-reg-md.md","hash":"cb792f8131abe3bfc94454f2b8c6743d278dd528","modified":1636090509093},{"_id":"public/2021/09/23/linear-reg-md/index.html","hash":"7d8cd644aa989d5636d4b98dfd53f5fa4424f6d7","modified":1726335261521},{"_id":"public/page/4/index.html","hash":"21effa7e1610d5bf25e29e2a5a409336fa244852","modified":1644685775646},{"_id":"source/_posts/AI-Review-md.md","hash":"0df9269a5c4202c61ae1e5ef6187645d0a6a3519","modified":1636105959339},{"_id":"source/_posts/人工智能复习/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1636101581455},{"_id":"source/_posts/人工智能复习/Page6.png","hash":"67e54645ace612a9e64e0fe3ffdbb7c390b4d741","modified":1636088552000},{"_id":"source/_posts/人工智能复习/Page1.png","hash":"8b1c08f708c390faea5e1a4783b52c0fb9eb7933","modified":1636088542000},{"_id":"source/_posts/人工智能复习/Page7.png","hash":"c526ed5ce060ebb2e9f92c7f8ad3800b25ef7aa2","modified":1636088554000},{"_id":"source/_posts/人工智能复习/Page19.png","hash":"88a8798ccaedf79b2d198acd1e9867964f0e8f12","modified":1636088578000},{"_id":"source/_posts/人工智能复习/Page17.png","hash":"4953e7876ac844cf5d0d19031ef13187f6d30aa8","modified":1636088574000},{"_id":"source/_posts/人工智能复习/Page2.png","hash":"79cb591ebdf240f84e3fa63ea125104376a9d567","modified":1636088544000},{"_id":"source/_posts/人工智能复习/Page3.png","hash":"4a16ddf62afdc286431a15f40cda464ac67a2714","modified":1636088546000},{"_id":"source/_posts/人工智能复习/Page4.png","hash":"d6082dfc6f975912afe926e193d1ea4777f5aca9","modified":1636088548000},{"_id":"source/_posts/人工智能复习/Page10.png","hash":"30431a3311debe130dc07ac37dc9f8eb87ea3b43","modified":1636088560000},{"_id":"source/_posts/人工智能复习/Page11.png","hash":"160c376d2e6c0d6111ed209eb18fe5b4b3e0e3a0","modified":1636088562000},{"_id":"source/_posts/人工智能复习/Page20.png","hash":"cc428175eb0c66d32d7ca6fdb3d284b7ee2febbc","modified":1636088580000},{"_id":"source/_posts/人工智能复习/Page5.png","hash":"e1dbe03814ad46ea2f6471977e055ec5ea4c3ac0","modified":1636088550000},{"_id":"source/_posts/人工智能复习/Page12.png","hash":"3603dd1dd6f3a74568544ca4705e34dce6dafb39","modified":1636088564000},{"_id":"source/_posts/人工智能复习/Page14.png","hash":"68b97e7c3dedbc6eff04a7b5ebf44b4763acd943","modified":1636088568000},{"_id":"source/_posts/人工智能复习/Page15.png","hash":"5252ef9bf382b423c0e94224c2a94d1846fe89d2","modified":1636088570000},{"_id":"source/_posts/人工智能复习/Page16.png","hash":"728e990499f700869b74c1a9b9158e960aae9611","modified":1636088572000},{"_id":"source/_posts/人工智能复习/Page13.png","hash":"4219774ac85624af0c11b29e56b40301203e56a8","modified":1636088566000},{"_id":"source/_posts/人工智能复习/Page8.png","hash":"bad96b7d3ccb8039ac4181579408a7be9ee0d8c4","modified":1636088556000},{"_id":"source/_posts/人工智能复习/Page18.png","hash":"3f3230bbfe3b041cf44105ce460d25c89a1af640","modified":1636088576000},{"_id":"source/_posts/人工智能复习/Page9.png","hash":"51cbc83ed4f14cc75366343d8f0a56c76808e971","modified":1636088558000},{"_id":"public/2021/11/05/AI-Review-md/index.html","hash":"2d7e40e38b609df93c06225672e65b3d8446630a","modified":1726335261521},{"_id":"public/archives/2021/11/index.html","hash":"729bbb335a1e39ddf2220e98fc41c757bd337b9d","modified":1726335261521},{"_id":"public/tags/AI-Course/index.html","hash":"eb5b84c245de8a596e60810cd56b894ecf9de419","modified":1726335261521},{"_id":"source/_posts/acm-jimuchengbao-md.md","hash":"f7ba490b96ee77779b93b32705a318b33bd0d17d","modified":1636215406543},{"_id":"public/2021/11/06/acm-jimuchengbao-md/index.html","hash":"e98aca104d4fa7ba9f38c6b4d295494f04ed53cc","modified":1726335261521},{"_id":"source/_posts/acm-shouwangzhe-md.md","hash":"d021e260daf5ae39e183c23e7ed94ea3711f9d3c","modified":1636216422640},{"_id":"public/2021/11/07/acm-shouwangzhe-md/index.html","hash":"16fe3c14a435586d06f9e40e5eb124647b2b7887","modified":1726335261521},{"_id":"source/_posts/acm-jiafenerchashu-md.md","hash":"20c016fda6d6978145e19cd94dce30928b48f239","modified":1636259811703},{"_id":"public/2021/11/07/acm-jiafenerchashu-md/index.html","hash":"5c06010ed681f456ba0e700bd949a3d7358dd8ab","modified":1726335261521},{"_id":"source/_posts/acm-qingdi-md.md","hash":"17867e367cb9ed4ec53574b98aa51032bf4440c2","modified":1636284401795},{"_id":"public/archives/page/3/index.html","hash":"6099c9db4fa4b18c4ca5f07751376de9b4b97f9d","modified":1726335261521},{"_id":"public/archives/2021/page/3/index.html","hash":"3e212c4a6f68161c6babbc9d71baea4c2d5cc91f","modified":1726335261521},{"_id":"public/page/5/index.html","hash":"a84eb63a32dae0c0de48f1d86518dbf83a2225d2","modified":1644685775646},{"_id":"public/2021/11/07/acm-qingdi-md/index.html","hash":"c87e31dae9038f0dbecee2b475fb791e75982b2b","modified":1726335261521},{"_id":"source/_posts/acm-jihehuafen-md.md","hash":"92a0f4da05d84c25d7fd5aabb7b2411ea12520fa","modified":1636289853062},{"_id":"public/2021/11/07/acm-jihehuafen-md/index.html","hash":"cc043bf2f277da8ef8363050a01778e5967e94cf","modified":1726335261521},{"_id":"source/_posts/3-layer-MLP-md.md","hash":"c38aba0de761e85555d6f8f4c5a75115c1f2f9f9","modified":1636475556164},{"_id":"source/_posts/acm-zhengshumicifang-md.md","hash":"ba5180152af3ff52757e4cca5aa8f370bed0c72e","modified":1636354512204},{"_id":"public/2021/11/08/acm-zhengshumicifang-md/index.html","hash":"8c267ed6cb2b334a455e5741544373a00f2c179a","modified":1726335261521},{"_id":"public/2021/11/09/3-layer-MLP-md/index.html","hash":"e57ae0f7a498220e95f003b9ff7b6a1aeba81bd6","modified":1726335261521},{"_id":"source/_posts/Adaboost-md.md","hash":"8c13915f89e8b0960de64c8756f49dee4bfdc859","modified":1640022431453},{"_id":"public/2021/12/21/Adaboost-md/index.html","hash":"b795dfb386fabbed505e8b3de61b1271207ccea3","modified":1726335261521},{"_id":"public/archives/2021/12/index.html","hash":"e0f45b9854d4a36913d242e78f337abc5ef787fc","modified":1726335261521},{"_id":"source/_posts/HPC-mandelbrot-md.md","hash":"9ac796b042ba4730a30809bda4ce8d09b4dddf77","modified":1642069058719},{"_id":"public/2022/01/13/HPC-mandelbrot-md/index.html","hash":"d508548242a70560ef9f7a1e8d62592fcc953dcc","modified":1726335261521},{"_id":"public/page/6/index.html","hash":"a6cb477e3a6714e29ee9d723dfc730048a40883f","modified":1644685775646},{"_id":"public/tags/高性能计算/index.html","hash":"ba16092f0ff1911575c1e1db95ce617bf0c0ac60","modified":1726335261521},{"_id":"public/archives/2022/01/index.html","hash":"2425daba28764c500b63ef517cc58a1bf5493813","modified":1726335261521},{"_id":"public/archives/2022/index.html","hash":"00fdc83165813399419bed8eb49ca0483c5d89ce","modified":1726335261521},{"_id":"source/_posts/MPbasedRandomWalk-md.md","hash":"69dd2fd4778b811e6660e626f4cfeb7e943bc1e8","modified":1644691328817},{"_id":"source/_posts/HPC-OpenCL-md.md","hash":"dd063efb39bfc0024f01278cf8425bc96553e718","modified":1643192988774},{"_id":"public/tags/HPC/index.html","hash":"14a10e33ea75767399884b6eebab5a655018ef36","modified":1726335261521},{"_id":"public/2022/02/13/MPbasedRandomWalk-md/index.html","hash":"a9920e4a176d61595eb5374d9241d07d584d1bcd","modified":1726335261521},{"_id":"public/archives/2022/02/index.html","hash":"a7127ceb34a75808dbc4435e67751ad506f48b39","modified":1726335261521},{"_id":"public/2022/01/19/HPC-OpenCL-md/index.html","hash":"8a5cb835a26149e4395c89483c573d0940847eba","modified":1726335261521},{"_id":"source/_posts/linux-command-md.md","hash":"8ed79bc9a0d2519ba5501c9432ed3859d95efc35","modified":1726335225217},{"_id":"source/_posts/ARNN-md.md","hash":"3667fca8347af87312989bf3e29ea774780d1ff8","modified":1726335225187},{"_id":"public/2022/03/06/linux-command-md/index.html","hash":"fb8b09729a6722846fe7acbd8058d7f189b1743e","modified":1726335261521},{"_id":"public/2022/04/26/ARNN-md/index.html","hash":"c7d175ce94e3b1aa2b0643d6b1f0f008fc5d1503","modified":1726335261521},{"_id":"public/archives/2022/03/index.html","hash":"0ef58c92c718b3c4a40994a13ad169d44e9e6466","modified":1726335261521},{"_id":"public/archives/2022/04/index.html","hash":"6ccb70709f4d7d1c90753084cab1147f5a6f48ec","modified":1726335261521}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Tips","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Tips\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2021-07-13T16:29:19.792Z","updated":"2021-07-24T04:12:08.626Z","_id":"ckr29u6li0000haoe014y422b","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"TransE的理解与实现","date":"2021-07-13T16:48:07.000Z","_content":"\n（依附于博主yuanwyue代码https://blog.csdn.net/shunaoxi2313/article/details/89766467）\n\n## 理解如下图\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401002822969.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center)![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401002903543.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center)\n## 附上那位博主的代码\n自己加了一些有没有的注释帮助理解\n\n```py\nimport codecs\nimport random\nimport math\nimport numpy as np\nimport copy\nimport time\n\nentity2id = {}\nrelation2id = {}\n\n\ndef data_loader(file):\n    file1 = file + \"train.txt\"  # entity entity relation\n    file2 = file + \"entity2id.txt\"\n    file3 = file + \"relation2id.txt\"\n\n    with open(file2, 'r') as f1, open(file3, 'r') as f2:\n        # 逐行读取\n        lines1 = f1.readlines()\n        lines2 = f2.readlines()\n        for line in lines1:\n            # 删除空白字符，并以'\\t'为界划分为list\n            line = line.strip().split('\\t')\n            if len(line) != 2:\n                continue\n            # 将符合标准的数据以dict形式写入entity2id\n            entity2id[line[0]] = line[1]\n\n        # 同上\n        for line in lines2:\n            line = line.strip().split('\\t')\n            if len(line) != 2:\n                continue\n            relation2id[line[0]] = line[1]\n\n    # 创建集合与list\n    entity_set = set()\n    relation_set = set()\n    triple_list = []\n\n    # codecs.open自动转码，打开test\n    with codecs.open(file1, 'r') as f:\n        content = f.readlines()\n        for line in content:\n            triple = line.strip().split(\"\\t\")\n            if len(triple) != 3:\n                continue\n\n            # 三元组,triple[0]和triple[1]均为entity,在entity2id中均能找到对应的编码写入_\n            h_ = entity2id[triple[0]]\n            t_ = entity2id[triple[1]]\n            r_ = relation2id[triple[2]]\n\n            # triple list是三元素id的list\n            triple_list.append([h_, t_, r_])\n\n            # 三个集合均为编码id的集合\n            entity_set.add(h_)\n            entity_set.add(t_)\n            relation_set.add(r_)\n\n    return entity_set, relation_set, triple_list\n\n\n# loss function的两个d\ndef distanceL2(h, r, t):\n    # 为方便求梯度，去掉sqrt\n    return np.sum(np.square(h + r - t))\n\n\ndef distanceL1(h, r, t):\n    return np.sum(np.fabs(h + r - t))\n\n\n# transE类\nclass TransE:\n    def __init__(self, entity_set, relation_set, triple_list,\n                 embedding_dim=100, learning_rate=0.01, margin=1, L1=True):\n        self.embedding_dim = embedding_dim\n        self.learning_rate = learning_rate\n        self.margin = margin\n        self.entity = entity_set\n        self.relation = relation_set\n        self.triple_list = triple_list\n        self.L1 = L1\n\n        self.loss = 0\n\n    # 初始化每个向量（随机这里是均匀分布）\n    def emb_initialize(self):\n        relation_dict = {}\n        entity_dict = {}\n\n        # 读取relation_set的每一行\n        for relation in self.relation:\n            # 均匀分布\n            # array(50),-6/sqrt(50) - 6/sqrt(50)\n            r_emb_temp = np.random.uniform(-6 / math.sqrt(self.embedding_dim),\n                                           6 / math.sqrt(self.embedding_dim),\n                                           self.embedding_dim)\n            # 每一个relation encode\n            relation_dict[relation] = r_emb_temp / np.linalg.norm(r_emb_temp, ord=2)\n\n        for entity in self.entity:\n            e_emb_temp = np.random.uniform(-6 / math.sqrt(self.embedding_dim),\n                                           6 / math.sqrt(self.embedding_dim),\n                                           self.embedding_dim)\n            entity_dict[entity] = e_emb_temp / np.linalg.norm(e_emb_temp, ord=2)\n\n        # 向量赋给relation和entity\n        self.relation = relation_dict\n        self.entity = entity_dict\n\n    def train(self, epochs):\n        # 设置batch size\n        nbatches = 400\n        batch_size = len(self.triple_list) // nbatches\n        print(\"batch size: \", batch_size)\n        for epoch in range(epochs):\n            start = time.time()\n            self.loss = 0\n\n            # SGD\n            for k in range(nbatches):\n                # Sbatch:list(batch_size)   随机不重复抽样\n                Sbatch = random.sample(self.triple_list, batch_size)\n                Tbatch = []\n\n                # negative sampling\n                for triple in Sbatch:\n                    # 每个triple选3个负样例\n                    # for i in range(3):\n                    corrupted_triple = self.Corrupt(triple)\n                    # Tbatch中存有正例和负例\n                    if (triple, corrupted_triple) not in Tbatch:\n                        Tbatch.append((triple, corrupted_triple))\n                self.update_embeddings(Tbatch)\n\n            end = time.time()\n            print(\"epoch: \", epoch, \"cost time: %s\" % (round((end - start), 3)))\n            print(\"loss: \", self.loss)\n\n            # 保存临时结果\n            if epoch % 20 == 0:\n                with codecs.open(\"entity_temp\", \"w\") as f_e:\n                    for e in self.entity.keys():\n                        f_e.write(e + \"\\t\")\n                        f_e.write(str(list(self.entity[e])))\n                        f_e.write(\"\\n\")\n                with codecs.open(\"relation_temp\", \"w\") as f_r:\n                    for r in self.relation.keys():\n                        f_r.write(r + \"\\t\")\n                        f_r.write(str(list(self.relation[r])))\n                        f_r.write(\"\\n\")\n\n        print(\"写入文件...\")\n        with codecs.open(\"entity_50dim_batch400\", \"w\") as f1:\n            for e in self.entity.keys():\n                f1.write(e + \"\\t\")\n                f1.write(str(list(self.entity[e])))\n                f1.write(\"\\n\")\n\n        with codecs.open(\"relation50dim_batch400\", \"w\") as f2:\n            for r in self.relation.keys():\n                f2.write(r + \"\\t\")\n                f2.write(str(list(self.relation[r])))\n                f2.write(\"\\n\")\n        print(\"写入完成\")\n\n    # change head entity or tail entity\n    def Corrupt(self, triple):\n        corrupted_triple = copy.deepcopy(triple)\n        # 随机替换head\\tail\n        seed = random.random()\n        if seed > 0.5:\n            # 替换head\n            rand_head = triple[0]\n            while rand_head == triple[0]:\n                rand_head = random.sample(self.entity.keys(), 1)[0]\n            corrupted_triple[0] = rand_head\n\n        else:\n            # 替换tail\n            rand_tail = triple[1]\n            while rand_tail == triple[1]:\n                rand_tail = random.sample(self.entity.keys(), 1)[0]\n            corrupted_triple[1] = rand_tail\n        return corrupted_triple\n\n    def update_embeddings(self, Tbatch):\n        copy_entity = copy.deepcopy(self.entity)\n        copy_relation = copy.deepcopy(self.relation)\n\n        for triple, corrupted_triple in Tbatch:\n            # 取copy里的vector累积更新\n            h_correct_update = copy_entity[triple[0]]\n            t_correct_update = copy_entity[triple[1]]\n            relation_update = copy_relation[triple[2]]\n\n            h_corrupt_update = copy_entity[corrupted_triple[0]]\n            t_corrupt_update = copy_entity[corrupted_triple[1]]\n\n            # 取原始的vector计算梯度\n            h_correct = self.entity[triple[0]]\n            t_correct = self.entity[triple[1]]\n            relation = self.relation[triple[2]]\n\n            h_corrupt = self.entity[corrupted_triple[0]]\n            t_corrupt = self.entity[corrupted_triple[1]]\n\n            if self.L1:\n                dist_correct = distanceL1(h_correct, relation, t_correct)\n                dist_corrupt = distanceL1(h_corrupt, relation, t_corrupt)\n            else:\n                dist_correct = distanceL2(h_correct, relation, t_correct)\n                dist_corrupt = distanceL2(h_corrupt, relation, t_corrupt)\n\n            err = self.hinge_loss(dist_correct, dist_corrupt)\n\n            # delta d < -m 时跳出\n            if err > 0:\n                self.loss += err\n\n                grad_pos = 2 * (h_correct + relation - t_correct)\n                grad_neg = 2 * (h_corrupt + relation - t_corrupt)\n\n                if self.L1:\n                    for i in range(len(grad_pos)):\n                        if grad_pos[i] > 0:\n                            grad_pos[i] = 1\n                        else:\n                            grad_pos[i] = -1\n\n                    for i in range(len(grad_neg)):\n                        if grad_neg[i] > 0:\n                            grad_neg[i] = 1\n                        else:\n                            grad_neg[i] = -1\n\n                # head系数为正，减梯度；tail系数为负，加梯度\n                h_correct_update -= self.learning_rate * grad_pos\n                t_correct_update -= (-1) * self.learning_rate * grad_pos\n\n                # corrupt项整体为负，因此符号与correct相反\n                if triple[0] == corrupted_triple[0]:  # 若替换的是尾实体，则头实体更新两次\n                    h_correct_update -= (-1) * self.learning_rate * grad_neg\n                    t_corrupt_update -= self.learning_rate * grad_neg\n\n                elif triple[1] == corrupted_triple[1]:  # 若替换的是头实体，则尾实体更新两次\n                    h_corrupt_update -= (-1) * self.learning_rate * grad_neg\n                    t_correct_update -= self.learning_rate * grad_neg\n\n                # relation更新两次\n                relation_update -= self.learning_rate * grad_pos\n                relation_update -= (-1) * self.learning_rate * grad_neg\n\n        # batch norm\n        for i in copy_entity.keys():\n            copy_entity[i] /= np.linalg.norm(copy_entity[i])\n        for i in copy_relation.keys():\n            copy_relation[i] /= np.linalg.norm(copy_relation[i])\n\n        # 达到批量更新的目的\n        self.entity = copy_entity\n        self.relation = copy_relation\n\n    def hinge_loss(self, dist_correct, dist_corrupt):\n        return max(0, dist_correct - dist_corrupt + self.margin)\n\n\nif __name__ == '__main__':\n    file1 = \"./FB15k/\"\n    entity_set, relation_set, triple_list = data_loader(file1)\n    print(\"load file...\")\n    print(\"Complete load. entity : %d , relation : %d , triple : %d\" % (\n        len(entity_set), len(relation_set), len(triple_list)))\n\n    transE = TransE(entity_set, relation_set, triple_list, embedding_dim=50, learning_rate=0.01, margin=1, L1=True)\n    transE.emb_initialize()\n    transE.train(epochs=1001)\n\n```","source":"_posts/firstPage-md.md","raw":"---\ntitle: TransE的理解与实现\ndate: 2021-07-14 00:48:07\ntags: NLP的一些收获\n---\n\n（依附于博主yuanwyue代码https://blog.csdn.net/shunaoxi2313/article/details/89766467）\n\n## 理解如下图\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401002822969.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center)![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401002903543.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center)\n## 附上那位博主的代码\n自己加了一些有没有的注释帮助理解\n\n```py\nimport codecs\nimport random\nimport math\nimport numpy as np\nimport copy\nimport time\n\nentity2id = {}\nrelation2id = {}\n\n\ndef data_loader(file):\n    file1 = file + \"train.txt\"  # entity entity relation\n    file2 = file + \"entity2id.txt\"\n    file3 = file + \"relation2id.txt\"\n\n    with open(file2, 'r') as f1, open(file3, 'r') as f2:\n        # 逐行读取\n        lines1 = f1.readlines()\n        lines2 = f2.readlines()\n        for line in lines1:\n            # 删除空白字符，并以'\\t'为界划分为list\n            line = line.strip().split('\\t')\n            if len(line) != 2:\n                continue\n            # 将符合标准的数据以dict形式写入entity2id\n            entity2id[line[0]] = line[1]\n\n        # 同上\n        for line in lines2:\n            line = line.strip().split('\\t')\n            if len(line) != 2:\n                continue\n            relation2id[line[0]] = line[1]\n\n    # 创建集合与list\n    entity_set = set()\n    relation_set = set()\n    triple_list = []\n\n    # codecs.open自动转码，打开test\n    with codecs.open(file1, 'r') as f:\n        content = f.readlines()\n        for line in content:\n            triple = line.strip().split(\"\\t\")\n            if len(triple) != 3:\n                continue\n\n            # 三元组,triple[0]和triple[1]均为entity,在entity2id中均能找到对应的编码写入_\n            h_ = entity2id[triple[0]]\n            t_ = entity2id[triple[1]]\n            r_ = relation2id[triple[2]]\n\n            # triple list是三元素id的list\n            triple_list.append([h_, t_, r_])\n\n            # 三个集合均为编码id的集合\n            entity_set.add(h_)\n            entity_set.add(t_)\n            relation_set.add(r_)\n\n    return entity_set, relation_set, triple_list\n\n\n# loss function的两个d\ndef distanceL2(h, r, t):\n    # 为方便求梯度，去掉sqrt\n    return np.sum(np.square(h + r - t))\n\n\ndef distanceL1(h, r, t):\n    return np.sum(np.fabs(h + r - t))\n\n\n# transE类\nclass TransE:\n    def __init__(self, entity_set, relation_set, triple_list,\n                 embedding_dim=100, learning_rate=0.01, margin=1, L1=True):\n        self.embedding_dim = embedding_dim\n        self.learning_rate = learning_rate\n        self.margin = margin\n        self.entity = entity_set\n        self.relation = relation_set\n        self.triple_list = triple_list\n        self.L1 = L1\n\n        self.loss = 0\n\n    # 初始化每个向量（随机这里是均匀分布）\n    def emb_initialize(self):\n        relation_dict = {}\n        entity_dict = {}\n\n        # 读取relation_set的每一行\n        for relation in self.relation:\n            # 均匀分布\n            # array(50),-6/sqrt(50) - 6/sqrt(50)\n            r_emb_temp = np.random.uniform(-6 / math.sqrt(self.embedding_dim),\n                                           6 / math.sqrt(self.embedding_dim),\n                                           self.embedding_dim)\n            # 每一个relation encode\n            relation_dict[relation] = r_emb_temp / np.linalg.norm(r_emb_temp, ord=2)\n\n        for entity in self.entity:\n            e_emb_temp = np.random.uniform(-6 / math.sqrt(self.embedding_dim),\n                                           6 / math.sqrt(self.embedding_dim),\n                                           self.embedding_dim)\n            entity_dict[entity] = e_emb_temp / np.linalg.norm(e_emb_temp, ord=2)\n\n        # 向量赋给relation和entity\n        self.relation = relation_dict\n        self.entity = entity_dict\n\n    def train(self, epochs):\n        # 设置batch size\n        nbatches = 400\n        batch_size = len(self.triple_list) // nbatches\n        print(\"batch size: \", batch_size)\n        for epoch in range(epochs):\n            start = time.time()\n            self.loss = 0\n\n            # SGD\n            for k in range(nbatches):\n                # Sbatch:list(batch_size)   随机不重复抽样\n                Sbatch = random.sample(self.triple_list, batch_size)\n                Tbatch = []\n\n                # negative sampling\n                for triple in Sbatch:\n                    # 每个triple选3个负样例\n                    # for i in range(3):\n                    corrupted_triple = self.Corrupt(triple)\n                    # Tbatch中存有正例和负例\n                    if (triple, corrupted_triple) not in Tbatch:\n                        Tbatch.append((triple, corrupted_triple))\n                self.update_embeddings(Tbatch)\n\n            end = time.time()\n            print(\"epoch: \", epoch, \"cost time: %s\" % (round((end - start), 3)))\n            print(\"loss: \", self.loss)\n\n            # 保存临时结果\n            if epoch % 20 == 0:\n                with codecs.open(\"entity_temp\", \"w\") as f_e:\n                    for e in self.entity.keys():\n                        f_e.write(e + \"\\t\")\n                        f_e.write(str(list(self.entity[e])))\n                        f_e.write(\"\\n\")\n                with codecs.open(\"relation_temp\", \"w\") as f_r:\n                    for r in self.relation.keys():\n                        f_r.write(r + \"\\t\")\n                        f_r.write(str(list(self.relation[r])))\n                        f_r.write(\"\\n\")\n\n        print(\"写入文件...\")\n        with codecs.open(\"entity_50dim_batch400\", \"w\") as f1:\n            for e in self.entity.keys():\n                f1.write(e + \"\\t\")\n                f1.write(str(list(self.entity[e])))\n                f1.write(\"\\n\")\n\n        with codecs.open(\"relation50dim_batch400\", \"w\") as f2:\n            for r in self.relation.keys():\n                f2.write(r + \"\\t\")\n                f2.write(str(list(self.relation[r])))\n                f2.write(\"\\n\")\n        print(\"写入完成\")\n\n    # change head entity or tail entity\n    def Corrupt(self, triple):\n        corrupted_triple = copy.deepcopy(triple)\n        # 随机替换head\\tail\n        seed = random.random()\n        if seed > 0.5:\n            # 替换head\n            rand_head = triple[0]\n            while rand_head == triple[0]:\n                rand_head = random.sample(self.entity.keys(), 1)[0]\n            corrupted_triple[0] = rand_head\n\n        else:\n            # 替换tail\n            rand_tail = triple[1]\n            while rand_tail == triple[1]:\n                rand_tail = random.sample(self.entity.keys(), 1)[0]\n            corrupted_triple[1] = rand_tail\n        return corrupted_triple\n\n    def update_embeddings(self, Tbatch):\n        copy_entity = copy.deepcopy(self.entity)\n        copy_relation = copy.deepcopy(self.relation)\n\n        for triple, corrupted_triple in Tbatch:\n            # 取copy里的vector累积更新\n            h_correct_update = copy_entity[triple[0]]\n            t_correct_update = copy_entity[triple[1]]\n            relation_update = copy_relation[triple[2]]\n\n            h_corrupt_update = copy_entity[corrupted_triple[0]]\n            t_corrupt_update = copy_entity[corrupted_triple[1]]\n\n            # 取原始的vector计算梯度\n            h_correct = self.entity[triple[0]]\n            t_correct = self.entity[triple[1]]\n            relation = self.relation[triple[2]]\n\n            h_corrupt = self.entity[corrupted_triple[0]]\n            t_corrupt = self.entity[corrupted_triple[1]]\n\n            if self.L1:\n                dist_correct = distanceL1(h_correct, relation, t_correct)\n                dist_corrupt = distanceL1(h_corrupt, relation, t_corrupt)\n            else:\n                dist_correct = distanceL2(h_correct, relation, t_correct)\n                dist_corrupt = distanceL2(h_corrupt, relation, t_corrupt)\n\n            err = self.hinge_loss(dist_correct, dist_corrupt)\n\n            # delta d < -m 时跳出\n            if err > 0:\n                self.loss += err\n\n                grad_pos = 2 * (h_correct + relation - t_correct)\n                grad_neg = 2 * (h_corrupt + relation - t_corrupt)\n\n                if self.L1:\n                    for i in range(len(grad_pos)):\n                        if grad_pos[i] > 0:\n                            grad_pos[i] = 1\n                        else:\n                            grad_pos[i] = -1\n\n                    for i in range(len(grad_neg)):\n                        if grad_neg[i] > 0:\n                            grad_neg[i] = 1\n                        else:\n                            grad_neg[i] = -1\n\n                # head系数为正，减梯度；tail系数为负，加梯度\n                h_correct_update -= self.learning_rate * grad_pos\n                t_correct_update -= (-1) * self.learning_rate * grad_pos\n\n                # corrupt项整体为负，因此符号与correct相反\n                if triple[0] == corrupted_triple[0]:  # 若替换的是尾实体，则头实体更新两次\n                    h_correct_update -= (-1) * self.learning_rate * grad_neg\n                    t_corrupt_update -= self.learning_rate * grad_neg\n\n                elif triple[1] == corrupted_triple[1]:  # 若替换的是头实体，则尾实体更新两次\n                    h_corrupt_update -= (-1) * self.learning_rate * grad_neg\n                    t_correct_update -= self.learning_rate * grad_neg\n\n                # relation更新两次\n                relation_update -= self.learning_rate * grad_pos\n                relation_update -= (-1) * self.learning_rate * grad_neg\n\n        # batch norm\n        for i in copy_entity.keys():\n            copy_entity[i] /= np.linalg.norm(copy_entity[i])\n        for i in copy_relation.keys():\n            copy_relation[i] /= np.linalg.norm(copy_relation[i])\n\n        # 达到批量更新的目的\n        self.entity = copy_entity\n        self.relation = copy_relation\n\n    def hinge_loss(self, dist_correct, dist_corrupt):\n        return max(0, dist_correct - dist_corrupt + self.margin)\n\n\nif __name__ == '__main__':\n    file1 = \"./FB15k/\"\n    entity_set, relation_set, triple_list = data_loader(file1)\n    print(\"load file...\")\n    print(\"Complete load. entity : %d , relation : %d , triple : %d\" % (\n        len(entity_set), len(relation_set), len(triple_list)))\n\n    transE = TransE(entity_set, relation_set, triple_list, embedding_dim=50, learning_rate=0.01, margin=1, L1=True)\n    transE.emb_initialize()\n    transE.train(epochs=1001)\n\n```","slug":"firstPage-md","published":1,"updated":"2021-07-13T17:13:17.606Z","_id":"ckr2agn4l0000zuoehjwa0jva","comments":1,"layout":"post","photos":[],"link":"","content":"<p>（依附于博主yuanwyue代码<a href=\"https://blog.csdn.net/shunaoxi2313/article/details/89766467%EF%BC%89\">https://blog.csdn.net/shunaoxi2313/article/details/89766467）</a></p>\n<h2 id=\"理解如下图\"><a href=\"#理解如下图\" class=\"headerlink\" title=\"理解如下图\"></a>理解如下图</h2><p><img src=\"https://img-blog.csdnimg.cn/20210401002822969.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><img src=\"https://img-blog.csdnimg.cn/20210401002903543.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"附上那位博主的代码\"><a href=\"#附上那位博主的代码\" class=\"headerlink\" title=\"附上那位博主的代码\"></a>附上那位博主的代码</h2><p>自己加了一些有没有的注释帮助理解</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br><span class=\"line\">262</span><br><span class=\"line\">263</span><br><span class=\"line\">264</span><br><span class=\"line\">265</span><br><span class=\"line\">266</span><br><span class=\"line\">267</span><br><span class=\"line\">268</span><br><span class=\"line\">269</span><br><span class=\"line\">270</span><br><span class=\"line\">271</span><br><span class=\"line\">272</span><br><span class=\"line\">273</span><br><span class=\"line\">274</span><br><span class=\"line\">275</span><br><span class=\"line\">276</span><br><span class=\"line\">277</span><br><span class=\"line\">278</span><br><span class=\"line\">279</span><br><span class=\"line\">280</span><br><span class=\"line\">281</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> codecs</span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> copy</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"></span><br><span class=\"line\">entity2id = &#123;&#125;</span><br><span class=\"line\">relation2id = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">data_loader</span>(<span class=\"params\">file</span>):</span></span><br><span class=\"line\">    file1 = file + <span class=\"string\">&quot;train.txt&quot;</span>  <span class=\"comment\"># entity entity relation</span></span><br><span class=\"line\">    file2 = file + <span class=\"string\">&quot;entity2id.txt&quot;</span></span><br><span class=\"line\">    file3 = file + <span class=\"string\">&quot;relation2id.txt&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(file2, <span class=\"string\">&#x27;r&#x27;</span>) <span class=\"keyword\">as</span> f1, <span class=\"built_in\">open</span>(file3, <span class=\"string\">&#x27;r&#x27;</span>) <span class=\"keyword\">as</span> f2:</span><br><span class=\"line\">        <span class=\"comment\"># 逐行读取</span></span><br><span class=\"line\">        lines1 = f1.readlines()</span><br><span class=\"line\">        lines2 = f2.readlines()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines1:</span><br><span class=\"line\">            <span class=\"comment\"># 删除空白字符，并以&#x27;\\t&#x27;为界划分为list</span></span><br><span class=\"line\">            line = line.strip().split(<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(line) != <span class=\"number\">2</span>:</span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            <span class=\"comment\"># 将符合标准的数据以dict形式写入entity2id</span></span><br><span class=\"line\">            entity2id[line[<span class=\"number\">0</span>]] = line[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 同上</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines2:</span><br><span class=\"line\">            line = line.strip().split(<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(line) != <span class=\"number\">2</span>:</span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            relation2id[line[<span class=\"number\">0</span>]] = line[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 创建集合与list</span></span><br><span class=\"line\">    entity_set = <span class=\"built_in\">set</span>()</span><br><span class=\"line\">    relation_set = <span class=\"built_in\">set</span>()</span><br><span class=\"line\">    triple_list = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># codecs.open自动转码，打开test</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> codecs.<span class=\"built_in\">open</span>(file1, <span class=\"string\">&#x27;r&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        content = f.readlines()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> content:</span><br><span class=\"line\">            triple = line.strip().split(<span class=\"string\">&quot;\\t&quot;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(triple) != <span class=\"number\">3</span>:</span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># 三元组,triple[0]和triple[1]均为entity,在entity2id中均能找到对应的编码写入_</span></span><br><span class=\"line\">            h_ = entity2id[triple[<span class=\"number\">0</span>]]</span><br><span class=\"line\">            t_ = entity2id[triple[<span class=\"number\">1</span>]]</span><br><span class=\"line\">            r_ = relation2id[triple[<span class=\"number\">2</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># triple list是三元素id的list</span></span><br><span class=\"line\">            triple_list.append([h_, t_, r_])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># 三个集合均为编码id的集合</span></span><br><span class=\"line\">            entity_set.add(h_)</span><br><span class=\"line\">            entity_set.add(t_)</span><br><span class=\"line\">            relation_set.add(r_)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> entity_set, relation_set, triple_list</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># loss function的两个d</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">distanceL2</span>(<span class=\"params\">h, r, t</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 为方便求梯度，去掉sqrt</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.<span class=\"built_in\">sum</span>(np.square(h + r - t))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">distanceL1</span>(<span class=\"params\">h, r, t</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.<span class=\"built_in\">sum</span>(np.fabs(h + r - t))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># transE类</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TransE</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, entity_set, relation_set, triple_list,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                 embedding_dim=<span class=\"number\">100</span>, learning_rate=<span class=\"number\">0.01</span>, margin=<span class=\"number\">1</span>, L1=<span class=\"literal\">True</span></span>):</span></span><br><span class=\"line\">        self.embedding_dim = embedding_dim</span><br><span class=\"line\">        self.learning_rate = learning_rate</span><br><span class=\"line\">        self.margin = margin</span><br><span class=\"line\">        self.entity = entity_set</span><br><span class=\"line\">        self.relation = relation_set</span><br><span class=\"line\">        self.triple_list = triple_list</span><br><span class=\"line\">        self.L1 = L1</span><br><span class=\"line\"></span><br><span class=\"line\">        self.loss = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 初始化每个向量（随机这里是均匀分布）</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">emb_initialize</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        relation_dict = &#123;&#125;</span><br><span class=\"line\">        entity_dict = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 读取relation_set的每一行</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> relation <span class=\"keyword\">in</span> self.relation:</span><br><span class=\"line\">            <span class=\"comment\"># 均匀分布</span></span><br><span class=\"line\">            <span class=\"comment\"># array(50),-6/sqrt(50) - 6/sqrt(50)</span></span><br><span class=\"line\">            r_emb_temp = np.random.uniform(-<span class=\"number\">6</span> / math.sqrt(self.embedding_dim),</span><br><span class=\"line\">                                           <span class=\"number\">6</span> / math.sqrt(self.embedding_dim),</span><br><span class=\"line\">                                           self.embedding_dim)</span><br><span class=\"line\">            <span class=\"comment\"># 每一个relation encode</span></span><br><span class=\"line\">            relation_dict[relation] = r_emb_temp / np.linalg.norm(r_emb_temp, <span class=\"built_in\">ord</span>=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> entity <span class=\"keyword\">in</span> self.entity:</span><br><span class=\"line\">            e_emb_temp = np.random.uniform(-<span class=\"number\">6</span> / math.sqrt(self.embedding_dim),</span><br><span class=\"line\">                                           <span class=\"number\">6</span> / math.sqrt(self.embedding_dim),</span><br><span class=\"line\">                                           self.embedding_dim)</span><br><span class=\"line\">            entity_dict[entity] = e_emb_temp / np.linalg.norm(e_emb_temp, <span class=\"built_in\">ord</span>=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 向量赋给relation和entity</span></span><br><span class=\"line\">        self.relation = relation_dict</span><br><span class=\"line\">        self.entity = entity_dict</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span>(<span class=\"params\">self, epochs</span>):</span></span><br><span class=\"line\">        <span class=\"comment\"># 设置batch size</span></span><br><span class=\"line\">        nbatches = <span class=\"number\">400</span></span><br><span class=\"line\">        batch_size = <span class=\"built_in\">len</span>(self.triple_list) // nbatches</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;batch size: &quot;</span>, batch_size)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\">            start = time.time()</span><br><span class=\"line\">            self.loss = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># SGD</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(nbatches):</span><br><span class=\"line\">                <span class=\"comment\"># Sbatch:list(batch_size)   随机不重复抽样</span></span><br><span class=\"line\">                Sbatch = random.sample(self.triple_list, batch_size)</span><br><span class=\"line\">                Tbatch = []</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># negative sampling</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> triple <span class=\"keyword\">in</span> Sbatch:</span><br><span class=\"line\">                    <span class=\"comment\"># 每个triple选3个负样例</span></span><br><span class=\"line\">                    <span class=\"comment\"># for i in range(3):</span></span><br><span class=\"line\">                    corrupted_triple = self.Corrupt(triple)</span><br><span class=\"line\">                    <span class=\"comment\"># Tbatch中存有正例和负例</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (triple, corrupted_triple) <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> Tbatch:</span><br><span class=\"line\">                        Tbatch.append((triple, corrupted_triple))</span><br><span class=\"line\">                self.update_embeddings(Tbatch)</span><br><span class=\"line\"></span><br><span class=\"line\">            end = time.time()</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&quot;epoch: &quot;</span>, epoch, <span class=\"string\">&quot;cost time: %s&quot;</span> % (<span class=\"built_in\">round</span>((end - start), <span class=\"number\">3</span>)))</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&quot;loss: &quot;</span>, self.loss)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># 保存临时结果</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> epoch % <span class=\"number\">20</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">with</span> codecs.<span class=\"built_in\">open</span>(<span class=\"string\">&quot;entity_temp&quot;</span>, <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f_e:</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> self.entity.keys():</span><br><span class=\"line\">                        f_e.write(e + <span class=\"string\">&quot;\\t&quot;</span>)</span><br><span class=\"line\">                        f_e.write(<span class=\"built_in\">str</span>(<span class=\"built_in\">list</span>(self.entity[e])))</span><br><span class=\"line\">                        f_e.write(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">with</span> codecs.<span class=\"built_in\">open</span>(<span class=\"string\">&quot;relation_temp&quot;</span>, <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f_r:</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> self.relation.keys():</span><br><span class=\"line\">                        f_r.write(r + <span class=\"string\">&quot;\\t&quot;</span>)</span><br><span class=\"line\">                        f_r.write(<span class=\"built_in\">str</span>(<span class=\"built_in\">list</span>(self.relation[r])))</span><br><span class=\"line\">                        f_r.write(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;写入文件...&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">with</span> codecs.<span class=\"built_in\">open</span>(<span class=\"string\">&quot;entity_50dim_batch400&quot;</span>, <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f1:</span><br><span class=\"line\">            <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> self.entity.keys():</span><br><span class=\"line\">                f1.write(e + <span class=\"string\">&quot;\\t&quot;</span>)</span><br><span class=\"line\">                f1.write(<span class=\"built_in\">str</span>(<span class=\"built_in\">list</span>(self.entity[e])))</span><br><span class=\"line\">                f1.write(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">with</span> codecs.<span class=\"built_in\">open</span>(<span class=\"string\">&quot;relation50dim_batch400&quot;</span>, <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f2:</span><br><span class=\"line\">            <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> self.relation.keys():</span><br><span class=\"line\">                f2.write(r + <span class=\"string\">&quot;\\t&quot;</span>)</span><br><span class=\"line\">                f2.write(<span class=\"built_in\">str</span>(<span class=\"built_in\">list</span>(self.relation[r])))</span><br><span class=\"line\">                f2.write(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;写入完成&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># change head entity or tail entity</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Corrupt</span>(<span class=\"params\">self, triple</span>):</span></span><br><span class=\"line\">        corrupted_triple = copy.deepcopy(triple)</span><br><span class=\"line\">        <span class=\"comment\"># 随机替换head\\tail</span></span><br><span class=\"line\">        seed = random.random()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> seed &gt; <span class=\"number\">0.5</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 替换head</span></span><br><span class=\"line\">            rand_head = triple[<span class=\"number\">0</span>]</span><br><span class=\"line\">            <span class=\"keyword\">while</span> rand_head == triple[<span class=\"number\">0</span>]:</span><br><span class=\"line\">                rand_head = random.sample(self.entity.keys(), <span class=\"number\">1</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">            corrupted_triple[<span class=\"number\">0</span>] = rand_head</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 替换tail</span></span><br><span class=\"line\">            rand_tail = triple[<span class=\"number\">1</span>]</span><br><span class=\"line\">            <span class=\"keyword\">while</span> rand_tail == triple[<span class=\"number\">1</span>]:</span><br><span class=\"line\">                rand_tail = random.sample(self.entity.keys(), <span class=\"number\">1</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">            corrupted_triple[<span class=\"number\">1</span>] = rand_tail</span><br><span class=\"line\">        <span class=\"keyword\">return</span> corrupted_triple</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_embeddings</span>(<span class=\"params\">self, Tbatch</span>):</span></span><br><span class=\"line\">        copy_entity = copy.deepcopy(self.entity)</span><br><span class=\"line\">        copy_relation = copy.deepcopy(self.relation)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> triple, corrupted_triple <span class=\"keyword\">in</span> Tbatch:</span><br><span class=\"line\">            <span class=\"comment\"># 取copy里的vector累积更新</span></span><br><span class=\"line\">            h_correct_update = copy_entity[triple[<span class=\"number\">0</span>]]</span><br><span class=\"line\">            t_correct_update = copy_entity[triple[<span class=\"number\">1</span>]]</span><br><span class=\"line\">            relation_update = copy_relation[triple[<span class=\"number\">2</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">            h_corrupt_update = copy_entity[corrupted_triple[<span class=\"number\">0</span>]]</span><br><span class=\"line\">            t_corrupt_update = copy_entity[corrupted_triple[<span class=\"number\">1</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># 取原始的vector计算梯度</span></span><br><span class=\"line\">            h_correct = self.entity[triple[<span class=\"number\">0</span>]]</span><br><span class=\"line\">            t_correct = self.entity[triple[<span class=\"number\">1</span>]]</span><br><span class=\"line\">            relation = self.relation[triple[<span class=\"number\">2</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">            h_corrupt = self.entity[corrupted_triple[<span class=\"number\">0</span>]]</span><br><span class=\"line\">            t_corrupt = self.entity[corrupted_triple[<span class=\"number\">1</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.L1:</span><br><span class=\"line\">                dist_correct = distanceL1(h_correct, relation, t_correct)</span><br><span class=\"line\">                dist_corrupt = distanceL1(h_corrupt, relation, t_corrupt)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                dist_correct = distanceL2(h_correct, relation, t_correct)</span><br><span class=\"line\">                dist_corrupt = distanceL2(h_corrupt, relation, t_corrupt)</span><br><span class=\"line\"></span><br><span class=\"line\">            err = self.hinge_loss(dist_correct, dist_corrupt)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># delta d &lt; -m 时跳出</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> err &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                self.loss += err</span><br><span class=\"line\"></span><br><span class=\"line\">                grad_pos = <span class=\"number\">2</span> * (h_correct + relation - t_correct)</span><br><span class=\"line\">                grad_neg = <span class=\"number\">2</span> * (h_corrupt + relation - t_corrupt)</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> self.L1:</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(grad_pos)):</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> grad_pos[i] &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                            grad_pos[i] = <span class=\"number\">1</span></span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            grad_pos[i] = -<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(grad_neg)):</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> grad_neg[i] &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                            grad_neg[i] = <span class=\"number\">1</span></span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            grad_neg[i] = -<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># head系数为正，减梯度；tail系数为负，加梯度</span></span><br><span class=\"line\">                h_correct_update -= self.learning_rate * grad_pos</span><br><span class=\"line\">                t_correct_update -= (-<span class=\"number\">1</span>) * self.learning_rate * grad_pos</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># corrupt项整体为负，因此符号与correct相反</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> triple[<span class=\"number\">0</span>] == corrupted_triple[<span class=\"number\">0</span>]:  <span class=\"comment\"># 若替换的是尾实体，则头实体更新两次</span></span><br><span class=\"line\">                    h_correct_update -= (-<span class=\"number\">1</span>) * self.learning_rate * grad_neg</span><br><span class=\"line\">                    t_corrupt_update -= self.learning_rate * grad_neg</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">elif</span> triple[<span class=\"number\">1</span>] == corrupted_triple[<span class=\"number\">1</span>]:  <span class=\"comment\"># 若替换的是头实体，则尾实体更新两次</span></span><br><span class=\"line\">                    h_corrupt_update -= (-<span class=\"number\">1</span>) * self.learning_rate * grad_neg</span><br><span class=\"line\">                    t_correct_update -= self.learning_rate * grad_neg</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># relation更新两次</span></span><br><span class=\"line\">                relation_update -= self.learning_rate * grad_pos</span><br><span class=\"line\">                relation_update -= (-<span class=\"number\">1</span>) * self.learning_rate * grad_neg</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># batch norm</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> copy_entity.keys():</span><br><span class=\"line\">            copy_entity[i] /= np.linalg.norm(copy_entity[i])</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> copy_relation.keys():</span><br><span class=\"line\">            copy_relation[i] /= np.linalg.norm(copy_relation[i])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 达到批量更新的目的</span></span><br><span class=\"line\">        self.entity = copy_entity</span><br><span class=\"line\">        self.relation = copy_relation</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hinge_loss</span>(<span class=\"params\">self, dist_correct, dist_corrupt</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">max</span>(<span class=\"number\">0</span>, dist_correct - dist_corrupt + self.margin)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    file1 = <span class=\"string\">&quot;./FB15k/&quot;</span></span><br><span class=\"line\">    entity_set, relation_set, triple_list = data_loader(file1)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;load file...&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Complete load. entity : %d , relation : %d , triple : %d&quot;</span> % (</span><br><span class=\"line\">        <span class=\"built_in\">len</span>(entity_set), <span class=\"built_in\">len</span>(relation_set), <span class=\"built_in\">len</span>(triple_list)))</span><br><span class=\"line\"></span><br><span class=\"line\">    transE = TransE(entity_set, relation_set, triple_list, embedding_dim=<span class=\"number\">50</span>, learning_rate=<span class=\"number\">0.01</span>, margin=<span class=\"number\">1</span>, L1=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    transE.emb_initialize()</span><br><span class=\"line\">    transE.train(epochs=<span class=\"number\">1001</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>（依附于博主yuanwyue代码<a href=\"https://blog.csdn.net/shunaoxi2313/article/details/89766467%EF%BC%89\">https://blog.csdn.net/shunaoxi2313/article/details/89766467）</a></p>\n<h2 id=\"理解如下图\"><a href=\"#理解如下图\" class=\"headerlink\" title=\"理解如下图\"></a>理解如下图</h2><p><img src=\"https://img-blog.csdnimg.cn/20210401002822969.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><img src=\"https://img-blog.csdnimg.cn/20210401002903543.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"附上那位博主的代码\"><a href=\"#附上那位博主的代码\" class=\"headerlink\" title=\"附上那位博主的代码\"></a>附上那位博主的代码</h2><p>自己加了一些有没有的注释帮助理解</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br><span class=\"line\">262</span><br><span class=\"line\">263</span><br><span class=\"line\">264</span><br><span class=\"line\">265</span><br><span class=\"line\">266</span><br><span class=\"line\">267</span><br><span class=\"line\">268</span><br><span class=\"line\">269</span><br><span class=\"line\">270</span><br><span class=\"line\">271</span><br><span class=\"line\">272</span><br><span class=\"line\">273</span><br><span class=\"line\">274</span><br><span class=\"line\">275</span><br><span class=\"line\">276</span><br><span class=\"line\">277</span><br><span class=\"line\">278</span><br><span class=\"line\">279</span><br><span class=\"line\">280</span><br><span class=\"line\">281</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> codecs</span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> copy</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"></span><br><span class=\"line\">entity2id = &#123;&#125;</span><br><span class=\"line\">relation2id = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">data_loader</span>(<span class=\"params\">file</span>):</span></span><br><span class=\"line\">    file1 = file + <span class=\"string\">&quot;train.txt&quot;</span>  <span class=\"comment\"># entity entity relation</span></span><br><span class=\"line\">    file2 = file + <span class=\"string\">&quot;entity2id.txt&quot;</span></span><br><span class=\"line\">    file3 = file + <span class=\"string\">&quot;relation2id.txt&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(file2, <span class=\"string\">&#x27;r&#x27;</span>) <span class=\"keyword\">as</span> f1, <span class=\"built_in\">open</span>(file3, <span class=\"string\">&#x27;r&#x27;</span>) <span class=\"keyword\">as</span> f2:</span><br><span class=\"line\">        <span class=\"comment\"># 逐行读取</span></span><br><span class=\"line\">        lines1 = f1.readlines()</span><br><span class=\"line\">        lines2 = f2.readlines()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines1:</span><br><span class=\"line\">            <span class=\"comment\"># 删除空白字符，并以&#x27;\\t&#x27;为界划分为list</span></span><br><span class=\"line\">            line = line.strip().split(<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(line) != <span class=\"number\">2</span>:</span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            <span class=\"comment\"># 将符合标准的数据以dict形式写入entity2id</span></span><br><span class=\"line\">            entity2id[line[<span class=\"number\">0</span>]] = line[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 同上</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines2:</span><br><span class=\"line\">            line = line.strip().split(<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(line) != <span class=\"number\">2</span>:</span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            relation2id[line[<span class=\"number\">0</span>]] = line[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 创建集合与list</span></span><br><span class=\"line\">    entity_set = <span class=\"built_in\">set</span>()</span><br><span class=\"line\">    relation_set = <span class=\"built_in\">set</span>()</span><br><span class=\"line\">    triple_list = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># codecs.open自动转码，打开test</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> codecs.<span class=\"built_in\">open</span>(file1, <span class=\"string\">&#x27;r&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        content = f.readlines()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> content:</span><br><span class=\"line\">            triple = line.strip().split(<span class=\"string\">&quot;\\t&quot;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(triple) != <span class=\"number\">3</span>:</span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># 三元组,triple[0]和triple[1]均为entity,在entity2id中均能找到对应的编码写入_</span></span><br><span class=\"line\">            h_ = entity2id[triple[<span class=\"number\">0</span>]]</span><br><span class=\"line\">            t_ = entity2id[triple[<span class=\"number\">1</span>]]</span><br><span class=\"line\">            r_ = relation2id[triple[<span class=\"number\">2</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># triple list是三元素id的list</span></span><br><span class=\"line\">            triple_list.append([h_, t_, r_])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># 三个集合均为编码id的集合</span></span><br><span class=\"line\">            entity_set.add(h_)</span><br><span class=\"line\">            entity_set.add(t_)</span><br><span class=\"line\">            relation_set.add(r_)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> entity_set, relation_set, triple_list</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># loss function的两个d</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">distanceL2</span>(<span class=\"params\">h, r, t</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 为方便求梯度，去掉sqrt</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.<span class=\"built_in\">sum</span>(np.square(h + r - t))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">distanceL1</span>(<span class=\"params\">h, r, t</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.<span class=\"built_in\">sum</span>(np.fabs(h + r - t))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># transE类</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TransE</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, entity_set, relation_set, triple_list,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">                 embedding_dim=<span class=\"number\">100</span>, learning_rate=<span class=\"number\">0.01</span>, margin=<span class=\"number\">1</span>, L1=<span class=\"literal\">True</span></span>):</span></span><br><span class=\"line\">        self.embedding_dim = embedding_dim</span><br><span class=\"line\">        self.learning_rate = learning_rate</span><br><span class=\"line\">        self.margin = margin</span><br><span class=\"line\">        self.entity = entity_set</span><br><span class=\"line\">        self.relation = relation_set</span><br><span class=\"line\">        self.triple_list = triple_list</span><br><span class=\"line\">        self.L1 = L1</span><br><span class=\"line\"></span><br><span class=\"line\">        self.loss = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 初始化每个向量（随机这里是均匀分布）</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">emb_initialize</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        relation_dict = &#123;&#125;</span><br><span class=\"line\">        entity_dict = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 读取relation_set的每一行</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> relation <span class=\"keyword\">in</span> self.relation:</span><br><span class=\"line\">            <span class=\"comment\"># 均匀分布</span></span><br><span class=\"line\">            <span class=\"comment\"># array(50),-6/sqrt(50) - 6/sqrt(50)</span></span><br><span class=\"line\">            r_emb_temp = np.random.uniform(-<span class=\"number\">6</span> / math.sqrt(self.embedding_dim),</span><br><span class=\"line\">                                           <span class=\"number\">6</span> / math.sqrt(self.embedding_dim),</span><br><span class=\"line\">                                           self.embedding_dim)</span><br><span class=\"line\">            <span class=\"comment\"># 每一个relation encode</span></span><br><span class=\"line\">            relation_dict[relation] = r_emb_temp / np.linalg.norm(r_emb_temp, <span class=\"built_in\">ord</span>=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> entity <span class=\"keyword\">in</span> self.entity:</span><br><span class=\"line\">            e_emb_temp = np.random.uniform(-<span class=\"number\">6</span> / math.sqrt(self.embedding_dim),</span><br><span class=\"line\">                                           <span class=\"number\">6</span> / math.sqrt(self.embedding_dim),</span><br><span class=\"line\">                                           self.embedding_dim)</span><br><span class=\"line\">            entity_dict[entity] = e_emb_temp / np.linalg.norm(e_emb_temp, <span class=\"built_in\">ord</span>=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 向量赋给relation和entity</span></span><br><span class=\"line\">        self.relation = relation_dict</span><br><span class=\"line\">        self.entity = entity_dict</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span>(<span class=\"params\">self, epochs</span>):</span></span><br><span class=\"line\">        <span class=\"comment\"># 设置batch size</span></span><br><span class=\"line\">        nbatches = <span class=\"number\">400</span></span><br><span class=\"line\">        batch_size = <span class=\"built_in\">len</span>(self.triple_list) // nbatches</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;batch size: &quot;</span>, batch_size)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\">            start = time.time()</span><br><span class=\"line\">            self.loss = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># SGD</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(nbatches):</span><br><span class=\"line\">                <span class=\"comment\"># Sbatch:list(batch_size)   随机不重复抽样</span></span><br><span class=\"line\">                Sbatch = random.sample(self.triple_list, batch_size)</span><br><span class=\"line\">                Tbatch = []</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># negative sampling</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> triple <span class=\"keyword\">in</span> Sbatch:</span><br><span class=\"line\">                    <span class=\"comment\"># 每个triple选3个负样例</span></span><br><span class=\"line\">                    <span class=\"comment\"># for i in range(3):</span></span><br><span class=\"line\">                    corrupted_triple = self.Corrupt(triple)</span><br><span class=\"line\">                    <span class=\"comment\"># Tbatch中存有正例和负例</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (triple, corrupted_triple) <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> Tbatch:</span><br><span class=\"line\">                        Tbatch.append((triple, corrupted_triple))</span><br><span class=\"line\">                self.update_embeddings(Tbatch)</span><br><span class=\"line\"></span><br><span class=\"line\">            end = time.time()</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&quot;epoch: &quot;</span>, epoch, <span class=\"string\">&quot;cost time: %s&quot;</span> % (<span class=\"built_in\">round</span>((end - start), <span class=\"number\">3</span>)))</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&quot;loss: &quot;</span>, self.loss)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># 保存临时结果</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> epoch % <span class=\"number\">20</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">with</span> codecs.<span class=\"built_in\">open</span>(<span class=\"string\">&quot;entity_temp&quot;</span>, <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f_e:</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> self.entity.keys():</span><br><span class=\"line\">                        f_e.write(e + <span class=\"string\">&quot;\\t&quot;</span>)</span><br><span class=\"line\">                        f_e.write(<span class=\"built_in\">str</span>(<span class=\"built_in\">list</span>(self.entity[e])))</span><br><span class=\"line\">                        f_e.write(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">with</span> codecs.<span class=\"built_in\">open</span>(<span class=\"string\">&quot;relation_temp&quot;</span>, <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f_r:</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> self.relation.keys():</span><br><span class=\"line\">                        f_r.write(r + <span class=\"string\">&quot;\\t&quot;</span>)</span><br><span class=\"line\">                        f_r.write(<span class=\"built_in\">str</span>(<span class=\"built_in\">list</span>(self.relation[r])))</span><br><span class=\"line\">                        f_r.write(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;写入文件...&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">with</span> codecs.<span class=\"built_in\">open</span>(<span class=\"string\">&quot;entity_50dim_batch400&quot;</span>, <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f1:</span><br><span class=\"line\">            <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> self.entity.keys():</span><br><span class=\"line\">                f1.write(e + <span class=\"string\">&quot;\\t&quot;</span>)</span><br><span class=\"line\">                f1.write(<span class=\"built_in\">str</span>(<span class=\"built_in\">list</span>(self.entity[e])))</span><br><span class=\"line\">                f1.write(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">with</span> codecs.<span class=\"built_in\">open</span>(<span class=\"string\">&quot;relation50dim_batch400&quot;</span>, <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f2:</span><br><span class=\"line\">            <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> self.relation.keys():</span><br><span class=\"line\">                f2.write(r + <span class=\"string\">&quot;\\t&quot;</span>)</span><br><span class=\"line\">                f2.write(<span class=\"built_in\">str</span>(<span class=\"built_in\">list</span>(self.relation[r])))</span><br><span class=\"line\">                f2.write(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;写入完成&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># change head entity or tail entity</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Corrupt</span>(<span class=\"params\">self, triple</span>):</span></span><br><span class=\"line\">        corrupted_triple = copy.deepcopy(triple)</span><br><span class=\"line\">        <span class=\"comment\"># 随机替换head\\tail</span></span><br><span class=\"line\">        seed = random.random()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> seed &gt; <span class=\"number\">0.5</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 替换head</span></span><br><span class=\"line\">            rand_head = triple[<span class=\"number\">0</span>]</span><br><span class=\"line\">            <span class=\"keyword\">while</span> rand_head == triple[<span class=\"number\">0</span>]:</span><br><span class=\"line\">                rand_head = random.sample(self.entity.keys(), <span class=\"number\">1</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">            corrupted_triple[<span class=\"number\">0</span>] = rand_head</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 替换tail</span></span><br><span class=\"line\">            rand_tail = triple[<span class=\"number\">1</span>]</span><br><span class=\"line\">            <span class=\"keyword\">while</span> rand_tail == triple[<span class=\"number\">1</span>]:</span><br><span class=\"line\">                rand_tail = random.sample(self.entity.keys(), <span class=\"number\">1</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">            corrupted_triple[<span class=\"number\">1</span>] = rand_tail</span><br><span class=\"line\">        <span class=\"keyword\">return</span> corrupted_triple</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_embeddings</span>(<span class=\"params\">self, Tbatch</span>):</span></span><br><span class=\"line\">        copy_entity = copy.deepcopy(self.entity)</span><br><span class=\"line\">        copy_relation = copy.deepcopy(self.relation)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> triple, corrupted_triple <span class=\"keyword\">in</span> Tbatch:</span><br><span class=\"line\">            <span class=\"comment\"># 取copy里的vector累积更新</span></span><br><span class=\"line\">            h_correct_update = copy_entity[triple[<span class=\"number\">0</span>]]</span><br><span class=\"line\">            t_correct_update = copy_entity[triple[<span class=\"number\">1</span>]]</span><br><span class=\"line\">            relation_update = copy_relation[triple[<span class=\"number\">2</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">            h_corrupt_update = copy_entity[corrupted_triple[<span class=\"number\">0</span>]]</span><br><span class=\"line\">            t_corrupt_update = copy_entity[corrupted_triple[<span class=\"number\">1</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># 取原始的vector计算梯度</span></span><br><span class=\"line\">            h_correct = self.entity[triple[<span class=\"number\">0</span>]]</span><br><span class=\"line\">            t_correct = self.entity[triple[<span class=\"number\">1</span>]]</span><br><span class=\"line\">            relation = self.relation[triple[<span class=\"number\">2</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">            h_corrupt = self.entity[corrupted_triple[<span class=\"number\">0</span>]]</span><br><span class=\"line\">            t_corrupt = self.entity[corrupted_triple[<span class=\"number\">1</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.L1:</span><br><span class=\"line\">                dist_correct = distanceL1(h_correct, relation, t_correct)</span><br><span class=\"line\">                dist_corrupt = distanceL1(h_corrupt, relation, t_corrupt)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                dist_correct = distanceL2(h_correct, relation, t_correct)</span><br><span class=\"line\">                dist_corrupt = distanceL2(h_corrupt, relation, t_corrupt)</span><br><span class=\"line\"></span><br><span class=\"line\">            err = self.hinge_loss(dist_correct, dist_corrupt)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># delta d &lt; -m 时跳出</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> err &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                self.loss += err</span><br><span class=\"line\"></span><br><span class=\"line\">                grad_pos = <span class=\"number\">2</span> * (h_correct + relation - t_correct)</span><br><span class=\"line\">                grad_neg = <span class=\"number\">2</span> * (h_corrupt + relation - t_corrupt)</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> self.L1:</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(grad_pos)):</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> grad_pos[i] &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                            grad_pos[i] = <span class=\"number\">1</span></span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            grad_pos[i] = -<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(grad_neg)):</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> grad_neg[i] &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                            grad_neg[i] = <span class=\"number\">1</span></span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            grad_neg[i] = -<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># head系数为正，减梯度；tail系数为负，加梯度</span></span><br><span class=\"line\">                h_correct_update -= self.learning_rate * grad_pos</span><br><span class=\"line\">                t_correct_update -= (-<span class=\"number\">1</span>) * self.learning_rate * grad_pos</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># corrupt项整体为负，因此符号与correct相反</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> triple[<span class=\"number\">0</span>] == corrupted_triple[<span class=\"number\">0</span>]:  <span class=\"comment\"># 若替换的是尾实体，则头实体更新两次</span></span><br><span class=\"line\">                    h_correct_update -= (-<span class=\"number\">1</span>) * self.learning_rate * grad_neg</span><br><span class=\"line\">                    t_corrupt_update -= self.learning_rate * grad_neg</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">elif</span> triple[<span class=\"number\">1</span>] == corrupted_triple[<span class=\"number\">1</span>]:  <span class=\"comment\"># 若替换的是头实体，则尾实体更新两次</span></span><br><span class=\"line\">                    h_corrupt_update -= (-<span class=\"number\">1</span>) * self.learning_rate * grad_neg</span><br><span class=\"line\">                    t_correct_update -= self.learning_rate * grad_neg</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># relation更新两次</span></span><br><span class=\"line\">                relation_update -= self.learning_rate * grad_pos</span><br><span class=\"line\">                relation_update -= (-<span class=\"number\">1</span>) * self.learning_rate * grad_neg</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># batch norm</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> copy_entity.keys():</span><br><span class=\"line\">            copy_entity[i] /= np.linalg.norm(copy_entity[i])</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> copy_relation.keys():</span><br><span class=\"line\">            copy_relation[i] /= np.linalg.norm(copy_relation[i])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 达到批量更新的目的</span></span><br><span class=\"line\">        self.entity = copy_entity</span><br><span class=\"line\">        self.relation = copy_relation</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hinge_loss</span>(<span class=\"params\">self, dist_correct, dist_corrupt</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">max</span>(<span class=\"number\">0</span>, dist_correct - dist_corrupt + self.margin)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    file1 = <span class=\"string\">&quot;./FB15k/&quot;</span></span><br><span class=\"line\">    entity_set, relation_set, triple_list = data_loader(file1)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;load file...&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Complete load. entity : %d , relation : %d , triple : %d&quot;</span> % (</span><br><span class=\"line\">        <span class=\"built_in\">len</span>(entity_set), <span class=\"built_in\">len</span>(relation_set), <span class=\"built_in\">len</span>(triple_list)))</span><br><span class=\"line\"></span><br><span class=\"line\">    transE = TransE(entity_set, relation_set, triple_list, embedding_dim=<span class=\"number\">50</span>, learning_rate=<span class=\"number\">0.01</span>, margin=<span class=\"number\">1</span>, L1=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    transE.emb_initialize()</span><br><span class=\"line\">    transE.train(epochs=<span class=\"number\">1001</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>"},{"title":"一道快速幂","date":"2021-07-14T05:21:52.000Z","_content":"# Atcoder Regular Contest 113 B-A^B^C\n\nhttps://atcoder.jp/contests/arc113/tasks/arc113_b\n\n## Problem Statement：\n\nGiven positive integers A,B,C, find the digit at the ones place in the decimal notation of <a><img src=\"https://latex.codecogs.com/png.latex?A^{B^C}\" title=\"A^{B^C}\" /></a>.\n\n## Constraints:\n\n<a><img src=\"https://latex.codecogs.com/png.latex?1&space;<=&space;A,B,C&space;<=&space;10^9\" title=\"1 <= A,B,C <= 10^9\" /></a>\nA,B,C are integers.\n\n## Inputs:\n\nInput is given from Standard Input in the following format:\n\n    A B C\n\n## Outputs:\n\nPrint the digit at the one's place in the decimal notation of <a><img src=\"https://latex.codecogs.com/png.latex?A^{B^C}\" title=\"A^{B^C}\" /></a>.\n\n## Sample Input:\n\n    3141592 6535897 9323846\n\n## Sample Output:\n\n    2\n\n## 分析：\n\n数据很大，思路首先是使用快速幂，然后思考一下，输出的是结果%10的结果，而这个结果只与A的个位有关。\n\n然后```A % 10```的幂也是循环的，发现对于0、1、5、6这样的值直接输出就可以，因为不管```B^C```为多少，都不会改变。然后4、9这样值循环大小为2，其余的为4。把循环大小作为```B^C```的模，这样能最好得优化。\n\n**坑**：```B^C```取模后会=0，这种情况要让他们恢复相应的循环的大小，因为A的0次幂肯定是0嘛。。要做这样一个判断。\n\n代码：\n```cpp\n#include<iostream>\n#include<iostream>\n#include<string>\n#include<cstring>\n#include<algorithm>\n#include<cmath>\n#include<vector>\n#include<cstdio>\n\nusing namespace std;\n\nlong long a,b,c;\nlong long A;\n\n\nlong long quick_pow(long long base,long long pow)\n{\n    long long temp = 0;\n    if(A == 4 || A == 9)\n    {\n        temp = 2;\n    }\n    else\n    {\n        temp = 4;\n    }\n    long long ans = 1;\n    while(pow)\n    {\n        if(pow & 1)\n        {\n            ans = ans * base % temp;\n        }\n        base = base * base % temp;//caculate the x^n\n        pow >>= 1;\n    }\n    if(ans == 0)\n    {\n        if(A == 4 || A == 9)\n        {\n            ans = 2;\n        }\n        else\n        {\n            ans = 4;\n        }\n    }\n    return ans;\n}\n\nlong long quick_pow2(long long base, long long p, long long m)\n{\n    long long ans = 1 % m;\n    while(p)\n    {\n        if(p & 1)\n        {\n            ans = ans * base % m;\n        }\n        base = base * base % m;\n        p >>= 1;\n    }\n    return ans;\n}\n\nint main()\n{\n    cin>>a>>b>>c;\n    A = a % 10;\n    if(A == 0 || A == 5 || A == 6 || A == 1)\n    {\n        cout<<A<<endl;\n    }\n    else\n    {\n        long long p = quick_pow(b, c);\n        cout<<quick_pow2(a, p, 10)<<endl;\n    }\n    return 0;\n}\n```","source":"_posts/acm1-md.md","raw":"---\ntitle: 一道快速幂\ndate: 2021-07-14 13:21:52\ntags: ACM\n---\n# Atcoder Regular Contest 113 B-A^B^C\n\nhttps://atcoder.jp/contests/arc113/tasks/arc113_b\n\n## Problem Statement：\n\nGiven positive integers A,B,C, find the digit at the ones place in the decimal notation of <a><img src=\"https://latex.codecogs.com/png.latex?A^{B^C}\" title=\"A^{B^C}\" /></a>.\n\n## Constraints:\n\n<a><img src=\"https://latex.codecogs.com/png.latex?1&space;<=&space;A,B,C&space;<=&space;10^9\" title=\"1 <= A,B,C <= 10^9\" /></a>\nA,B,C are integers.\n\n## Inputs:\n\nInput is given from Standard Input in the following format:\n\n    A B C\n\n## Outputs:\n\nPrint the digit at the one's place in the decimal notation of <a><img src=\"https://latex.codecogs.com/png.latex?A^{B^C}\" title=\"A^{B^C}\" /></a>.\n\n## Sample Input:\n\n    3141592 6535897 9323846\n\n## Sample Output:\n\n    2\n\n## 分析：\n\n数据很大，思路首先是使用快速幂，然后思考一下，输出的是结果%10的结果，而这个结果只与A的个位有关。\n\n然后```A % 10```的幂也是循环的，发现对于0、1、5、6这样的值直接输出就可以，因为不管```B^C```为多少，都不会改变。然后4、9这样值循环大小为2，其余的为4。把循环大小作为```B^C```的模，这样能最好得优化。\n\n**坑**：```B^C```取模后会=0，这种情况要让他们恢复相应的循环的大小，因为A的0次幂肯定是0嘛。。要做这样一个判断。\n\n代码：\n```cpp\n#include<iostream>\n#include<iostream>\n#include<string>\n#include<cstring>\n#include<algorithm>\n#include<cmath>\n#include<vector>\n#include<cstdio>\n\nusing namespace std;\n\nlong long a,b,c;\nlong long A;\n\n\nlong long quick_pow(long long base,long long pow)\n{\n    long long temp = 0;\n    if(A == 4 || A == 9)\n    {\n        temp = 2;\n    }\n    else\n    {\n        temp = 4;\n    }\n    long long ans = 1;\n    while(pow)\n    {\n        if(pow & 1)\n        {\n            ans = ans * base % temp;\n        }\n        base = base * base % temp;//caculate the x^n\n        pow >>= 1;\n    }\n    if(ans == 0)\n    {\n        if(A == 4 || A == 9)\n        {\n            ans = 2;\n        }\n        else\n        {\n            ans = 4;\n        }\n    }\n    return ans;\n}\n\nlong long quick_pow2(long long base, long long p, long long m)\n{\n    long long ans = 1 % m;\n    while(p)\n    {\n        if(p & 1)\n        {\n            ans = ans * base % m;\n        }\n        base = base * base % m;\n        p >>= 1;\n    }\n    return ans;\n}\n\nint main()\n{\n    cin>>a>>b>>c;\n    A = a % 10;\n    if(A == 0 || A == 5 || A == 6 || A == 1)\n    {\n        cout<<A<<endl;\n    }\n    else\n    {\n        long long p = quick_pow(b, c);\n        cout<<quick_pow2(a, p, 10)<<endl;\n    }\n    return 0;\n}\n```","slug":"acm1-md","published":1,"updated":"2021-07-14T05:29:39.811Z","_id":"ckr31fffa0000wjoe66a24j5l","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Atcoder-Regular-Contest-113-B-A-B-C\"><a href=\"#Atcoder-Regular-Contest-113-B-A-B-C\" class=\"headerlink\" title=\"Atcoder Regular Contest 113 B-A^B^C\"></a>Atcoder Regular Contest 113 B-A^B^C</h1><p><a href=\"https://atcoder.jp/contests/arc113/tasks/arc113_b\">https://atcoder.jp/contests/arc113/tasks/arc113_b</a></p>\n<h2 id=\"Problem-Statement：\"><a href=\"#Problem-Statement：\" class=\"headerlink\" title=\"Problem Statement：\"></a>Problem Statement：</h2><p>Given positive integers A,B,C, find the digit at the ones place in the decimal notation of <a><img src=\"https://latex.codecogs.com/png.latex?A^{B^C}\" title=\"A^{B^C}\" /></a>.</p>\n<h2 id=\"Constraints\"><a href=\"#Constraints\" class=\"headerlink\" title=\"Constraints:\"></a>Constraints:</h2><p><a><img src=\"https://latex.codecogs.com/png.latex?1&space;<=&space;A,B,C&space;<=&space;10^9\" title=\"1 <= A,B,C <= 10^9\" /></a><br>A,B,C are integers.</p>\n<h2 id=\"Inputs\"><a href=\"#Inputs\" class=\"headerlink\" title=\"Inputs:\"></a>Inputs:</h2><p>Input is given from Standard Input in the following format:</p>\n<pre><code>A B C\n</code></pre>\n<h2 id=\"Outputs\"><a href=\"#Outputs\" class=\"headerlink\" title=\"Outputs:\"></a>Outputs:</h2><p>Print the digit at the one’s place in the decimal notation of <a><img src=\"https://latex.codecogs.com/png.latex?A^{B^C}\" title=\"A^{B^C}\" /></a>.</p>\n<h2 id=\"Sample-Input\"><a href=\"#Sample-Input\" class=\"headerlink\" title=\"Sample Input:\"></a>Sample Input:</h2><pre><code>3141592 6535897 9323846\n</code></pre>\n<h2 id=\"Sample-Output\"><a href=\"#Sample-Output\" class=\"headerlink\" title=\"Sample Output:\"></a>Sample Output:</h2><pre><code>2\n</code></pre>\n<h2 id=\"分析：\"><a href=\"#分析：\" class=\"headerlink\" title=\"分析：\"></a>分析：</h2><p>数据很大，思路首先是使用快速幂，然后思考一下，输出的是结果%10的结果，而这个结果只与A的个位有关。</p>\n<p>然后<code>A % 10</code>的幂也是循环的，发现对于0、1、5、6这样的值直接输出就可以，因为不管<code>B^C</code>为多少，都不会改变。然后4、9这样值循环大小为2，其余的为4。把循环大小作为<code>B^C</code>的模，这样能最好得优化。</p>\n<p><strong>坑</strong>：<code>B^C</code>取模后会=0，这种情况要让他们恢复相应的循环的大小，因为A的0次幂肯定是0嘛。。要做这样一个判断。</p>\n<p>代码：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;string&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstring&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;algorithm&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cmath&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;vector&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> a,b,c;</span><br><span class=\"line\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> A;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> <span class=\"title\">quick_pow</span><span class=\"params\">(<span class=\"keyword\">long</span> <span class=\"keyword\">long</span> base,<span class=\"keyword\">long</span> <span class=\"keyword\">long</span> pow)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> temp = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"number\">4</span> || A == <span class=\"number\">9</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        temp = <span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        temp = <span class=\"number\">4</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> ans = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(pow)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(pow &amp; <span class=\"number\">1</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            ans = ans * base % temp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        base = base * base % temp;<span class=\"comment\">//caculate the x^n</span></span><br><span class=\"line\">        pow &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(ans == <span class=\"number\">0</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(A == <span class=\"number\">4</span> || A == <span class=\"number\">9</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            ans = <span class=\"number\">2</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            ans = <span class=\"number\">4</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> <span class=\"title\">quick_pow2</span><span class=\"params\">(<span class=\"keyword\">long</span> <span class=\"keyword\">long</span> base, <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> p, <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> m)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> ans = <span class=\"number\">1</span> % m;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(p)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(p &amp; <span class=\"number\">1</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            ans = ans * base % m;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        base = base * base % m;</span><br><span class=\"line\">        p &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    cin&gt;&gt;a&gt;&gt;b&gt;&gt;c;</span><br><span class=\"line\">    A = a % <span class=\"number\">10</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"number\">0</span> || A == <span class=\"number\">5</span> || A == <span class=\"number\">6</span> || A == <span class=\"number\">1</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        cout&lt;&lt;A&lt;&lt;endl;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> p = <span class=\"built_in\">quick_pow</span>(b, c);</span><br><span class=\"line\">        cout&lt;&lt;<span class=\"built_in\">quick_pow2</span>(a, p, <span class=\"number\">10</span>)&lt;&lt;endl;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Atcoder-Regular-Contest-113-B-A-B-C\"><a href=\"#Atcoder-Regular-Contest-113-B-A-B-C\" class=\"headerlink\" title=\"Atcoder Regular Contest 113 B-A^B^C\"></a>Atcoder Regular Contest 113 B-A^B^C</h1><p><a href=\"https://atcoder.jp/contests/arc113/tasks/arc113_b\">https://atcoder.jp/contests/arc113/tasks/arc113_b</a></p>\n<h2 id=\"Problem-Statement：\"><a href=\"#Problem-Statement：\" class=\"headerlink\" title=\"Problem Statement：\"></a>Problem Statement：</h2><p>Given positive integers A,B,C, find the digit at the ones place in the decimal notation of <a><img src=\"https://latex.codecogs.com/png.latex?A^{B^C}\" title=\"A^{B^C}\" /></a>.</p>\n<h2 id=\"Constraints\"><a href=\"#Constraints\" class=\"headerlink\" title=\"Constraints:\"></a>Constraints:</h2><p><a><img src=\"https://latex.codecogs.com/png.latex?1&space;<=&space;A,B,C&space;<=&space;10^9\" title=\"1 <= A,B,C <= 10^9\" /></a><br>A,B,C are integers.</p>\n<h2 id=\"Inputs\"><a href=\"#Inputs\" class=\"headerlink\" title=\"Inputs:\"></a>Inputs:</h2><p>Input is given from Standard Input in the following format:</p>\n<pre><code>A B C\n</code></pre>\n<h2 id=\"Outputs\"><a href=\"#Outputs\" class=\"headerlink\" title=\"Outputs:\"></a>Outputs:</h2><p>Print the digit at the one’s place in the decimal notation of <a><img src=\"https://latex.codecogs.com/png.latex?A^{B^C}\" title=\"A^{B^C}\" /></a>.</p>\n<h2 id=\"Sample-Input\"><a href=\"#Sample-Input\" class=\"headerlink\" title=\"Sample Input:\"></a>Sample Input:</h2><pre><code>3141592 6535897 9323846\n</code></pre>\n<h2 id=\"Sample-Output\"><a href=\"#Sample-Output\" class=\"headerlink\" title=\"Sample Output:\"></a>Sample Output:</h2><pre><code>2\n</code></pre>\n<h2 id=\"分析：\"><a href=\"#分析：\" class=\"headerlink\" title=\"分析：\"></a>分析：</h2><p>数据很大，思路首先是使用快速幂，然后思考一下，输出的是结果%10的结果，而这个结果只与A的个位有关。</p>\n<p>然后<code>A % 10</code>的幂也是循环的，发现对于0、1、5、6这样的值直接输出就可以，因为不管<code>B^C</code>为多少，都不会改变。然后4、9这样值循环大小为2，其余的为4。把循环大小作为<code>B^C</code>的模，这样能最好得优化。</p>\n<p><strong>坑</strong>：<code>B^C</code>取模后会=0，这种情况要让他们恢复相应的循环的大小，因为A的0次幂肯定是0嘛。。要做这样一个判断。</p>\n<p>代码：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;string&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstring&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;algorithm&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cmath&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;vector&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> a,b,c;</span><br><span class=\"line\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> A;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> <span class=\"title\">quick_pow</span><span class=\"params\">(<span class=\"keyword\">long</span> <span class=\"keyword\">long</span> base,<span class=\"keyword\">long</span> <span class=\"keyword\">long</span> pow)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> temp = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"number\">4</span> || A == <span class=\"number\">9</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        temp = <span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        temp = <span class=\"number\">4</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> ans = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(pow)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(pow &amp; <span class=\"number\">1</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            ans = ans * base % temp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        base = base * base % temp;<span class=\"comment\">//caculate the x^n</span></span><br><span class=\"line\">        pow &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(ans == <span class=\"number\">0</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(A == <span class=\"number\">4</span> || A == <span class=\"number\">9</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            ans = <span class=\"number\">2</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            ans = <span class=\"number\">4</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> <span class=\"title\">quick_pow2</span><span class=\"params\">(<span class=\"keyword\">long</span> <span class=\"keyword\">long</span> base, <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> p, <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> m)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> ans = <span class=\"number\">1</span> % m;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(p)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(p &amp; <span class=\"number\">1</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            ans = ans * base % m;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        base = base * base % m;</span><br><span class=\"line\">        p &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    cin&gt;&gt;a&gt;&gt;b&gt;&gt;c;</span><br><span class=\"line\">    A = a % <span class=\"number\">10</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"number\">0</span> || A == <span class=\"number\">5</span> || A == <span class=\"number\">6</span> || A == <span class=\"number\">1</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        cout&lt;&lt;A&lt;&lt;endl;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> p = <span class=\"built_in\">quick_pow</span>(b, c);</span><br><span class=\"line\">        cout&lt;&lt;<span class=\"built_in\">quick_pow2</span>(a, p, <span class=\"number\">10</span>)&lt;&lt;endl;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"双指针不重复搜索","date":"2021-07-14T15:25:19.000Z","_content":"# Leetcode 15.三数之和\n\n给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。\n\n注意：答案中不可以包含重复的三元组。\n\n## Example：\n1\n\n    输入：nums = [-1,0,1,2,-1,-4]\n    输出：[[-1,-1,2],[-1,0,1]]\n\n2\n\n    输入：nums = []\n    输出：[]\n\n3\n\n    输入：nums = [0]\n    输出：[]\n\n## 思路\n\n固定三元组的一个元素，然后在剩余的元素中选出两个符合要求的元素。在这个搜索过程中，为了避免重复，先对nums进行一个sort，对于i（第一个元素）之后的元素进行双指针搜索，通过x+y+z的结果与0的大小关系来决定是left右移还是right左移。\n\n在避免重复方面，首先对于后两个元素的搜索，left值的重复会导致重复，right值的重复也会导致重复，因此让left或right继续移动直到没有重复，即可避免。然后是第一个元素，同样的方法即可。\n\n再有注意特殊情况，大小小于3的数组和最小值都>0的数组，输出为空。\n\n## 代码\n```cpp\nclass Solution {\npublic:\n    vector<vector<int>> threeSum(vector<int>& nums) {\n        vector<vector<int>> ans;\n        if(nums.size() < 3){//少于三个\n            return ans;\n        }\n        sort(nums.begin(),nums.end());\n        if(nums[0] > 0) {//最小的都>0\n            return ans;\n        }\n        int i = 0;\n        while(i < nums.size()) {\n            if(nums[i] > 0) {//不会存在该解\n                break;\n            }\n            //双指针\n            int left = i + 1;\n            int right = nums.size() - 1;\n            while(left < right) {\n                long long y = static_cast<long long>(nums[i]);\n                long long x = static_cast<long long>(nums[left]);\n                long long z = static_cast<long long>(nums[right]);\n\n                if(x + y > 0 - z){\n                    right--;\n                } //z要减小\n                else if(x + y < 0 - z){\n                    left++;\n                }//x要增加\n                else{\n                    ans.push_back({nums[i], nums[left], nums[right]});\n                    //处理特殊情况，出现相同的left或right值要跳过\n                    while(left < right && nums[left] == nums[left + 1]) {\n                        left++;\n                    }\n                    while(left < right && nums[right] == nums[right - 1]) {\n                        right--;\n                    }\n                    left++;\n                    right--;\n                }\n            }\n            //处理特殊情况，出现相同的i的值\n            while(i + 1 < nums.size() && nums[i] == nums[i + 1]) {\n                i++;\n            }\n            i++;\n        }\n        return ans;\n    }\n};\n```","source":"_posts/acm2-md.md","raw":"---\ntitle: 双指针不重复搜索\ndate: 2021-07-14 23:25:19\ntags: ACM\n---\n# Leetcode 15.三数之和\n\n给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。\n\n注意：答案中不可以包含重复的三元组。\n\n## Example：\n1\n\n    输入：nums = [-1,0,1,2,-1,-4]\n    输出：[[-1,-1,2],[-1,0,1]]\n\n2\n\n    输入：nums = []\n    输出：[]\n\n3\n\n    输入：nums = [0]\n    输出：[]\n\n## 思路\n\n固定三元组的一个元素，然后在剩余的元素中选出两个符合要求的元素。在这个搜索过程中，为了避免重复，先对nums进行一个sort，对于i（第一个元素）之后的元素进行双指针搜索，通过x+y+z的结果与0的大小关系来决定是left右移还是right左移。\n\n在避免重复方面，首先对于后两个元素的搜索，left值的重复会导致重复，right值的重复也会导致重复，因此让left或right继续移动直到没有重复，即可避免。然后是第一个元素，同样的方法即可。\n\n再有注意特殊情况，大小小于3的数组和最小值都>0的数组，输出为空。\n\n## 代码\n```cpp\nclass Solution {\npublic:\n    vector<vector<int>> threeSum(vector<int>& nums) {\n        vector<vector<int>> ans;\n        if(nums.size() < 3){//少于三个\n            return ans;\n        }\n        sort(nums.begin(),nums.end());\n        if(nums[0] > 0) {//最小的都>0\n            return ans;\n        }\n        int i = 0;\n        while(i < nums.size()) {\n            if(nums[i] > 0) {//不会存在该解\n                break;\n            }\n            //双指针\n            int left = i + 1;\n            int right = nums.size() - 1;\n            while(left < right) {\n                long long y = static_cast<long long>(nums[i]);\n                long long x = static_cast<long long>(nums[left]);\n                long long z = static_cast<long long>(nums[right]);\n\n                if(x + y > 0 - z){\n                    right--;\n                } //z要减小\n                else if(x + y < 0 - z){\n                    left++;\n                }//x要增加\n                else{\n                    ans.push_back({nums[i], nums[left], nums[right]});\n                    //处理特殊情况，出现相同的left或right值要跳过\n                    while(left < right && nums[left] == nums[left + 1]) {\n                        left++;\n                    }\n                    while(left < right && nums[right] == nums[right - 1]) {\n                        right--;\n                    }\n                    left++;\n                    right--;\n                }\n            }\n            //处理特殊情况，出现相同的i的值\n            while(i + 1 < nums.size() && nums[i] == nums[i + 1]) {\n                i++;\n            }\n            i++;\n        }\n        return ans;\n    }\n};\n```","slug":"acm2-md","published":1,"updated":"2021-07-14T16:02:10.962Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckr3o95n200000woe8xima8wd","content":"<h1 id=\"Leetcode-15-三数之和\"><a href=\"#Leetcode-15-三数之和\" class=\"headerlink\" title=\"Leetcode 15.三数之和\"></a>Leetcode 15.三数之和</h1><p>给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。</p>\n<p>注意：答案中不可以包含重复的三元组。</p>\n<h2 id=\"Example：\"><a href=\"#Example：\" class=\"headerlink\" title=\"Example：\"></a>Example：</h2><p>1</p>\n<pre><code>输入：nums = [-1,0,1,2,-1,-4]\n输出：[[-1,-1,2],[-1,0,1]]\n</code></pre>\n<p>2</p>\n<pre><code>输入：nums = []\n输出：[]\n</code></pre>\n<p>3</p>\n<pre><code>输入：nums = [0]\n输出：[]\n</code></pre>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>固定三元组的一个元素，然后在剩余的元素中选出两个符合要求的元素。在这个搜索过程中，为了避免重复，先对nums进行一个sort，对于i（第一个元素）之后的元素进行双指针搜索，通过x+y+z的结果与0的大小关系来决定是left右移还是right左移。</p>\n<p>在避免重复方面，首先对于后两个元素的搜索，left值的重复会导致重复，right值的重复也会导致重复，因此让left或right继续移动直到没有重复，即可避免。然后是第一个元素，同样的方法即可。</p>\n<p>再有注意特殊情况，大小小于3的数组和最小值都&gt;0的数组，输出为空。</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> &#123;</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    vector&lt;vector&lt;<span class=\"keyword\">int</span>&gt;&gt; <span class=\"built_in\">threeSum</span>(vector&lt;<span class=\"keyword\">int</span>&gt;&amp; nums) &#123;</span><br><span class=\"line\">        vector&lt;vector&lt;<span class=\"keyword\">int</span>&gt;&gt; ans;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(nums.<span class=\"built_in\">size</span>() &lt; <span class=\"number\">3</span>)&#123;<span class=\"comment\">//少于三个</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"built_in\">sort</span>(nums.<span class=\"built_in\">begin</span>(),nums.<span class=\"built_in\">end</span>());</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(nums[<span class=\"number\">0</span>] &gt; <span class=\"number\">0</span>) &#123;<span class=\"comment\">//最小的都&gt;0</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(i &lt; nums.<span class=\"built_in\">size</span>()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(nums[i] &gt; <span class=\"number\">0</span>) &#123;<span class=\"comment\">//不会存在该解</span></span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"comment\">//双指针</span></span><br><span class=\"line\">            <span class=\"keyword\">int</span> left = i + <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> right = nums.<span class=\"built_in\">size</span>() - <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(left &lt; right) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> y = <span class=\"keyword\">static_cast</span>&lt;<span class=\"keyword\">long</span> <span class=\"keyword\">long</span>&gt;(nums[i]);</span><br><span class=\"line\">                <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> x = <span class=\"keyword\">static_cast</span>&lt;<span class=\"keyword\">long</span> <span class=\"keyword\">long</span>&gt;(nums[left]);</span><br><span class=\"line\">                <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> z = <span class=\"keyword\">static_cast</span>&lt;<span class=\"keyword\">long</span> <span class=\"keyword\">long</span>&gt;(nums[right]);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span>(x + y &gt; <span class=\"number\">0</span> - z)&#123;</span><br><span class=\"line\">                    right--;</span><br><span class=\"line\">                &#125; <span class=\"comment\">//z要减小</span></span><br><span class=\"line\">                <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(x + y &lt; <span class=\"number\">0</span> - z)&#123;</span><br><span class=\"line\">                    left++;</span><br><span class=\"line\">                &#125;<span class=\"comment\">//x要增加</span></span><br><span class=\"line\">                <span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                    ans.<span class=\"built_in\">push_back</span>(&#123;nums[i], nums[left], nums[right]&#125;);</span><br><span class=\"line\">                    <span class=\"comment\">//处理特殊情况，出现相同的left或right值要跳过</span></span><br><span class=\"line\">                    <span class=\"keyword\">while</span>(left &lt; right &amp;&amp; nums[left] == nums[left + <span class=\"number\">1</span>]) &#123;</span><br><span class=\"line\">                        left++;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                    <span class=\"keyword\">while</span>(left &lt; right &amp;&amp; nums[right] == nums[right - <span class=\"number\">1</span>]) &#123;</span><br><span class=\"line\">                        right--;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                    left++;</span><br><span class=\"line\">                    right--;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"comment\">//处理特殊情况，出现相同的i的值</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span>(i + <span class=\"number\">1</span> &lt; nums.<span class=\"built_in\">size</span>() &amp;&amp; nums[i] == nums[i + <span class=\"number\">1</span>]) &#123;</span><br><span class=\"line\">                i++;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            i++;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Leetcode-15-三数之和\"><a href=\"#Leetcode-15-三数之和\" class=\"headerlink\" title=\"Leetcode 15.三数之和\"></a>Leetcode 15.三数之和</h1><p>给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。</p>\n<p>注意：答案中不可以包含重复的三元组。</p>\n<h2 id=\"Example：\"><a href=\"#Example：\" class=\"headerlink\" title=\"Example：\"></a>Example：</h2><p>1</p>\n<pre><code>输入：nums = [-1,0,1,2,-1,-4]\n输出：[[-1,-1,2],[-1,0,1]]\n</code></pre>\n<p>2</p>\n<pre><code>输入：nums = []\n输出：[]\n</code></pre>\n<p>3</p>\n<pre><code>输入：nums = [0]\n输出：[]\n</code></pre>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>固定三元组的一个元素，然后在剩余的元素中选出两个符合要求的元素。在这个搜索过程中，为了避免重复，先对nums进行一个sort，对于i（第一个元素）之后的元素进行双指针搜索，通过x+y+z的结果与0的大小关系来决定是left右移还是right左移。</p>\n<p>在避免重复方面，首先对于后两个元素的搜索，left值的重复会导致重复，right值的重复也会导致重复，因此让left或right继续移动直到没有重复，即可避免。然后是第一个元素，同样的方法即可。</p>\n<p>再有注意特殊情况，大小小于3的数组和最小值都&gt;0的数组，输出为空。</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> &#123;</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    vector&lt;vector&lt;<span class=\"keyword\">int</span>&gt;&gt; <span class=\"built_in\">threeSum</span>(vector&lt;<span class=\"keyword\">int</span>&gt;&amp; nums) &#123;</span><br><span class=\"line\">        vector&lt;vector&lt;<span class=\"keyword\">int</span>&gt;&gt; ans;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(nums.<span class=\"built_in\">size</span>() &lt; <span class=\"number\">3</span>)&#123;<span class=\"comment\">//少于三个</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"built_in\">sort</span>(nums.<span class=\"built_in\">begin</span>(),nums.<span class=\"built_in\">end</span>());</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(nums[<span class=\"number\">0</span>] &gt; <span class=\"number\">0</span>) &#123;<span class=\"comment\">//最小的都&gt;0</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(i &lt; nums.<span class=\"built_in\">size</span>()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(nums[i] &gt; <span class=\"number\">0</span>) &#123;<span class=\"comment\">//不会存在该解</span></span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"comment\">//双指针</span></span><br><span class=\"line\">            <span class=\"keyword\">int</span> left = i + <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> right = nums.<span class=\"built_in\">size</span>() - <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(left &lt; right) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> y = <span class=\"keyword\">static_cast</span>&lt;<span class=\"keyword\">long</span> <span class=\"keyword\">long</span>&gt;(nums[i]);</span><br><span class=\"line\">                <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> x = <span class=\"keyword\">static_cast</span>&lt;<span class=\"keyword\">long</span> <span class=\"keyword\">long</span>&gt;(nums[left]);</span><br><span class=\"line\">                <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> z = <span class=\"keyword\">static_cast</span>&lt;<span class=\"keyword\">long</span> <span class=\"keyword\">long</span>&gt;(nums[right]);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span>(x + y &gt; <span class=\"number\">0</span> - z)&#123;</span><br><span class=\"line\">                    right--;</span><br><span class=\"line\">                &#125; <span class=\"comment\">//z要减小</span></span><br><span class=\"line\">                <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(x + y &lt; <span class=\"number\">0</span> - z)&#123;</span><br><span class=\"line\">                    left++;</span><br><span class=\"line\">                &#125;<span class=\"comment\">//x要增加</span></span><br><span class=\"line\">                <span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                    ans.<span class=\"built_in\">push_back</span>(&#123;nums[i], nums[left], nums[right]&#125;);</span><br><span class=\"line\">                    <span class=\"comment\">//处理特殊情况，出现相同的left或right值要跳过</span></span><br><span class=\"line\">                    <span class=\"keyword\">while</span>(left &lt; right &amp;&amp; nums[left] == nums[left + <span class=\"number\">1</span>]) &#123;</span><br><span class=\"line\">                        left++;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                    <span class=\"keyword\">while</span>(left &lt; right &amp;&amp; nums[right] == nums[right - <span class=\"number\">1</span>]) &#123;</span><br><span class=\"line\">                        right--;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                    left++;</span><br><span class=\"line\">                    right--;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"comment\">//处理特殊情况，出现相同的i的值</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span>(i + <span class=\"number\">1</span> &lt; nums.<span class=\"built_in\">size</span>() &amp;&amp; nums[i] == nums[i + <span class=\"number\">1</span>]) &#123;</span><br><span class=\"line\">                i++;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            i++;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>"},{"title":"二分查找","date":"2021-07-15T17:10:40.000Z","_content":"\n# 剑指 Offer 53 - I. 在排序数组中查找数字 I\n\n统计一个数字在排序数组中出现的次数。\n\n## 示例 1:\n\n    输入: nums = [5,7,7,8,8,10], target = 8\n    输出: 2\n\n## 示例 2:\n\n    输入: nums = [5,7,7,8,8,10], target = 6\n    输出: 0\n\n0 <= 数组长度 <= 50000\n\n## 思路\n\n啊看上去是个遍历水题数据也不大，但是看了评论才知道如果面试官出这道题其实是要考察二分查找的，所以手法还是要专业。所以就先二分查找到一个target，然后向前向后遍历计数即可。\n\n## 代码\n\n```cpp\nclass Solution {\npublic:\n    int search(vector<int>& nums, int target) {\n        int ans = 0;\n        int n = nums.size();\n        if(n == 0){\n            return 0;\n        }\n        int l = 0;\n        int r = n - 1;\n        while(l < r) {\n            int mid = (l + r) / 2;\n            if(nums[mid] == target) {\n                l = mid;\n                r = mid;\n                break;\n            }\n            else if(nums[mid] < target) {\n                l = mid + 1;\n            }\n            else {\n                r = mid;\n            }\n        }\n        for(int i = l;i >= 0 && nums[i] == target;i--) {\n            ans++;\n        }\n        for(int i = l + 1;i < n && nums[i] == target;i++) {\n            ans++;\n        }\n        return ans;\n    }\n};\n```","source":"_posts/acm3-md.md","raw":"---\ntitle: 二分查找\ndate: 2021-07-16 01:10:40\ntags: ACM\n---\n\n# 剑指 Offer 53 - I. 在排序数组中查找数字 I\n\n统计一个数字在排序数组中出现的次数。\n\n## 示例 1:\n\n    输入: nums = [5,7,7,8,8,10], target = 8\n    输出: 2\n\n## 示例 2:\n\n    输入: nums = [5,7,7,8,8,10], target = 6\n    输出: 0\n\n0 <= 数组长度 <= 50000\n\n## 思路\n\n啊看上去是个遍历水题数据也不大，但是看了评论才知道如果面试官出这道题其实是要考察二分查找的，所以手法还是要专业。所以就先二分查找到一个target，然后向前向后遍历计数即可。\n\n## 代码\n\n```cpp\nclass Solution {\npublic:\n    int search(vector<int>& nums, int target) {\n        int ans = 0;\n        int n = nums.size();\n        if(n == 0){\n            return 0;\n        }\n        int l = 0;\n        int r = n - 1;\n        while(l < r) {\n            int mid = (l + r) / 2;\n            if(nums[mid] == target) {\n                l = mid;\n                r = mid;\n                break;\n            }\n            else if(nums[mid] < target) {\n                l = mid + 1;\n            }\n            else {\n                r = mid;\n            }\n        }\n        for(int i = l;i >= 0 && nums[i] == target;i--) {\n            ans++;\n        }\n        for(int i = l + 1;i < n && nums[i] == target;i++) {\n            ans++;\n        }\n        return ans;\n    }\n};\n```","slug":"acm3-md","published":1,"updated":"2021-07-24T04:03:03.661Z","_id":"ckr56actz0000xvoe1jyv1ae3","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"剑指-Offer-53-I-在排序数组中查找数字-I\"><a href=\"#剑指-Offer-53-I-在排序数组中查找数字-I\" class=\"headerlink\" title=\"剑指 Offer 53 - I. 在排序数组中查找数字 I\"></a>剑指 Offer 53 - I. 在排序数组中查找数字 I</h1><p>统计一个数字在排序数组中出现的次数。</p>\n<h2 id=\"示例-1\"><a href=\"#示例-1\" class=\"headerlink\" title=\"示例 1:\"></a>示例 1:</h2><pre><code>输入: nums = [5,7,7,8,8,10], target = 8\n输出: 2\n</code></pre>\n<h2 id=\"示例-2\"><a href=\"#示例-2\" class=\"headerlink\" title=\"示例 2:\"></a>示例 2:</h2><pre><code>输入: nums = [5,7,7,8,8,10], target = 6\n输出: 0\n</code></pre>\n<p>0 &lt;= 数组长度 &lt;= 50000</p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>啊看上去是个遍历水题数据也不大，但是看了评论才知道如果面试官出这道题其实是要考察二分查找的，所以手法还是要专业。所以就先二分查找到一个target，然后向前向后遍历计数即可。</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> &#123;</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">search</span><span class=\"params\">(vector&lt;<span class=\"keyword\">int</span>&gt;&amp; nums, <span class=\"keyword\">int</span> target)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> ans = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> n = nums.<span class=\"built_in\">size</span>();</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(n == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> l = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> r = n - <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(l &lt; r) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> mid = (l + r) / <span class=\"number\">2</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(nums[mid] == target) &#123;</span><br><span class=\"line\">                l = mid;</span><br><span class=\"line\">                r = mid;</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(nums[mid] &lt; target) &#123;</span><br><span class=\"line\">                l = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                r = mid;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = l;i &gt;= <span class=\"number\">0</span> &amp;&amp; nums[i] == target;i--) &#123;</span><br><span class=\"line\">            ans++;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = l + <span class=\"number\">1</span>;i &lt; n &amp;&amp; nums[i] == target;i++) &#123;</span><br><span class=\"line\">            ans++;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"剑指-Offer-53-I-在排序数组中查找数字-I\"><a href=\"#剑指-Offer-53-I-在排序数组中查找数字-I\" class=\"headerlink\" title=\"剑指 Offer 53 - I. 在排序数组中查找数字 I\"></a>剑指 Offer 53 - I. 在排序数组中查找数字 I</h1><p>统计一个数字在排序数组中出现的次数。</p>\n<h2 id=\"示例-1\"><a href=\"#示例-1\" class=\"headerlink\" title=\"示例 1:\"></a>示例 1:</h2><pre><code>输入: nums = [5,7,7,8,8,10], target = 8\n输出: 2\n</code></pre>\n<h2 id=\"示例-2\"><a href=\"#示例-2\" class=\"headerlink\" title=\"示例 2:\"></a>示例 2:</h2><pre><code>输入: nums = [5,7,7,8,8,10], target = 6\n输出: 0\n</code></pre>\n<p>0 &lt;= 数组长度 &lt;= 50000</p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>啊看上去是个遍历水题数据也不大，但是看了评论才知道如果面试官出这道题其实是要考察二分查找的，所以手法还是要专业。所以就先二分查找到一个target，然后向前向后遍历计数即可。</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> &#123;</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">search</span><span class=\"params\">(vector&lt;<span class=\"keyword\">int</span>&gt;&amp; nums, <span class=\"keyword\">int</span> target)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> ans = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> n = nums.<span class=\"built_in\">size</span>();</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(n == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> l = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> r = n - <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(l &lt; r) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> mid = (l + r) / <span class=\"number\">2</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(nums[mid] == target) &#123;</span><br><span class=\"line\">                l = mid;</span><br><span class=\"line\">                r = mid;</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(nums[mid] &lt; target) &#123;</span><br><span class=\"line\">                l = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                r = mid;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = l;i &gt;= <span class=\"number\">0</span> &amp;&amp; nums[i] == target;i--) &#123;</span><br><span class=\"line\">            ans++;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = l + <span class=\"number\">1</span>;i &lt; n &amp;&amp; nums[i] == target;i++) &#123;</span><br><span class=\"line\">            ans++;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>"},{"title":"unordered_map","date":"2021-07-17T16:56:32.000Z","_content":"\n# Leetcode 10.02. 变位词组\n\n编写一种方法，对字符串数组进行排序，将所有变位词组合在一起。变位词是指字母相同，但排列不同的字符串。\n\n## 实例\n\n    输入: [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"],\n    输出:\n    [\n    [\"ate\",\"eat\",\"tea\"],\n    [\"nat\",\"tan\"],\n    [\"bat\"]\n    ]\n\n## 思路\n\n我直接暴力😭。\n\n这里学到了C++11里面的unordered_map，其实是一个内部使用hash表结构的关联容器。基本上就是用于快速检索。\n它通过key来寻找对应的value，内部无序。\n对于iterator，itr.first是key，itr.second是value。\n\n还有一个，今天才知道sort还可以对string内的字符排序...\n\n代码：\n\n```cpp\nclass Solution {\npublic:\n    vector<vector<string>> groupAnagrams(vector<string>& strs) {\n        vector<vector<string>> ans;\n        unordered_map<string, vector<string>> hash;\n        for(int i = 0; i < strs.size();i++) {\n            string tmp = strs[i];\n            sort(tmp.begin(), tmp.end());\n            hash[tmp].push_back(strs[i]);\n        }\n        for(auto itr = hash.begin();itr != hash.end();itr++){\n            ans.push_back(itr->second);\n        }\n        return ans;\n    }\n};\n```\n\n","source":"_posts/acm4-md.md","raw":"---\ntitle: unordered_map\ndate: 2021-07-18 00:56:32\ntags: ACM\n---\n\n# Leetcode 10.02. 变位词组\n\n编写一种方法，对字符串数组进行排序，将所有变位词组合在一起。变位词是指字母相同，但排列不同的字符串。\n\n## 实例\n\n    输入: [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"],\n    输出:\n    [\n    [\"ate\",\"eat\",\"tea\"],\n    [\"nat\",\"tan\"],\n    [\"bat\"]\n    ]\n\n## 思路\n\n我直接暴力😭。\n\n这里学到了C++11里面的unordered_map，其实是一个内部使用hash表结构的关联容器。基本上就是用于快速检索。\n它通过key来寻找对应的value，内部无序。\n对于iterator，itr.first是key，itr.second是value。\n\n还有一个，今天才知道sort还可以对string内的字符排序...\n\n代码：\n\n```cpp\nclass Solution {\npublic:\n    vector<vector<string>> groupAnagrams(vector<string>& strs) {\n        vector<vector<string>> ans;\n        unordered_map<string, vector<string>> hash;\n        for(int i = 0; i < strs.size();i++) {\n            string tmp = strs[i];\n            sort(tmp.begin(), tmp.end());\n            hash[tmp].push_back(strs[i]);\n        }\n        for(auto itr = hash.begin();itr != hash.end();itr++){\n            ans.push_back(itr->second);\n        }\n        return ans;\n    }\n};\n```\n\n","slug":"acm4-md","published":1,"updated":"2021-07-17T17:11:23.278Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckr812mmq0000lkoeedif9tvc","content":"<h1 id=\"Leetcode-10-02-变位词组\"><a href=\"#Leetcode-10-02-变位词组\" class=\"headerlink\" title=\"Leetcode 10.02. 变位词组\"></a>Leetcode 10.02. 变位词组</h1><p>编写一种方法，对字符串数组进行排序，将所有变位词组合在一起。变位词是指字母相同，但排列不同的字符串。</p>\n<h2 id=\"实例\"><a href=\"#实例\" class=\"headerlink\" title=\"实例\"></a>实例</h2><pre><code>输入: [&quot;eat&quot;, &quot;tea&quot;, &quot;tan&quot;, &quot;ate&quot;, &quot;nat&quot;, &quot;bat&quot;],\n输出:\n[\n[&quot;ate&quot;,&quot;eat&quot;,&quot;tea&quot;],\n[&quot;nat&quot;,&quot;tan&quot;],\n[&quot;bat&quot;]\n]\n</code></pre>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>我直接暴力😭。</p>\n<p>这里学到了C++11里面的unordered_map，其实是一个内部使用hash表结构的关联容器。基本上就是用于快速检索。<br>它通过key来寻找对应的value，内部无序。<br>对于iterator，itr.first是key，itr.second是value。</p>\n<p>还有一个，今天才知道sort还可以对string内的字符排序…</p>\n<p>代码：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> &#123;</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    vector&lt;vector&lt;string&gt;&gt; <span class=\"built_in\">groupAnagrams</span>(vector&lt;string&gt;&amp; strs) &#123;</span><br><span class=\"line\">        vector&lt;vector&lt;string&gt;&gt; ans;</span><br><span class=\"line\">        unordered_map&lt;string, vector&lt;string&gt;&gt; hash;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; strs.<span class=\"built_in\">size</span>();i++) &#123;</span><br><span class=\"line\">            string tmp = strs[i];</span><br><span class=\"line\">            <span class=\"built_in\">sort</span>(tmp.<span class=\"built_in\">begin</span>(), tmp.<span class=\"built_in\">end</span>());</span><br><span class=\"line\">            hash[tmp].<span class=\"built_in\">push_back</span>(strs[i]);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">auto</span> itr = hash.<span class=\"built_in\">begin</span>();itr != hash.<span class=\"built_in\">end</span>();itr++)&#123;</span><br><span class=\"line\">            ans.<span class=\"built_in\">push_back</span>(itr-&gt;second);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Leetcode-10-02-变位词组\"><a href=\"#Leetcode-10-02-变位词组\" class=\"headerlink\" title=\"Leetcode 10.02. 变位词组\"></a>Leetcode 10.02. 变位词组</h1><p>编写一种方法，对字符串数组进行排序，将所有变位词组合在一起。变位词是指字母相同，但排列不同的字符串。</p>\n<h2 id=\"实例\"><a href=\"#实例\" class=\"headerlink\" title=\"实例\"></a>实例</h2><pre><code>输入: [&quot;eat&quot;, &quot;tea&quot;, &quot;tan&quot;, &quot;ate&quot;, &quot;nat&quot;, &quot;bat&quot;],\n输出:\n[\n[&quot;ate&quot;,&quot;eat&quot;,&quot;tea&quot;],\n[&quot;nat&quot;,&quot;tan&quot;],\n[&quot;bat&quot;]\n]\n</code></pre>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>我直接暴力😭。</p>\n<p>这里学到了C++11里面的unordered_map，其实是一个内部使用hash表结构的关联容器。基本上就是用于快速检索。<br>它通过key来寻找对应的value，内部无序。<br>对于iterator，itr.first是key，itr.second是value。</p>\n<p>还有一个，今天才知道sort还可以对string内的字符排序…</p>\n<p>代码：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> &#123;</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    vector&lt;vector&lt;string&gt;&gt; <span class=\"built_in\">groupAnagrams</span>(vector&lt;string&gt;&amp; strs) &#123;</span><br><span class=\"line\">        vector&lt;vector&lt;string&gt;&gt; ans;</span><br><span class=\"line\">        unordered_map&lt;string, vector&lt;string&gt;&gt; hash;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; strs.<span class=\"built_in\">size</span>();i++) &#123;</span><br><span class=\"line\">            string tmp = strs[i];</span><br><span class=\"line\">            <span class=\"built_in\">sort</span>(tmp.<span class=\"built_in\">begin</span>(), tmp.<span class=\"built_in\">end</span>());</span><br><span class=\"line\">            hash[tmp].<span class=\"built_in\">push_back</span>(strs[i]);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">auto</span> itr = hash.<span class=\"built_in\">begin</span>();itr != hash.<span class=\"built_in\">end</span>();itr++)&#123;</span><br><span class=\"line\">            ans.<span class=\"built_in\">push_back</span>(itr-&gt;second);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n"},{"title":"QT中的Signal和Slots","date":"2021-07-19T16:18:23.000Z","_content":"\n## 自定义信号和槽函数的同时重载\n\n记录一下，省得一周的小学期又白上.....\n\nSignal和Slots相当交互中一个事件触发的条件与结果。而在QT中，条件与结果常常是某一class的对象中的某个函数，所以使用```connection(arg1, arg2, arg3, arg4)```来连接二者，一旦arg1对象下的arg2操作实现，则arg3对象下的arg4操作进行。\n\n通常分别建立两个head和cpp来实现这两个对象的构建。\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/1.png\"  height=\"240\" width=\"189\">\n\n像这样，其中hello.h是声明的signal类，world.h是声明的slots类。分别如下：\n\nhello.h\n```cpp\n#ifndef HELLO_H\n#define HELLO_H\n\n#include <QObject>\n#include <QString>\n\nclass hello : public QObject\n{\n    Q_OBJECT\npublic:\n    explicit hello(QObject *parent = nullptr);\n\nsignals:\n    void CallWorld();\n    void CallWorld(QString mood);   //overload\npublic slots:\n\n};\n\n#endif // HELLO_H\n```\n\nworld.h\n```cpp\n#ifndef WORLD_H\n#define WORLD_H\n\n#include <QObject>\n#include <QString>\n\nclass world : public QObject\n{\n    Q_OBJECT\npublic:\n    explicit world(QObject *parent = nullptr);\n\nsignals:\n\n\npublic slots:\n    void ReceiveHello();\n    void ReceiveHello(QString mood);    //overload\n};\n\n#endif // WORLD_H\n```\n同时也要在主类中去声明这俩对象。\n```cpp\nprivate:\n    Ui::SignalAndSlot *ui;\n    hello *he;\n    world *wo;\n    void Ready();\n```\n\n下面的Ready函数是用来触发条件的。里面一般是调用signal的函数。当然这个函数的实现应该是在cpp中。像这样：\n\n```cpp\nvoid SignalAndSlot::Ready()\n{\n    //emit he->CallWorld();\n    emit he->CallWorld(\"happy\");\n}\n```\n至于这个emit，很迷，是个发射signal指令？不懂。\n\n然后就可以在这个cpp文件中进行connection了。注意这里需要重载，重载的操作使用的是函数指针来让connect明确要调用的是哪一个函数。\n\n```cpp\nSignalAndSlot::SignalAndSlot(QWidget *parent)\n    : QWidget(parent)\n    , ui(new Ui::SignalAndSlot)\n{\n    ui->setupUi(this);\n    this->he = new hello(this);\n    this->wo = new world(this);\n    //connect(he, &hello::CallWorld, wo, &world::ReceiveHello);\n    //overload\n    void(hello::*helloSignal)(QString) = &hello::CallWorld;\n    void(world::*worldSlots)(QString) = &world::ReceiveHello;\n    connect(he, helloSignal, wo, worldSlots);\n    Ready();\n}\n```\n另外别忘了include头文件。connect完了这个worldSlots就处于一个待触发的状态了。一旦Ready()触发，它就被调用。\n\n至于```word.cpp```和```hello.cpp```，当然是用来写signal和slots函数了，光在.h声明了也得实现呢。\n\n还有，一直没懂这个```main.cpp```以外的cpp文件是咋执行的，仔细一想鹊食，其它cpp都是对构造函数的定义，然后main中直接去调用它们的头文件就可以去调用某个类中的函数。原来是这样，我是笨蛋。\n\n总结一下就是基本都是用类对象及其构造函数操作的，找到点感觉了。","source":"_posts/qtsignalsAndSlots-md.md","raw":"---\ntitle: QT中的Signal和Slots\ndate: 2021-07-20 00:18:23\ntags: QT\n---\n\n## 自定义信号和槽函数的同时重载\n\n记录一下，省得一周的小学期又白上.....\n\nSignal和Slots相当交互中一个事件触发的条件与结果。而在QT中，条件与结果常常是某一class的对象中的某个函数，所以使用```connection(arg1, arg2, arg3, arg4)```来连接二者，一旦arg1对象下的arg2操作实现，则arg3对象下的arg4操作进行。\n\n通常分别建立两个head和cpp来实现这两个对象的构建。\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/1.png\"  height=\"240\" width=\"189\">\n\n像这样，其中hello.h是声明的signal类，world.h是声明的slots类。分别如下：\n\nhello.h\n```cpp\n#ifndef HELLO_H\n#define HELLO_H\n\n#include <QObject>\n#include <QString>\n\nclass hello : public QObject\n{\n    Q_OBJECT\npublic:\n    explicit hello(QObject *parent = nullptr);\n\nsignals:\n    void CallWorld();\n    void CallWorld(QString mood);   //overload\npublic slots:\n\n};\n\n#endif // HELLO_H\n```\n\nworld.h\n```cpp\n#ifndef WORLD_H\n#define WORLD_H\n\n#include <QObject>\n#include <QString>\n\nclass world : public QObject\n{\n    Q_OBJECT\npublic:\n    explicit world(QObject *parent = nullptr);\n\nsignals:\n\n\npublic slots:\n    void ReceiveHello();\n    void ReceiveHello(QString mood);    //overload\n};\n\n#endif // WORLD_H\n```\n同时也要在主类中去声明这俩对象。\n```cpp\nprivate:\n    Ui::SignalAndSlot *ui;\n    hello *he;\n    world *wo;\n    void Ready();\n```\n\n下面的Ready函数是用来触发条件的。里面一般是调用signal的函数。当然这个函数的实现应该是在cpp中。像这样：\n\n```cpp\nvoid SignalAndSlot::Ready()\n{\n    //emit he->CallWorld();\n    emit he->CallWorld(\"happy\");\n}\n```\n至于这个emit，很迷，是个发射signal指令？不懂。\n\n然后就可以在这个cpp文件中进行connection了。注意这里需要重载，重载的操作使用的是函数指针来让connect明确要调用的是哪一个函数。\n\n```cpp\nSignalAndSlot::SignalAndSlot(QWidget *parent)\n    : QWidget(parent)\n    , ui(new Ui::SignalAndSlot)\n{\n    ui->setupUi(this);\n    this->he = new hello(this);\n    this->wo = new world(this);\n    //connect(he, &hello::CallWorld, wo, &world::ReceiveHello);\n    //overload\n    void(hello::*helloSignal)(QString) = &hello::CallWorld;\n    void(world::*worldSlots)(QString) = &world::ReceiveHello;\n    connect(he, helloSignal, wo, worldSlots);\n    Ready();\n}\n```\n另外别忘了include头文件。connect完了这个worldSlots就处于一个待触发的状态了。一旦Ready()触发，它就被调用。\n\n至于```word.cpp```和```hello.cpp```，当然是用来写signal和slots函数了，光在.h声明了也得实现呢。\n\n还有，一直没懂这个```main.cpp```以外的cpp文件是咋执行的，仔细一想鹊食，其它cpp都是对构造函数的定义，然后main中直接去调用它们的头文件就可以去调用某个类中的函数。原来是这样，我是笨蛋。\n\n总结一下就是基本都是用类对象及其构造函数操作的，找到点感觉了。","slug":"qtsignalsAndSlots-md","published":1,"updated":"2021-07-20T08:31:01.785Z","_id":"ckravtm2q0000qwoe3vt9d9wm","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"自定义信号和槽函数的同时重载\"><a href=\"#自定义信号和槽函数的同时重载\" class=\"headerlink\" title=\"自定义信号和槽函数的同时重载\"></a>自定义信号和槽函数的同时重载</h2><p>记录一下，省得一周的小学期又白上…..</p>\n<p>Signal和Slots相当交互中一个事件触发的条件与结果。而在QT中，条件与结果常常是某一class的对象中的某个函数，所以使用<code>connection(arg1, arg2, arg3, arg4)</code>来连接二者，一旦arg1对象下的arg2操作实现，则arg3对象下的arg4操作进行。</p>\n<p>通常分别建立两个head和cpp来实现这两个对象的构建。</p>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/1.png\"  height=\"240\" width=\"189\">\n\n<p>像这样，其中hello.h是声明的signal类，world.h是声明的slots类。分别如下：</p>\n<p>hello.h</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifndef</span> HELLO_H</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> HELLO_H</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;QObject&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;QString&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">hello</span> :</span> <span class=\"keyword\">public</span> QObject</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    Q_OBJECT</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">hello</span><span class=\"params\">(QObject *parent = <span class=\"literal\">nullptr</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">signals:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">CallWorld</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">CallWorld</span><span class=\"params\">(QString mood)</span></span>;   <span class=\"comment\">//overload</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> slots:</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span> <span class=\"comment\">// HELLO_H</span></span></span><br></pre></td></tr></table></figure>\n\n<p>world.h</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifndef</span> WORLD_H</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> WORLD_H</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;QObject&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;QString&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">world</span> :</span> <span class=\"keyword\">public</span> QObject</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    Q_OBJECT</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">world</span><span class=\"params\">(QObject *parent = <span class=\"literal\">nullptr</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">signals:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> slots:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">ReceiveHello</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">ReceiveHello</span><span class=\"params\">(QString mood)</span></span>;    <span class=\"comment\">//overload</span></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span> <span class=\"comment\">// WORLD_H</span></span></span><br></pre></td></tr></table></figure>\n<p>同时也要在主类中去声明这俩对象。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">    Ui::SignalAndSlot *ui;</span><br><span class=\"line\">    hello *he;</span><br><span class=\"line\">    world *wo;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">Ready</span><span class=\"params\">()</span></span>;</span><br></pre></td></tr></table></figure>\n\n<p>下面的Ready函数是用来触发条件的。里面一般是调用signal的函数。当然这个函数的实现应该是在cpp中。像这样：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">SignalAndSlot::Ready</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//emit he-&gt;CallWorld();</span></span><br><span class=\"line\">    emit he-&gt;<span class=\"built_in\">CallWorld</span>(<span class=\"string\">&quot;happy&quot;</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>至于这个emit，很迷，是个发射signal指令？不懂。</p>\n<p>然后就可以在这个cpp文件中进行connection了。注意这里需要重载，重载的操作使用的是函数指针来让connect明确要调用的是哪一个函数。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SignalAndSlot::<span class=\"built_in\">SignalAndSlot</span>(QWidget *parent)</span><br><span class=\"line\">    : <span class=\"built_in\">QWidget</span>(parent)</span><br><span class=\"line\">    , <span class=\"built_in\">ui</span>(<span class=\"keyword\">new</span> Ui::SignalAndSlot)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    ui-&gt;<span class=\"built_in\">setupUi</span>(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;he = <span class=\"keyword\">new</span> <span class=\"built_in\">hello</span>(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;wo = <span class=\"keyword\">new</span> <span class=\"built_in\">world</span>(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">    <span class=\"comment\">//connect(he, &amp;hello::CallWorld, wo, &amp;world::ReceiveHello);</span></span><br><span class=\"line\">    <span class=\"comment\">//overload</span></span><br><span class=\"line\">    <span class=\"built_in\"><span class=\"keyword\">void</span></span>(hello::*helloSignal)(QString) = &amp;hello::CallWorld;</span><br><span class=\"line\">    <span class=\"built_in\"><span class=\"keyword\">void</span></span>(world::*worldSlots)(QString) = &amp;world::ReceiveHello;</span><br><span class=\"line\">    <span class=\"built_in\">connect</span>(he, helloSignal, wo, worldSlots);</span><br><span class=\"line\">    <span class=\"built_in\">Ready</span>();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>另外别忘了include头文件。connect完了这个worldSlots就处于一个待触发的状态了。一旦Ready()触发，它就被调用。</p>\n<p>至于<code>word.cpp</code>和<code>hello.cpp</code>，当然是用来写signal和slots函数了，光在.h声明了也得实现呢。</p>\n<p>还有，一直没懂这个<code>main.cpp</code>以外的cpp文件是咋执行的，仔细一想鹊食，其它cpp都是对构造函数的定义，然后main中直接去调用它们的头文件就可以去调用某个类中的函数。原来是这样，我是笨蛋。</p>\n<p>总结一下就是基本都是用类对象及其构造函数操作的，找到点感觉了。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"自定义信号和槽函数的同时重载\"><a href=\"#自定义信号和槽函数的同时重载\" class=\"headerlink\" title=\"自定义信号和槽函数的同时重载\"></a>自定义信号和槽函数的同时重载</h2><p>记录一下，省得一周的小学期又白上…..</p>\n<p>Signal和Slots相当交互中一个事件触发的条件与结果。而在QT中，条件与结果常常是某一class的对象中的某个函数，所以使用<code>connection(arg1, arg2, arg3, arg4)</code>来连接二者，一旦arg1对象下的arg2操作实现，则arg3对象下的arg4操作进行。</p>\n<p>通常分别建立两个head和cpp来实现这两个对象的构建。</p>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/1.png\"  height=\"240\" width=\"189\">\n\n<p>像这样，其中hello.h是声明的signal类，world.h是声明的slots类。分别如下：</p>\n<p>hello.h</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifndef</span> HELLO_H</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> HELLO_H</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;QObject&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;QString&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">hello</span> :</span> <span class=\"keyword\">public</span> QObject</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    Q_OBJECT</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">hello</span><span class=\"params\">(QObject *parent = <span class=\"literal\">nullptr</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">signals:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">CallWorld</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">CallWorld</span><span class=\"params\">(QString mood)</span></span>;   <span class=\"comment\">//overload</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> slots:</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span> <span class=\"comment\">// HELLO_H</span></span></span><br></pre></td></tr></table></figure>\n\n<p>world.h</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifndef</span> WORLD_H</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> WORLD_H</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;QObject&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;QString&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">world</span> :</span> <span class=\"keyword\">public</span> QObject</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    Q_OBJECT</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">world</span><span class=\"params\">(QObject *parent = <span class=\"literal\">nullptr</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">signals:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> slots:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">ReceiveHello</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">ReceiveHello</span><span class=\"params\">(QString mood)</span></span>;    <span class=\"comment\">//overload</span></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span> <span class=\"comment\">// WORLD_H</span></span></span><br></pre></td></tr></table></figure>\n<p>同时也要在主类中去声明这俩对象。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">    Ui::SignalAndSlot *ui;</span><br><span class=\"line\">    hello *he;</span><br><span class=\"line\">    world *wo;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">Ready</span><span class=\"params\">()</span></span>;</span><br></pre></td></tr></table></figure>\n\n<p>下面的Ready函数是用来触发条件的。里面一般是调用signal的函数。当然这个函数的实现应该是在cpp中。像这样：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">SignalAndSlot::Ready</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//emit he-&gt;CallWorld();</span></span><br><span class=\"line\">    emit he-&gt;<span class=\"built_in\">CallWorld</span>(<span class=\"string\">&quot;happy&quot;</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>至于这个emit，很迷，是个发射signal指令？不懂。</p>\n<p>然后就可以在这个cpp文件中进行connection了。注意这里需要重载，重载的操作使用的是函数指针来让connect明确要调用的是哪一个函数。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SignalAndSlot::<span class=\"built_in\">SignalAndSlot</span>(QWidget *parent)</span><br><span class=\"line\">    : <span class=\"built_in\">QWidget</span>(parent)</span><br><span class=\"line\">    , <span class=\"built_in\">ui</span>(<span class=\"keyword\">new</span> Ui::SignalAndSlot)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    ui-&gt;<span class=\"built_in\">setupUi</span>(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;he = <span class=\"keyword\">new</span> <span class=\"built_in\">hello</span>(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;wo = <span class=\"keyword\">new</span> <span class=\"built_in\">world</span>(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">    <span class=\"comment\">//connect(he, &amp;hello::CallWorld, wo, &amp;world::ReceiveHello);</span></span><br><span class=\"line\">    <span class=\"comment\">//overload</span></span><br><span class=\"line\">    <span class=\"built_in\"><span class=\"keyword\">void</span></span>(hello::*helloSignal)(QString) = &amp;hello::CallWorld;</span><br><span class=\"line\">    <span class=\"built_in\"><span class=\"keyword\">void</span></span>(world::*worldSlots)(QString) = &amp;world::ReceiveHello;</span><br><span class=\"line\">    <span class=\"built_in\">connect</span>(he, helloSignal, wo, worldSlots);</span><br><span class=\"line\">    <span class=\"built_in\">Ready</span>();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>另外别忘了include头文件。connect完了这个worldSlots就处于一个待触发的状态了。一旦Ready()触发，它就被调用。</p>\n<p>至于<code>word.cpp</code>和<code>hello.cpp</code>，当然是用来写signal和slots函数了，光在.h声明了也得实现呢。</p>\n<p>还有，一直没懂这个<code>main.cpp</code>以外的cpp文件是咋执行的，仔细一想鹊食，其它cpp都是对构造函数的定义，然后main中直接去调用它们的头文件就可以去调用某个类中的函数。原来是这样，我是笨蛋。</p>\n<p>总结一下就是基本都是用类对象及其构造函数操作的，找到点感觉了。</p>\n"},{"title":"关于卷积核多通道输出的理解）","date":"2021-04-10T04:07:15.000Z","_content":"\n## 理解如图所示\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210410200304596.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center)\n\n","source":"_posts/juanjihe-md.md","raw":"---\ntitle: 关于卷积核多通道输出的理解）\ndate: 2021-04-10 12:07:15\ntags: NLP的一些收获\n---\n\n## 理解如图所示\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210410200304596.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center)\n\n","slug":"juanjihe-md","published":1,"updated":"2021-07-24T04:11:34.032Z","_id":"ckrh97fqd0000m8oee4j386k8","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"理解如图所示\"><a href=\"#理解如图所示\" class=\"headerlink\" title=\"理解如图所示\"></a>理解如图所示</h2><p><img src=\"https://img-blog.csdnimg.cn/20210410200304596.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"理解如图所示\"><a href=\"#理解如图所示\" class=\"headerlink\" title=\"理解如图所示\"></a>理解如图所示</h2><p><img src=\"https://img-blog.csdnimg.cn/20210410200304596.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjA1Mjg4Ng==,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n"},{"title":"贝格方法计算椭圆周长","date":"2021-06-12T13:13:39.000Z","_content":"\n### 椭圆周长定积分公式\n由于椭圆的周长可以看作是很多$\\Delta x$与$\\Delta y$直角边构成的斜边的和。因此就是\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\sqrt{dx^2&plus;dy^2}\" title=\"\\sqrt{dx^2+dy^2}\" /></a>\n\n，此处为了简化直接用参数方程替换，就是\n\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?4\\times&space;\\int_{0}^{&space;\\frac{\\pi}{2}}&space;\\sqrt{a^2&space;sin\\theta&space;&plus;&space;b^2&space;cos\\theta}&space;d\\theta\" title=\"4\\times \\int_{0}^{ \\frac{\\pi}{2}} \\sqrt{a^2 sin\\theta + b^2 cos\\theta} d\\theta\" /></a>\n\n### 龙贝格积分法Matlab代码\n```Matlab\nfunction Romberg(fun,a,b,tol)\nM = 1;      %每次的步数\nk = 0;      %积分表的行\nh = b - a;  %最大步长\ntol1 = 1;\nR = zeros(10,10); %分配矩阵大小\nR(1,1) = h*(feval(fun,a) + feval(fun,b))/2; %第一个值\nwhile tol1 >= tol\n    k = k + 1;\n    h = h/2;\n    tmp = 0;\n    %一列中上下行的关系\n    for i = 1:M\n        tmp = tmp + fun(a + h*(2*i - 1));\n    end\n    R(k+1,1) = R(k,1)/2 + h*tmp;\n    %更新步数\n    M = 2*M;\n    %构造在同一行中，左右列元素的关系\n    for m = 1:min(k,3)\n        R(k + 1,m + 1) = R(k+1,m)+(R(k+1,m)-R(k,m))/(4^m-1);\n    end\n    %计算第四列的龙贝格的误差\n    tol1=abs(R(k,min(k,4))-R(k+1,min(k,4)));\nend\nq = R(k+1, 4)\nR\n```\n\n### 命令\n此处针对a = 20，b = 10的椭圆方程而言。\n```\n>> a = 0;\n>> b = pi/2;\n>> f = @(x)4*sqrt(400.*sin(x).*sin(x)+100.*cos(x).*cos(x));\n>> tol = 1e-4;\n>> Romberg(f,a,b,tol);\n````","source":"_posts/jisuanfangfa1-md.md","raw":"---\ntitle: 贝格方法计算椭圆周长\ndate: 2021-06-12 21:13:39\ntags: 数值计算\n---\n\n### 椭圆周长定积分公式\n由于椭圆的周长可以看作是很多$\\Delta x$与$\\Delta y$直角边构成的斜边的和。因此就是\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\sqrt{dx^2&plus;dy^2}\" title=\"\\sqrt{dx^2+dy^2}\" /></a>\n\n，此处为了简化直接用参数方程替换，就是\n\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?4\\times&space;\\int_{0}^{&space;\\frac{\\pi}{2}}&space;\\sqrt{a^2&space;sin\\theta&space;&plus;&space;b^2&space;cos\\theta}&space;d\\theta\" title=\"4\\times \\int_{0}^{ \\frac{\\pi}{2}} \\sqrt{a^2 sin\\theta + b^2 cos\\theta} d\\theta\" /></a>\n\n### 龙贝格积分法Matlab代码\n```Matlab\nfunction Romberg(fun,a,b,tol)\nM = 1;      %每次的步数\nk = 0;      %积分表的行\nh = b - a;  %最大步长\ntol1 = 1;\nR = zeros(10,10); %分配矩阵大小\nR(1,1) = h*(feval(fun,a) + feval(fun,b))/2; %第一个值\nwhile tol1 >= tol\n    k = k + 1;\n    h = h/2;\n    tmp = 0;\n    %一列中上下行的关系\n    for i = 1:M\n        tmp = tmp + fun(a + h*(2*i - 1));\n    end\n    R(k+1,1) = R(k,1)/2 + h*tmp;\n    %更新步数\n    M = 2*M;\n    %构造在同一行中，左右列元素的关系\n    for m = 1:min(k,3)\n        R(k + 1,m + 1) = R(k+1,m)+(R(k+1,m)-R(k,m))/(4^m-1);\n    end\n    %计算第四列的龙贝格的误差\n    tol1=abs(R(k,min(k,4))-R(k+1,min(k,4)));\nend\nq = R(k+1, 4)\nR\n```\n\n### 命令\n此处针对a = 20，b = 10的椭圆方程而言。\n```\n>> a = 0;\n>> b = pi/2;\n>> f = @(x)4*sqrt(400.*sin(x).*sin(x)+100.*cos(x).*cos(x));\n>> tol = 1e-4;\n>> Romberg(f,a,b,tol);\n````","slug":"jisuanfangfa1-md","published":1,"updated":"2021-07-24T04:20:35.793Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrh9lcce0000u3oee4aa0cle","content":"<h3 id=\"椭圆周长定积分公式\"><a href=\"#椭圆周长定积分公式\" class=\"headerlink\" title=\"椭圆周长定积分公式\"></a>椭圆周长定积分公式</h3><p>由于椭圆的周长可以看作是很多$\\Delta x$与$\\Delta y$直角边构成的斜边的和。因此就是</p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\sqrt{dx^2&plus;dy^2}\" title=\"\\sqrt{dx^2+dy^2}\" /></a></p>\n<p>，此处为了简化直接用参数方程替换，就是</p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?4\\times&space;\\int_{0}^{&space;\\frac{\\pi}{2}}&space;\\sqrt{a^2&space;sin\\theta&space;&plus;&space;b^2&space;cos\\theta}&space;d\\theta\" title=\"4\\times \\int_{0}^{ \\frac{\\pi}{2}} \\sqrt{a^2 sin\\theta + b^2 cos\\theta} d\\theta\" /></a></p>\n<h3 id=\"龙贝格积分法Matlab代码\"><a href=\"#龙贝格积分法Matlab代码\" class=\"headerlink\" title=\"龙贝格积分法Matlab代码\"></a>龙贝格积分法Matlab代码</h3><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">Romberg</span><span class=\"params\">(fun,a,b,tol)</span></span></span><br><span class=\"line\">M = <span class=\"number\">1</span>;      <span class=\"comment\">%每次的步数</span></span><br><span class=\"line\">k = <span class=\"number\">0</span>;      <span class=\"comment\">%积分表的行</span></span><br><span class=\"line\">h = b - a;  <span class=\"comment\">%最大步长</span></span><br><span class=\"line\">tol1 = <span class=\"number\">1</span>;</span><br><span class=\"line\">R = <span class=\"built_in\">zeros</span>(<span class=\"number\">10</span>,<span class=\"number\">10</span>); <span class=\"comment\">%分配矩阵大小</span></span><br><span class=\"line\">R(<span class=\"number\">1</span>,<span class=\"number\">1</span>) = h*(feval(fun,a) + feval(fun,b))/<span class=\"number\">2</span>; <span class=\"comment\">%第一个值</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> tol1 &gt;= tol</span><br><span class=\"line\">    k = k + <span class=\"number\">1</span>;</span><br><span class=\"line\">    h = h/<span class=\"number\">2</span>;</span><br><span class=\"line\">    tmp = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"comment\">%一列中上下行的关系</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:M</span><br><span class=\"line\">        tmp = tmp + fun(a + h*(<span class=\"number\">2</span>*<span class=\"built_in\">i</span> - <span class=\"number\">1</span>));</span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\">    R(k+<span class=\"number\">1</span>,<span class=\"number\">1</span>) = R(k,<span class=\"number\">1</span>)/<span class=\"number\">2</span> + h*tmp;</span><br><span class=\"line\">    <span class=\"comment\">%更新步数</span></span><br><span class=\"line\">    M = <span class=\"number\">2</span>*M;</span><br><span class=\"line\">    <span class=\"comment\">%构造在同一行中，左右列元素的关系</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> m = <span class=\"number\">1</span>:<span class=\"built_in\">min</span>(k,<span class=\"number\">3</span>)</span><br><span class=\"line\">        R(k + <span class=\"number\">1</span>,m + <span class=\"number\">1</span>) = R(k+<span class=\"number\">1</span>,m)+(R(k+<span class=\"number\">1</span>,m)-R(k,m))/(<span class=\"number\">4</span>^m<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\">    <span class=\"comment\">%计算第四列的龙贝格的误差</span></span><br><span class=\"line\">    tol1=<span class=\"built_in\">abs</span>(R(k,<span class=\"built_in\">min</span>(k,<span class=\"number\">4</span>))-R(k+<span class=\"number\">1</span>,<span class=\"built_in\">min</span>(k,<span class=\"number\">4</span>)));</span><br><span class=\"line\"><span class=\"keyword\">end</span></span><br><span class=\"line\">q = R(k+<span class=\"number\">1</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">R</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h3><p>此处针对a = 20，b = 10的椭圆方程而言。</p>\n<pre><code>&gt;&gt; a = 0;\n&gt;&gt; b = pi/2;\n&gt;&gt; f = @(x)4*sqrt(400.*sin(x).*sin(x)+100.*cos(x).*cos(x));\n&gt;&gt; tol = 1e-4;\n&gt;&gt; Romberg(f,a,b,tol);\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"椭圆周长定积分公式\"><a href=\"#椭圆周长定积分公式\" class=\"headerlink\" title=\"椭圆周长定积分公式\"></a>椭圆周长定积分公式</h3><p>由于椭圆的周长可以看作是很多$\\Delta x$与$\\Delta y$直角边构成的斜边的和。因此就是</p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\sqrt{dx^2&plus;dy^2}\" title=\"\\sqrt{dx^2+dy^2}\" /></a></p>\n<p>，此处为了简化直接用参数方程替换，就是</p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?4\\times&space;\\int_{0}^{&space;\\frac{\\pi}{2}}&space;\\sqrt{a^2&space;sin\\theta&space;&plus;&space;b^2&space;cos\\theta}&space;d\\theta\" title=\"4\\times \\int_{0}^{ \\frac{\\pi}{2}} \\sqrt{a^2 sin\\theta + b^2 cos\\theta} d\\theta\" /></a></p>\n<h3 id=\"龙贝格积分法Matlab代码\"><a href=\"#龙贝格积分法Matlab代码\" class=\"headerlink\" title=\"龙贝格积分法Matlab代码\"></a>龙贝格积分法Matlab代码</h3><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">Romberg</span><span class=\"params\">(fun,a,b,tol)</span></span></span><br><span class=\"line\">M = <span class=\"number\">1</span>;      <span class=\"comment\">%每次的步数</span></span><br><span class=\"line\">k = <span class=\"number\">0</span>;      <span class=\"comment\">%积分表的行</span></span><br><span class=\"line\">h = b - a;  <span class=\"comment\">%最大步长</span></span><br><span class=\"line\">tol1 = <span class=\"number\">1</span>;</span><br><span class=\"line\">R = <span class=\"built_in\">zeros</span>(<span class=\"number\">10</span>,<span class=\"number\">10</span>); <span class=\"comment\">%分配矩阵大小</span></span><br><span class=\"line\">R(<span class=\"number\">1</span>,<span class=\"number\">1</span>) = h*(feval(fun,a) + feval(fun,b))/<span class=\"number\">2</span>; <span class=\"comment\">%第一个值</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> tol1 &gt;= tol</span><br><span class=\"line\">    k = k + <span class=\"number\">1</span>;</span><br><span class=\"line\">    h = h/<span class=\"number\">2</span>;</span><br><span class=\"line\">    tmp = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"comment\">%一列中上下行的关系</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:M</span><br><span class=\"line\">        tmp = tmp + fun(a + h*(<span class=\"number\">2</span>*<span class=\"built_in\">i</span> - <span class=\"number\">1</span>));</span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\">    R(k+<span class=\"number\">1</span>,<span class=\"number\">1</span>) = R(k,<span class=\"number\">1</span>)/<span class=\"number\">2</span> + h*tmp;</span><br><span class=\"line\">    <span class=\"comment\">%更新步数</span></span><br><span class=\"line\">    M = <span class=\"number\">2</span>*M;</span><br><span class=\"line\">    <span class=\"comment\">%构造在同一行中，左右列元素的关系</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> m = <span class=\"number\">1</span>:<span class=\"built_in\">min</span>(k,<span class=\"number\">3</span>)</span><br><span class=\"line\">        R(k + <span class=\"number\">1</span>,m + <span class=\"number\">1</span>) = R(k+<span class=\"number\">1</span>,m)+(R(k+<span class=\"number\">1</span>,m)-R(k,m))/(<span class=\"number\">4</span>^m<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\">    <span class=\"comment\">%计算第四列的龙贝格的误差</span></span><br><span class=\"line\">    tol1=<span class=\"built_in\">abs</span>(R(k,<span class=\"built_in\">min</span>(k,<span class=\"number\">4</span>))-R(k+<span class=\"number\">1</span>,<span class=\"built_in\">min</span>(k,<span class=\"number\">4</span>)));</span><br><span class=\"line\"><span class=\"keyword\">end</span></span><br><span class=\"line\">q = R(k+<span class=\"number\">1</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">R</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h3><p>此处针对a = 20，b = 10的椭圆方程而言。</p>\n<pre><code>&gt;&gt; a = 0;\n&gt;&gt; b = pi/2;\n&gt;&gt; f = @(x)4*sqrt(400.*sin(x).*sin(x)+100.*cos(x).*cos(x));\n&gt;&gt; tol = 1e-4;\n&gt;&gt; Romberg(f,a,b,tol);\n</code></pre>\n"},{"title":"IOU","date":"2021-07-27T08:48:46.000Z","_content":"\n关于这个IOU，之前没接触过CV更没接触过物体检测，这个指标就不太清楚。\n\n这东西叫Intersection-over-union，它干啥的呢，物体检测是要定位出物体的bounding box的，在识别出bounding box的同时还得识别出物体的类别。为了评价这个bounding box的精度，就提出了IOU。\n\n它定义了两个bounding box的重叠度，表示出来就是：\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?IOU&space;=&space;\\frac{A\\cap&space;B}{A&space;\\cup&space;B}\" title=\"IOU = \\frac{A\\cap B}{A \\cup B}\" /></a>\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?IOU&space;=&space;\\frac{S_I}{S_A&space;&plus;&space;S_B&space;&plus;&space;S_I}\" title=\"IOU = \\frac{S_I}{S_A + S_B + S_I}\" /></a>\n\n就直观起来了。","source":"_posts/IOU-md.md","raw":"---\ntitle: IOU\ndate: 2021-07-27 16:48:46\ntags: CV\n---\n\n关于这个IOU，之前没接触过CV更没接触过物体检测，这个指标就不太清楚。\n\n这东西叫Intersection-over-union，它干啥的呢，物体检测是要定位出物体的bounding box的，在识别出bounding box的同时还得识别出物体的类别。为了评价这个bounding box的精度，就提出了IOU。\n\n它定义了两个bounding box的重叠度，表示出来就是：\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?IOU&space;=&space;\\frac{A\\cap&space;B}{A&space;\\cup&space;B}\" title=\"IOU = \\frac{A\\cap B}{A \\cup B}\" /></a>\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?IOU&space;=&space;\\frac{S_I}{S_A&space;&plus;&space;S_B&space;&plus;&space;S_I}\" title=\"IOU = \\frac{S_I}{S_A + S_B + S_I}\" /></a>\n\n就直观起来了。","slug":"IOU-md","published":1,"updated":"2021-07-27T09:02:06.626Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrltyy1n000035oe7v4x8722","content":"<p>关于这个IOU，之前没接触过CV更没接触过物体检测，这个指标就不太清楚。</p>\n<p>这东西叫Intersection-over-union，它干啥的呢，物体检测是要定位出物体的bounding box的，在识别出bounding box的同时还得识别出物体的类别。为了评价这个bounding box的精度，就提出了IOU。</p>\n<p>它定义了两个bounding box的重叠度，表示出来就是：</p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?IOU&space;=&space;\\frac{A\\cap&space;B}{A&space;\\cup&space;B}\" title=\"IOU = \\frac{A\\cap B}{A \\cup B}\" /></a></p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?IOU&space;=&space;\\frac{S_I}{S_A&space;&plus;&space;S_B&space;&plus;&space;S_I}\" title=\"IOU = \\frac{S_I}{S_A + S_B + S_I}\" /></a></p>\n<p>就直观起来了。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>关于这个IOU，之前没接触过CV更没接触过物体检测，这个指标就不太清楚。</p>\n<p>这东西叫Intersection-over-union，它干啥的呢，物体检测是要定位出物体的bounding box的，在识别出bounding box的同时还得识别出物体的类别。为了评价这个bounding box的精度，就提出了IOU。</p>\n<p>它定义了两个bounding box的重叠度，表示出来就是：</p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?IOU&space;=&space;\\frac{A\\cap&space;B}{A&space;\\cup&space;B}\" title=\"IOU = \\frac{A\\cap B}{A \\cup B}\" /></a></p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?IOU&space;=&space;\\frac{S_I}{S_A&space;&plus;&space;S_B&space;&plus;&space;S_I}\" title=\"IOU = \\frac{S_I}{S_A + S_B + S_I}\" /></a></p>\n<p>就直观起来了。</p>\n"},{"title":"BUTD","date":"2021-07-27T10:31:24.000Z","_content":"\n# Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering 总结\n\n论文地址: <a href = 'https://arxiv.org/pdf/1707.07998.pdf'>链接</a>\n\npytorch代码: <a href = 'https://github.com/ezeli/BUTD_model'>链接</a>\n\n这个文章2018年的，主要是提出了以一个使用Bottom-Up和Top-Down Attention模型相结合来解决Image Captioning和VQA的问题。\n\n先说VQA啊....一开始不清楚这是个啥，后来查了一下就是将图片和问题作为输入，然后组织出一条人类语言作为输出，用关总的话来说就是看图说话，当时还有点懵，现在直到他bb了个啥了，就这啊。\n\n其中Bottom-Up Attention指的是使用Faster R-CNN来对图像特征进行提取，Faster R-CNN没有采用滑动窗口（或者说Grid）来获取特征，而是采取采用提取Region Proposal的方法，少去了很多无用的feature。而且普通CNN用于对大样本的分类网路往往会很复杂。相当于对图像信息的Encode。\n\nTop-Down Attention指的是对上面网络得到的特征V进行权重的划分。它分为两个Layer，按顺序先是Top-Down Attention LSTM，然后是Language LSTM。一开始不明白VQA看这个部分的模型是懵的，Language LSTM存在的意义无从得知，查过后它的目的就很明确了，就是生成那段人类语言的输出，这里做的应该类似于NLP里的机器翻译，也是对序列的操作。它的输入序列即：\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?x_{t}^{2}&space;=&space;[\\hat{v}_t,&space;h^{1}_{t}]\" title=\"x_{t}^{2} = [\\hat{v}_t, h^{1}_{t}]\" /></a>\n\n前者为Faster R-CNN输出的feature，后者为第一个LSTM中每一个status的隐藏输出，然后再说这个LSTM的输入：\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?x_{1}^{2}&space;=&space;[h^{2}_{t&space;-&space;1},&space;\\bar{v},&space;W_e&space;\\Pi&space;_t]\" title=\"x_{1}^{2} = [h^{2}_{t - 1}, \\bar{v}, W_e \\Pi _t]\" /></a>\n\n所以它的输入序列包括Language LSTM的前一个status的隐藏输出，特征的均值，以及embedding matrix 和 one-hot向量的乘积，这个乘积我猜测它表达的结果是一个embedding vector，每一个status都会有一个特殊的one-hot vector，因此每个status的输入也是一个特殊的embedding vector。\n\nBaseLines方面它主要和使用Res NetCNN而非Bottom-Up的ResNet在captioning和VQA分别进行了比较，都取得了进步。","source":"_posts/BUTD-md.md","raw":"---\ntitle: BUTD\ndate: 2021-07-27 18:31:24\ntags: CV\n---\n\n# Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering 总结\n\n论文地址: <a href = 'https://arxiv.org/pdf/1707.07998.pdf'>链接</a>\n\npytorch代码: <a href = 'https://github.com/ezeli/BUTD_model'>链接</a>\n\n这个文章2018年的，主要是提出了以一个使用Bottom-Up和Top-Down Attention模型相结合来解决Image Captioning和VQA的问题。\n\n先说VQA啊....一开始不清楚这是个啥，后来查了一下就是将图片和问题作为输入，然后组织出一条人类语言作为输出，用关总的话来说就是看图说话，当时还有点懵，现在直到他bb了个啥了，就这啊。\n\n其中Bottom-Up Attention指的是使用Faster R-CNN来对图像特征进行提取，Faster R-CNN没有采用滑动窗口（或者说Grid）来获取特征，而是采取采用提取Region Proposal的方法，少去了很多无用的feature。而且普通CNN用于对大样本的分类网路往往会很复杂。相当于对图像信息的Encode。\n\nTop-Down Attention指的是对上面网络得到的特征V进行权重的划分。它分为两个Layer，按顺序先是Top-Down Attention LSTM，然后是Language LSTM。一开始不明白VQA看这个部分的模型是懵的，Language LSTM存在的意义无从得知，查过后它的目的就很明确了，就是生成那段人类语言的输出，这里做的应该类似于NLP里的机器翻译，也是对序列的操作。它的输入序列即：\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?x_{t}^{2}&space;=&space;[\\hat{v}_t,&space;h^{1}_{t}]\" title=\"x_{t}^{2} = [\\hat{v}_t, h^{1}_{t}]\" /></a>\n\n前者为Faster R-CNN输出的feature，后者为第一个LSTM中每一个status的隐藏输出，然后再说这个LSTM的输入：\n\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?x_{1}^{2}&space;=&space;[h^{2}_{t&space;-&space;1},&space;\\bar{v},&space;W_e&space;\\Pi&space;_t]\" title=\"x_{1}^{2} = [h^{2}_{t - 1}, \\bar{v}, W_e \\Pi _t]\" /></a>\n\n所以它的输入序列包括Language LSTM的前一个status的隐藏输出，特征的均值，以及embedding matrix 和 one-hot向量的乘积，这个乘积我猜测它表达的结果是一个embedding vector，每一个status都会有一个特殊的one-hot vector，因此每个status的输入也是一个特殊的embedding vector。\n\nBaseLines方面它主要和使用Res NetCNN而非Bottom-Up的ResNet在captioning和VQA分别进行了比较，都取得了进步。","slug":"BUTD-md","published":1,"updated":"2021-07-27T11:52:40.470Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrm02c0v000086oecbzc6w63","content":"<h1 id=\"Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering-总结\"><a href=\"#Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering-总结\" class=\"headerlink\" title=\"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering 总结\"></a>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering 总结</h1><p>论文地址: <a href = 'https://arxiv.org/pdf/1707.07998.pdf'>链接</a></p>\n<p>pytorch代码: <a href = 'https://github.com/ezeli/BUTD_model'>链接</a></p>\n<p>这个文章2018年的，主要是提出了以一个使用Bottom-Up和Top-Down Attention模型相结合来解决Image Captioning和VQA的问题。</p>\n<p>先说VQA啊….一开始不清楚这是个啥，后来查了一下就是将图片和问题作为输入，然后组织出一条人类语言作为输出，用关总的话来说就是看图说话，当时还有点懵，现在直到他bb了个啥了，就这啊。</p>\n<p>其中Bottom-Up Attention指的是使用Faster R-CNN来对图像特征进行提取，Faster R-CNN没有采用滑动窗口（或者说Grid）来获取特征，而是采取采用提取Region Proposal的方法，少去了很多无用的feature。而且普通CNN用于对大样本的分类网路往往会很复杂。相当于对图像信息的Encode。</p>\n<p>Top-Down Attention指的是对上面网络得到的特征V进行权重的划分。它分为两个Layer，按顺序先是Top-Down Attention LSTM，然后是Language LSTM。一开始不明白VQA看这个部分的模型是懵的，Language LSTM存在的意义无从得知，查过后它的目的就很明确了，就是生成那段人类语言的输出，这里做的应该类似于NLP里的机器翻译，也是对序列的操作。它的输入序列即：</p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?x_{t}^{2}&space;=&space;[\\hat{v}_t,&space;h^{1}_{t}]\" title=\"x_{t}^{2} = [\\hat{v}_t, h^{1}_{t}]\" /></a></p>\n<p>前者为Faster R-CNN输出的feature，后者为第一个LSTM中每一个status的隐藏输出，然后再说这个LSTM的输入：</p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?x_{1}^{2}&space;=&space;[h^{2}_{t&space;-&space;1},&space;\\bar{v},&space;W_e&space;\\Pi&space;_t]\" title=\"x_{1}^{2} = [h^{2}_{t - 1}, \\bar{v}, W_e \\Pi _t]\" /></a></p>\n<p>所以它的输入序列包括Language LSTM的前一个status的隐藏输出，特征的均值，以及embedding matrix 和 one-hot向量的乘积，这个乘积我猜测它表达的结果是一个embedding vector，每一个status都会有一个特殊的one-hot vector，因此每个status的输入也是一个特殊的embedding vector。</p>\n<p>BaseLines方面它主要和使用Res NetCNN而非Bottom-Up的ResNet在captioning和VQA分别进行了比较，都取得了进步。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering-总结\"><a href=\"#Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering-总结\" class=\"headerlink\" title=\"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering 总结\"></a>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering 总结</h1><p>论文地址: <a href = 'https://arxiv.org/pdf/1707.07998.pdf'>链接</a></p>\n<p>pytorch代码: <a href = 'https://github.com/ezeli/BUTD_model'>链接</a></p>\n<p>这个文章2018年的，主要是提出了以一个使用Bottom-Up和Top-Down Attention模型相结合来解决Image Captioning和VQA的问题。</p>\n<p>先说VQA啊….一开始不清楚这是个啥，后来查了一下就是将图片和问题作为输入，然后组织出一条人类语言作为输出，用关总的话来说就是看图说话，当时还有点懵，现在直到他bb了个啥了，就这啊。</p>\n<p>其中Bottom-Up Attention指的是使用Faster R-CNN来对图像特征进行提取，Faster R-CNN没有采用滑动窗口（或者说Grid）来获取特征，而是采取采用提取Region Proposal的方法，少去了很多无用的feature。而且普通CNN用于对大样本的分类网路往往会很复杂。相当于对图像信息的Encode。</p>\n<p>Top-Down Attention指的是对上面网络得到的特征V进行权重的划分。它分为两个Layer，按顺序先是Top-Down Attention LSTM，然后是Language LSTM。一开始不明白VQA看这个部分的模型是懵的，Language LSTM存在的意义无从得知，查过后它的目的就很明确了，就是生成那段人类语言的输出，这里做的应该类似于NLP里的机器翻译，也是对序列的操作。它的输入序列即：</p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?x_{t}^{2}&space;=&space;[\\hat{v}_t,&space;h^{1}_{t}]\" title=\"x_{t}^{2} = [\\hat{v}_t, h^{1}_{t}]\" /></a></p>\n<p>前者为Faster R-CNN输出的feature，后者为第一个LSTM中每一个status的隐藏输出，然后再说这个LSTM的输入：</p>\n<p><a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?x_{1}^{2}&space;=&space;[h^{2}_{t&space;-&space;1},&space;\\bar{v},&space;W_e&space;\\Pi&space;_t]\" title=\"x_{1}^{2} = [h^{2}_{t - 1}, \\bar{v}, W_e \\Pi _t]\" /></a></p>\n<p>所以它的输入序列包括Language LSTM的前一个status的隐藏输出，特征的均值，以及embedding matrix 和 one-hot向量的乘积，这个乘积我猜测它表达的结果是一个embedding vector，每一个status都会有一个特殊的one-hot vector，因此每个status的输入也是一个特殊的embedding vector。</p>\n<p>BaseLines方面它主要和使用Res NetCNN而非Bottom-Up的ResNet在captioning和VQA分别进行了比较，都取得了进步。</p>\n"},{"title":"NDCG.md","date":"2021-08-01T11:29:20.000Z","_content":"","source":"_posts/NDCG-md.md","raw":"---\ntitle: NDCG.md\ndate: 2021-08-01 19:29:20\ntags:\n---\n","slug":"NDCG-md","published":1,"updated":"2021-08-01T11:29:20.158Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cks0dw14x0000gfoeh7ck6rvp","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"STGN-Baseline代码修改","date":"2021-08-06T12:39:44.000Z","_content":"# Where to Go Next: A Spatio-Temporal Gated Network for Next POI Recommendation\n\n最近要给一个模型跑baseline，需要拿STGN做对比，所以就拿这个的代码进行修改。\n\nSTGN是一个基于时间和距离差的门结构的RNN推荐系统，模型不算复杂。\n\n数据处理方面也还是很轻松的，困难的在这个模型的训练没有设置早停，也没有计算NDCG。本来以为工作量不大，结果发现这是tensorflow写的。我一开始的思路是在train的Session里创建两个Graph或者用一个Graph但是要修改模型参数，其中一个用于train，另外一个用于predict。搜索了很多，好像没这么干的....后来问了学长，我感觉自己像个傻子，直接在训练用的graph里进行预测不就可以，一开始我的顾虑是二者input的shape不同，因为一个batch_size是10，一个是1。后来我联想到了训练时padding的方法，由于predict的是一个poi的下一个poi，所以在其余9个item的位置补0即可。发现真的可以，效果也不错，看来也没完全傻。","source":"_posts/STGN-process-md.md","raw":"---\ntitle: STGN-Baseline代码修改\ndate: 2021-08-06 20:39:44\ntags: NLP的一些收获\n---\n# Where to Go Next: A Spatio-Temporal Gated Network for Next POI Recommendation\n\n最近要给一个模型跑baseline，需要拿STGN做对比，所以就拿这个的代码进行修改。\n\nSTGN是一个基于时间和距离差的门结构的RNN推荐系统，模型不算复杂。\n\n数据处理方面也还是很轻松的，困难的在这个模型的训练没有设置早停，也没有计算NDCG。本来以为工作量不大，结果发现这是tensorflow写的。我一开始的思路是在train的Session里创建两个Graph或者用一个Graph但是要修改模型参数，其中一个用于train，另外一个用于predict。搜索了很多，好像没这么干的....后来问了学长，我感觉自己像个傻子，直接在训练用的graph里进行预测不就可以，一开始我的顾虑是二者input的shape不同，因为一个batch_size是10，一个是1。后来我联想到了训练时padding的方法，由于predict的是一个poi的下一个poi，所以在其余9个item的位置补0即可。发现真的可以，效果也不错，看来也没完全傻。","slug":"STGN-process-md","published":1,"updated":"2021-08-09T11:36:46.971Z","_id":"cks0dw1520001gfoe3epf7p0s","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Where-to-Go-Next-A-Spatio-Temporal-Gated-Network-for-Next-POI-Recommendation\"><a href=\"#Where-to-Go-Next-A-Spatio-Temporal-Gated-Network-for-Next-POI-Recommendation\" class=\"headerlink\" title=\"Where to Go Next: A Spatio-Temporal Gated Network for Next POI Recommendation\"></a>Where to Go Next: A Spatio-Temporal Gated Network for Next POI Recommendation</h1><p>最近要给一个模型跑baseline，需要拿STGN做对比，所以就拿这个的代码进行修改。</p>\n<p>STGN是一个基于时间和距离差的门结构的RNN推荐系统，模型不算复杂。</p>\n<p>数据处理方面也还是很轻松的，困难的在这个模型的训练没有设置早停，也没有计算NDCG。本来以为工作量不大，结果发现这是tensorflow写的。我一开始的思路是在train的Session里创建两个Graph或者用一个Graph但是要修改模型参数，其中一个用于train，另外一个用于predict。搜索了很多，好像没这么干的….后来问了学长，我感觉自己像个傻子，直接在训练用的graph里进行预测不就可以，一开始我的顾虑是二者input的shape不同，因为一个batch_size是10，一个是1。后来我联想到了训练时padding的方法，由于predict的是一个poi的下一个poi，所以在其余9个item的位置补0即可。发现真的可以，效果也不错，看来也没完全傻。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Where-to-Go-Next-A-Spatio-Temporal-Gated-Network-for-Next-POI-Recommendation\"><a href=\"#Where-to-Go-Next-A-Spatio-Temporal-Gated-Network-for-Next-POI-Recommendation\" class=\"headerlink\" title=\"Where to Go Next: A Spatio-Temporal Gated Network for Next POI Recommendation\"></a>Where to Go Next: A Spatio-Temporal Gated Network for Next POI Recommendation</h1><p>最近要给一个模型跑baseline，需要拿STGN做对比，所以就拿这个的代码进行修改。</p>\n<p>STGN是一个基于时间和距离差的门结构的RNN推荐系统，模型不算复杂。</p>\n<p>数据处理方面也还是很轻松的，困难的在这个模型的训练没有设置早停，也没有计算NDCG。本来以为工作量不大，结果发现这是tensorflow写的。我一开始的思路是在train的Session里创建两个Graph或者用一个Graph但是要修改模型参数，其中一个用于train，另外一个用于predict。搜索了很多，好像没这么干的….后来问了学长，我感觉自己像个傻子，直接在训练用的graph里进行预测不就可以，一开始我的顾虑是二者input的shape不同，因为一个batch_size是10，一个是1。后来我联想到了训练时padding的方法，由于predict的是一个poi的下一个poi，所以在其余9个item的位置补0即可。发现真的可以，效果也不错，看来也没完全傻。</p>\n"},{"title":"BLEU","date":"2021-08-17T10:23:27.000Z","_content":"\n# 一种机器翻译的评估方法 BLEU\n\n<a href='https://aclanthology.org/P02-1040.pdf'>论文链接 BLEU: a Method for Automatic Evaluation of Machine Translation</a>\n\n首先给出一组reference和candidate：\n\nCandidate1：It is a guide to action which ensures that the military always obeys the commands of the party.\n\nCandidate2：It is to insure the troops forever hearing the activity guidebook that party direct.\n\nReference1：It is a guide to action that ensures that the military will forever heed Party commands.\n\nReference2：It is the guiding principle which guarantees the military forces always being under the command of the Party.\n\nReference3：It is the practical guide for the army always to heed the directions of the party.\n\n任务是对两个候选案例进行评估。\n\n论文基于词交集和ngram短语交集设计了如下算法进行评估：\n\n<center>\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?BLEU&space;=&space;BP\\cdot&space;exp(\\sum^{N}_{n&space;=&space;1}\\omega_n&space;logP_n&space;)\" title=\"BLEU = BP\\cdot exp(\\sum^{N}_{n = 1}\\omega_n logP_n )\" /></a>\n</center>\n\n\n其中：\n<center>\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?BP&space;=&space;\\left\\{\\begin{matrix}&space;1&space;&&space;if&space;~c&space;>&space;r\\\\&space;e^{(1-\\frac{r}{c})}&space;&&space;if&space;~c&space;\\leq&space;r&space;\\end{matrix}\\right.\" title=\"BP = \\left\\{\\begin{matrix} 1 & if ~c > r\\\\ e^{(1-\\frac{r}{c})} & if ~c \\leq r \\end{matrix}\\right.\" /></a>\n</center>\n\n可以看出这个算法由两部分组成，BP和改进后的ngram。\n\n## Modified Ngram\n\n首先说明这个指标是针对于词的出现次数进行评估的。\n\n它也可以分为两部分，因为它显然是一个值与该位置权重的乘积的累加，也就是wn和log(Pn)，这里的n其实就是ngram的n。指的是词中word的个数。wn就是针对于不同的n的权重。\n\n在计算ngram的值时，引入Min和Max。其中Max表示某一个词在n个reference中的出现次数的最大值，Min表示的是某一个词在Candidate中出现的次数和Max中的最小值。然后再求Min与候选词数和的比值。因此可以看出这个指标是对过长的词句有明显的处罚，因为当Candidate中的word没有在reference中出现时，Min的值必为0而候选词的数目会增加。\n\n## BP\n\nBrevity Penalty补足Ngram的缺陷，对过短的candidate进行处罚，其中c是candidate中每一个句子的长度，r是refernce中最接近c的长度。当c越短时，这个系数当然会越小，指标值越小，很合理。\n\n## Merge\n\n将二者整合在一起就是BLEU，取值范围[0,1]，越大越好。\n\n","source":"_posts/BLEU-md.md","raw":"---\ntitle: BLEU\ndate: 2021-08-17 18:23:27\ntags: NLP的一些收获\n---\n\n# 一种机器翻译的评估方法 BLEU\n\n<a href='https://aclanthology.org/P02-1040.pdf'>论文链接 BLEU: a Method for Automatic Evaluation of Machine Translation</a>\n\n首先给出一组reference和candidate：\n\nCandidate1：It is a guide to action which ensures that the military always obeys the commands of the party.\n\nCandidate2：It is to insure the troops forever hearing the activity guidebook that party direct.\n\nReference1：It is a guide to action that ensures that the military will forever heed Party commands.\n\nReference2：It is the guiding principle which guarantees the military forces always being under the command of the Party.\n\nReference3：It is the practical guide for the army always to heed the directions of the party.\n\n任务是对两个候选案例进行评估。\n\n论文基于词交集和ngram短语交集设计了如下算法进行评估：\n\n<center>\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?BLEU&space;=&space;BP\\cdot&space;exp(\\sum^{N}_{n&space;=&space;1}\\omega_n&space;logP_n&space;)\" title=\"BLEU = BP\\cdot exp(\\sum^{N}_{n = 1}\\omega_n logP_n )\" /></a>\n</center>\n\n\n其中：\n<center>\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?BP&space;=&space;\\left\\{\\begin{matrix}&space;1&space;&&space;if&space;~c&space;>&space;r\\\\&space;e^{(1-\\frac{r}{c})}&space;&&space;if&space;~c&space;\\leq&space;r&space;\\end{matrix}\\right.\" title=\"BP = \\left\\{\\begin{matrix} 1 & if ~c > r\\\\ e^{(1-\\frac{r}{c})} & if ~c \\leq r \\end{matrix}\\right.\" /></a>\n</center>\n\n可以看出这个算法由两部分组成，BP和改进后的ngram。\n\n## Modified Ngram\n\n首先说明这个指标是针对于词的出现次数进行评估的。\n\n它也可以分为两部分，因为它显然是一个值与该位置权重的乘积的累加，也就是wn和log(Pn)，这里的n其实就是ngram的n。指的是词中word的个数。wn就是针对于不同的n的权重。\n\n在计算ngram的值时，引入Min和Max。其中Max表示某一个词在n个reference中的出现次数的最大值，Min表示的是某一个词在Candidate中出现的次数和Max中的最小值。然后再求Min与候选词数和的比值。因此可以看出这个指标是对过长的词句有明显的处罚，因为当Candidate中的word没有在reference中出现时，Min的值必为0而候选词的数目会增加。\n\n## BP\n\nBrevity Penalty补足Ngram的缺陷，对过短的candidate进行处罚，其中c是candidate中每一个句子的长度，r是refernce中最接近c的长度。当c越短时，这个系数当然会越小，指标值越小，很合理。\n\n## Merge\n\n将二者整合在一起就是BLEU，取值范围[0,1]，越大越好。\n\n","slug":"BLEU-md","published":1,"updated":"2021-08-17T13:40:50.712Z","_id":"cksg44og80000l3oe92nefynw","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"一种机器翻译的评估方法-BLEU\"><a href=\"#一种机器翻译的评估方法-BLEU\" class=\"headerlink\" title=\"一种机器翻译的评估方法 BLEU\"></a>一种机器翻译的评估方法 BLEU</h1><p><a href='https://aclanthology.org/P02-1040.pdf'>论文链接 BLEU: a Method for Automatic Evaluation of Machine Translation</a></p>\n<p>首先给出一组reference和candidate：</p>\n<p>Candidate1：It is a guide to action which ensures that the military always obeys the commands of the party.</p>\n<p>Candidate2：It is to insure the troops forever hearing the activity guidebook that party direct.</p>\n<p>Reference1：It is a guide to action that ensures that the military will forever heed Party commands.</p>\n<p>Reference2：It is the guiding principle which guarantees the military forces always being under the command of the Party.</p>\n<p>Reference3：It is the practical guide for the army always to heed the directions of the party.</p>\n<p>任务是对两个候选案例进行评估。</p>\n<p>论文基于词交集和ngram短语交集设计了如下算法进行评估：</p>\n<center>\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?BLEU&space;=&space;BP\\cdot&space;exp(\\sum^{N}_{n&space;=&space;1}\\omega_n&space;logP_n&space;)\" title=\"BLEU = BP\\cdot exp(\\sum^{N}_{n = 1}\\omega_n logP_n )\" /></a>\n</center>\n\n\n<p>其中：</p>\n<center>\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?BP&space;=&space;\\left\\{\\begin{matrix}&space;1&space;&&space;if&space;~c&space;>&space;r\\\\&space;e^{(1-\\frac{r}{c})}&space;&&space;if&space;~c&space;\\leq&space;r&space;\\end{matrix}\\right.\" title=\"BP = \\left\\{\\begin{matrix} 1 & if ~c > r\\\\ e^{(1-\\frac{r}{c})} & if ~c \\leq r \\end{matrix}\\right.\" /></a>\n</center>\n\n<p>可以看出这个算法由两部分组成，BP和改进后的ngram。</p>\n<h2 id=\"Modified-Ngram\"><a href=\"#Modified-Ngram\" class=\"headerlink\" title=\"Modified Ngram\"></a>Modified Ngram</h2><p>首先说明这个指标是针对于词的出现次数进行评估的。</p>\n<p>它也可以分为两部分，因为它显然是一个值与该位置权重的乘积的累加，也就是wn和log(Pn)，这里的n其实就是ngram的n。指的是词中word的个数。wn就是针对于不同的n的权重。</p>\n<p>在计算ngram的值时，引入Min和Max。其中Max表示某一个词在n个reference中的出现次数的最大值，Min表示的是某一个词在Candidate中出现的次数和Max中的最小值。然后再求Min与候选词数和的比值。因此可以看出这个指标是对过长的词句有明显的处罚，因为当Candidate中的word没有在reference中出现时，Min的值必为0而候选词的数目会增加。</p>\n<h2 id=\"BP\"><a href=\"#BP\" class=\"headerlink\" title=\"BP\"></a>BP</h2><p>Brevity Penalty补足Ngram的缺陷，对过短的candidate进行处罚，其中c是candidate中每一个句子的长度，r是refernce中最接近c的长度。当c越短时，这个系数当然会越小，指标值越小，很合理。</p>\n<h2 id=\"Merge\"><a href=\"#Merge\" class=\"headerlink\" title=\"Merge\"></a>Merge</h2><p>将二者整合在一起就是BLEU，取值范围[0,1]，越大越好。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"一种机器翻译的评估方法-BLEU\"><a href=\"#一种机器翻译的评估方法-BLEU\" class=\"headerlink\" title=\"一种机器翻译的评估方法 BLEU\"></a>一种机器翻译的评估方法 BLEU</h1><p><a href='https://aclanthology.org/P02-1040.pdf'>论文链接 BLEU: a Method for Automatic Evaluation of Machine Translation</a></p>\n<p>首先给出一组reference和candidate：</p>\n<p>Candidate1：It is a guide to action which ensures that the military always obeys the commands of the party.</p>\n<p>Candidate2：It is to insure the troops forever hearing the activity guidebook that party direct.</p>\n<p>Reference1：It is a guide to action that ensures that the military will forever heed Party commands.</p>\n<p>Reference2：It is the guiding principle which guarantees the military forces always being under the command of the Party.</p>\n<p>Reference3：It is the practical guide for the army always to heed the directions of the party.</p>\n<p>任务是对两个候选案例进行评估。</p>\n<p>论文基于词交集和ngram短语交集设计了如下算法进行评估：</p>\n<center>\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?BLEU&space;=&space;BP\\cdot&space;exp(\\sum^{N}_{n&space;=&space;1}\\omega_n&space;logP_n&space;)\" title=\"BLEU = BP\\cdot exp(\\sum^{N}_{n = 1}\\omega_n logP_n )\" /></a>\n</center>\n\n\n<p>其中：</p>\n<center>\n<a target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?BP&space;=&space;\\left\\{\\begin{matrix}&space;1&space;&&space;if&space;~c&space;>&space;r\\\\&space;e^{(1-\\frac{r}{c})}&space;&&space;if&space;~c&space;\\leq&space;r&space;\\end{matrix}\\right.\" title=\"BP = \\left\\{\\begin{matrix} 1 & if ~c > r\\\\ e^{(1-\\frac{r}{c})} & if ~c \\leq r \\end{matrix}\\right.\" /></a>\n</center>\n\n<p>可以看出这个算法由两部分组成，BP和改进后的ngram。</p>\n<h2 id=\"Modified-Ngram\"><a href=\"#Modified-Ngram\" class=\"headerlink\" title=\"Modified Ngram\"></a>Modified Ngram</h2><p>首先说明这个指标是针对于词的出现次数进行评估的。</p>\n<p>它也可以分为两部分，因为它显然是一个值与该位置权重的乘积的累加，也就是wn和log(Pn)，这里的n其实就是ngram的n。指的是词中word的个数。wn就是针对于不同的n的权重。</p>\n<p>在计算ngram的值时，引入Min和Max。其中Max表示某一个词在n个reference中的出现次数的最大值，Min表示的是某一个词在Candidate中出现的次数和Max中的最小值。然后再求Min与候选词数和的比值。因此可以看出这个指标是对过长的词句有明显的处罚，因为当Candidate中的word没有在reference中出现时，Min的值必为0而候选词的数目会增加。</p>\n<h2 id=\"BP\"><a href=\"#BP\" class=\"headerlink\" title=\"BP\"></a>BP</h2><p>Brevity Penalty补足Ngram的缺陷，对过短的candidate进行处罚，其中c是candidate中每一个句子的长度，r是refernce中最接近c的长度。当c越短时，这个系数当然会越小，指标值越小，很合理。</p>\n<h2 id=\"Merge\"><a href=\"#Merge\" class=\"headerlink\" title=\"Merge\"></a>Merge</h2><p>将二者整合在一起就是BLEU，取值范围[0,1]，越大越好。</p>\n"},{"title":"模型的评估与选择","date":"2021-09-16T00:33:09.000Z","_content":"\n## 评估方法\n\n通常将包含m样本的数据集划分为训练集和测试集。具体的拆分方法：\n\n### 留出法\n\n直接将数据集划分为两个互斥的集合，分别作为测试集和训练集。\n\n在训练集中训练出模型后，用测试集来评估测试误差，作为对泛化误差的估计。\n\n注意：\n\n数据集的划分要尽可能保持数据分布的一致性（分层采样）。\n\n一般若干次随机划分、重复进行实验评估后取平均值。\n\n比例2:1～4:1。\n\n### 交叉验证法\n\n将数据集分层采样划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的子集作为测试集，最终返回k个测试结果的均值，k最常用的取值是10，叫做10折交叉验证。\n\n其中将数据集划分为k个子集时为了减小因样本划分不同而引入的差别，k折交叉验证通常随机使用不同的划分重复p次，最终结果为p次k折交叉验证结果的均值。\n\n尤其地，当数据集包含m个样本，令k = m，则得到留一法。它不受随机样本划分的影响（只有一种）。结果较为准确。但是数据大的时候开销难以忍受。\n\n### 自助法(Bootstrapping)\n\n给定数据集$D$，对它采样生成D'，即每次从随机从D挑选一个样本放入D'，然后再放回D中。重复m次，得到包含m个样本的数据集D'。\n\n再将D' 最为训练集，D/D' 作为测试集（0.37m个样本）。\n\n数据集小时很有用。但是改变了初始数据集的分布，会引入估计偏差。\n\n## 调参与验证集\n\n参数分为超参和普通参数。\n\n超参一般为神经网络的层数，神经元个数等等。普通参数一般为权重系数等。\n\n验证集合时从训练集中再次划分为训练集和验证集，用来选择超参，选择出最优的超参再放到测试集去跑。一般测试集的结果会低一些。\n\n\n## 性能度量\n\n在回归任务中，性能度量通常使用均方误差（MSE）。\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}){i&space;=&space;1}(f(x_i)&space;-&space;y_i)^2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}){i&space;=&space;1}(f(x_i)&space;-&space;y_i)^2\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}){i = 1}(f(x_i) - y_i)^2\" /></a>\n</center>\n\n在分类任务中，经常用到错误率和精度。\n\n分类错误率：\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;\\neq&space;y_i)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;\\neq&space;y_i)\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}{i = 1}\\Pi (f(x_i) \\neq y_i)\" /></a>\n</center\n>\n精度：\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=1&space;-&space;E(f;D)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?1&space;-&space;E(f;D)\" title=\"1 - E(f;D)\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;=&space;y_i)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;=&space;y_i)\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}{i = 1}\\Pi (f(x_i) = y_i)\" /></a>\n</center>\n\n关于混淆矩阵，也是分类任务，对于真实类别为正例时，若预测为正例则为True Positive否则为False Negative。对于真实类别为负例时，若预测为正例则为False Positive否则为True Negative。\n\n于是给出查准率（Precision）：\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=P&space;=&space;\\frac{TP}{TP&space;&plus;&space;FP}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?P&space;=&space;\\frac{TP}{TP&space;&plus;&space;FP}\" title=\"P = \\frac{TP}{TP + FP}\" /></a>\n</center>\n\n与召回率（Recall）：\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=R&space;=&space;\\frac{TP}{TP&space;&plus;&space;FN}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?R&space;=&space;\\frac{TP}{TP&space;&plus;&space;FN}\" title=\"R = \\frac{TP}{TP + FN}\" /></a>\n</center>\n\n由于二者经常变化矛盾，于是提出P-R曲线。\n\n除了PR曲线，也可以使用另外两种度量：\n\nF1度量：\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=F1&space;=&space;\\frac{2\\times&space;P&space;\\times&space;R}{P&space;&plus;&space;R}&space;=&space;\\frac{2&space;\\times&space;TP}{m&space;&plus;&space;TP&space;-&space;TN}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?F1&space;=&space;\\frac{2\\times&space;P&space;\\times&space;R}{P&space;&plus;&space;R}&space;=&space;\\frac{2&space;\\times&space;TP}{m&space;&plus;&space;TP&space;-&space;TN}\" title=\"F1 = \\frac{2\\times P \\times R}{P + R} = \\frac{2 \\times TP}{m + TP - TN}\" /></a>\n</center>\n\nF_{\\beta}度量:\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=F_{\\beta}&space;=&space;\\frac{(1&space;&plus;&space;{\\beta}^2&space;\\times&space;P&space;\\times&space;R)}{(\\beta&space;^2&space;\\times&space;P)&space;&plus;&space;R}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?F_{\\beta}&space;=&space;\\frac{(1&space;&plus;&space;{\\beta}^2&space;\\times&space;P&space;\\times&space;R)}{(\\beta&space;^2&space;\\times&space;P)&space;&plus;&space;R}\" title=\"F_{\\beta} = \\frac{(1 + {\\beta}^2 \\times P \\times R)}{(\\beta ^2 \\times P) + R}\" /></a>\n</center>\n\n另外，也可以使用宏平均，算出多个二分类混淆矩阵上计算出平均macro-P与macro-R，再去计算F1。也存在微平均，不过是先计算矩阵元素的平均值再去计算P与R。\n\n为了衡量犯下不同错误的不同的代价，提出了代价敏感错误率。cost01为将0预测为1，cost10为将1预测为0。","source":"_posts/model-eval-md.md","raw":"---\ntitle: 模型的评估与选择\ndate: 2021-09-16 08:33:09\ntags: 机器学习\n---\n\n## 评估方法\n\n通常将包含m样本的数据集划分为训练集和测试集。具体的拆分方法：\n\n### 留出法\n\n直接将数据集划分为两个互斥的集合，分别作为测试集和训练集。\n\n在训练集中训练出模型后，用测试集来评估测试误差，作为对泛化误差的估计。\n\n注意：\n\n数据集的划分要尽可能保持数据分布的一致性（分层采样）。\n\n一般若干次随机划分、重复进行实验评估后取平均值。\n\n比例2:1～4:1。\n\n### 交叉验证法\n\n将数据集分层采样划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的子集作为测试集，最终返回k个测试结果的均值，k最常用的取值是10，叫做10折交叉验证。\n\n其中将数据集划分为k个子集时为了减小因样本划分不同而引入的差别，k折交叉验证通常随机使用不同的划分重复p次，最终结果为p次k折交叉验证结果的均值。\n\n尤其地，当数据集包含m个样本，令k = m，则得到留一法。它不受随机样本划分的影响（只有一种）。结果较为准确。但是数据大的时候开销难以忍受。\n\n### 自助法(Bootstrapping)\n\n给定数据集$D$，对它采样生成D'，即每次从随机从D挑选一个样本放入D'，然后再放回D中。重复m次，得到包含m个样本的数据集D'。\n\n再将D' 最为训练集，D/D' 作为测试集（0.37m个样本）。\n\n数据集小时很有用。但是改变了初始数据集的分布，会引入估计偏差。\n\n## 调参与验证集\n\n参数分为超参和普通参数。\n\n超参一般为神经网络的层数，神经元个数等等。普通参数一般为权重系数等。\n\n验证集合时从训练集中再次划分为训练集和验证集，用来选择超参，选择出最优的超参再放到测试集去跑。一般测试集的结果会低一些。\n\n\n## 性能度量\n\n在回归任务中，性能度量通常使用均方误差（MSE）。\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}){i&space;=&space;1}(f(x_i)&space;-&space;y_i)^2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}){i&space;=&space;1}(f(x_i)&space;-&space;y_i)^2\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}){i = 1}(f(x_i) - y_i)^2\" /></a>\n</center>\n\n在分类任务中，经常用到错误率和精度。\n\n分类错误率：\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;\\neq&space;y_i)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;\\neq&space;y_i)\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}{i = 1}\\Pi (f(x_i) \\neq y_i)\" /></a>\n</center\n>\n精度：\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=1&space;-&space;E(f;D)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?1&space;-&space;E(f;D)\" title=\"1 - E(f;D)\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;=&space;y_i)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;=&space;y_i)\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}{i = 1}\\Pi (f(x_i) = y_i)\" /></a>\n</center>\n\n关于混淆矩阵，也是分类任务，对于真实类别为正例时，若预测为正例则为True Positive否则为False Negative。对于真实类别为负例时，若预测为正例则为False Positive否则为True Negative。\n\n于是给出查准率（Precision）：\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=P&space;=&space;\\frac{TP}{TP&space;&plus;&space;FP}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?P&space;=&space;\\frac{TP}{TP&space;&plus;&space;FP}\" title=\"P = \\frac{TP}{TP + FP}\" /></a>\n</center>\n\n与召回率（Recall）：\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=R&space;=&space;\\frac{TP}{TP&space;&plus;&space;FN}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?R&space;=&space;\\frac{TP}{TP&space;&plus;&space;FN}\" title=\"R = \\frac{TP}{TP + FN}\" /></a>\n</center>\n\n由于二者经常变化矛盾，于是提出P-R曲线。\n\n除了PR曲线，也可以使用另外两种度量：\n\nF1度量：\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=F1&space;=&space;\\frac{2\\times&space;P&space;\\times&space;R}{P&space;&plus;&space;R}&space;=&space;\\frac{2&space;\\times&space;TP}{m&space;&plus;&space;TP&space;-&space;TN}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?F1&space;=&space;\\frac{2\\times&space;P&space;\\times&space;R}{P&space;&plus;&space;R}&space;=&space;\\frac{2&space;\\times&space;TP}{m&space;&plus;&space;TP&space;-&space;TN}\" title=\"F1 = \\frac{2\\times P \\times R}{P + R} = \\frac{2 \\times TP}{m + TP - TN}\" /></a>\n</center>\n\nF_{\\beta}度量:\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=F_{\\beta}&space;=&space;\\frac{(1&space;&plus;&space;{\\beta}^2&space;\\times&space;P&space;\\times&space;R)}{(\\beta&space;^2&space;\\times&space;P)&space;&plus;&space;R}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?F_{\\beta}&space;=&space;\\frac{(1&space;&plus;&space;{\\beta}^2&space;\\times&space;P&space;\\times&space;R)}{(\\beta&space;^2&space;\\times&space;P)&space;&plus;&space;R}\" title=\"F_{\\beta} = \\frac{(1 + {\\beta}^2 \\times P \\times R)}{(\\beta ^2 \\times P) + R}\" /></a>\n</center>\n\n另外，也可以使用宏平均，算出多个二分类混淆矩阵上计算出平均macro-P与macro-R，再去计算F1。也存在微平均，不过是先计算矩阵元素的平均值再去计算P与R。\n\n为了衡量犯下不同错误的不同的代价，提出了代价敏感错误率。cost01为将0预测为1，cost10为将1预测为0。","slug":"model-eval-md","published":1,"updated":"2021-09-19T16:40:57.325Z","_id":"cktmo1qvu00000zoedl411g4n","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"评估方法\"><a href=\"#评估方法\" class=\"headerlink\" title=\"评估方法\"></a>评估方法</h2><p>通常将包含m样本的数据集划分为训练集和测试集。具体的拆分方法：</p>\n<h3 id=\"留出法\"><a href=\"#留出法\" class=\"headerlink\" title=\"留出法\"></a>留出法</h3><p>直接将数据集划分为两个互斥的集合，分别作为测试集和训练集。</p>\n<p>在训练集中训练出模型后，用测试集来评估测试误差，作为对泛化误差的估计。</p>\n<p>注意：</p>\n<p>数据集的划分要尽可能保持数据分布的一致性（分层采样）。</p>\n<p>一般若干次随机划分、重复进行实验评估后取平均值。</p>\n<p>比例2:1～4:1。</p>\n<h3 id=\"交叉验证法\"><a href=\"#交叉验证法\" class=\"headerlink\" title=\"交叉验证法\"></a>交叉验证法</h3><p>将数据集分层采样划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的子集作为测试集，最终返回k个测试结果的均值，k最常用的取值是10，叫做10折交叉验证。</p>\n<p>其中将数据集划分为k个子集时为了减小因样本划分不同而引入的差别，k折交叉验证通常随机使用不同的划分重复p次，最终结果为p次k折交叉验证结果的均值。</p>\n<p>尤其地，当数据集包含m个样本，令k = m，则得到留一法。它不受随机样本划分的影响（只有一种）。结果较为准确。但是数据大的时候开销难以忍受。</p>\n<h3 id=\"自助法-Bootstrapping\"><a href=\"#自助法-Bootstrapping\" class=\"headerlink\" title=\"自助法(Bootstrapping)\"></a>自助法(Bootstrapping)</h3><p>给定数据集$D$，对它采样生成D’，即每次从随机从D挑选一个样本放入D’，然后再放回D中。重复m次，得到包含m个样本的数据集D’。</p>\n<p>再将D’ 最为训练集，D/D’ 作为测试集（0.37m个样本）。</p>\n<p>数据集小时很有用。但是改变了初始数据集的分布，会引入估计偏差。</p>\n<h2 id=\"调参与验证集\"><a href=\"#调参与验证集\" class=\"headerlink\" title=\"调参与验证集\"></a>调参与验证集</h2><p>参数分为超参和普通参数。</p>\n<p>超参一般为神经网络的层数，神经元个数等等。普通参数一般为权重系数等。</p>\n<p>验证集合时从训练集中再次划分为训练集和验证集，用来选择超参，选择出最优的超参再放到测试集去跑。一般测试集的结果会低一些。</p>\n<h2 id=\"性能度量\"><a href=\"#性能度量\" class=\"headerlink\" title=\"性能度量\"></a>性能度量</h2><p>在回归任务中，性能度量通常使用均方误差（MSE）。</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}){i&space;=&space;1}(f(x_i)&space;-&space;y_i)^2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}){i&space;=&space;1}(f(x_i)&space;-&space;y_i)^2\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}){i = 1}(f(x_i) - y_i)^2\" /></a>\n</center>\n\n<p>在分类任务中，经常用到错误率和精度。</p>\n<p>分类错误率：</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;\\neq&space;y_i)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;\\neq&space;y_i)\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}{i = 1}\\Pi (f(x_i) \\neq y_i)\" /></a>\n</center\n>\n精度：\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=1&space;-&space;E(f;D)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?1&space;-&space;E(f;D)\" title=\"1 - E(f;D)\" /></a>\n\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;=&space;y_i)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;=&space;y_i)\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}{i = 1}\\Pi (f(x_i) = y_i)\" /></a></p>\n</center>\n\n<p>关于混淆矩阵，也是分类任务，对于真实类别为正例时，若预测为正例则为True Positive否则为False Negative。对于真实类别为负例时，若预测为正例则为False Positive否则为True Negative。</p>\n<p>于是给出查准率（Precision）：</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=P&space;=&space;\\frac{TP}{TP&space;&plus;&space;FP}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?P&space;=&space;\\frac{TP}{TP&space;&plus;&space;FP}\" title=\"P = \\frac{TP}{TP + FP}\" /></a>\n</center>\n\n<p>与召回率（Recall）：</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=R&space;=&space;\\frac{TP}{TP&space;&plus;&space;FN}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?R&space;=&space;\\frac{TP}{TP&space;&plus;&space;FN}\" title=\"R = \\frac{TP}{TP + FN}\" /></a>\n</center>\n\n<p>由于二者经常变化矛盾，于是提出P-R曲线。</p>\n<p>除了PR曲线，也可以使用另外两种度量：</p>\n<p>F1度量：</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=F1&space;=&space;\\frac{2\\times&space;P&space;\\times&space;R}{P&space;&plus;&space;R}&space;=&space;\\frac{2&space;\\times&space;TP}{m&space;&plus;&space;TP&space;-&space;TN}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?F1&space;=&space;\\frac{2\\times&space;P&space;\\times&space;R}{P&space;&plus;&space;R}&space;=&space;\\frac{2&space;\\times&space;TP}{m&space;&plus;&space;TP&space;-&space;TN}\" title=\"F1 = \\frac{2\\times P \\times R}{P + R} = \\frac{2 \\times TP}{m + TP - TN}\" /></a>\n</center>\n\n<p>F_{\\beta}度量:</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=F_{\\beta}&space;=&space;\\frac{(1&space;&plus;&space;{\\beta}^2&space;\\times&space;P&space;\\times&space;R)}{(\\beta&space;^2&space;\\times&space;P)&space;&plus;&space;R}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?F_{\\beta}&space;=&space;\\frac{(1&space;&plus;&space;{\\beta}^2&space;\\times&space;P&space;\\times&space;R)}{(\\beta&space;^2&space;\\times&space;P)&space;&plus;&space;R}\" title=\"F_{\\beta} = \\frac{(1 + {\\beta}^2 \\times P \\times R)}{(\\beta ^2 \\times P) + R}\" /></a>\n</center>\n\n<p>另外，也可以使用宏平均，算出多个二分类混淆矩阵上计算出平均macro-P与macro-R，再去计算F1。也存在微平均，不过是先计算矩阵元素的平均值再去计算P与R。</p>\n<p>为了衡量犯下不同错误的不同的代价，提出了代价敏感错误率。cost01为将0预测为1，cost10为将1预测为0。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"评估方法\"><a href=\"#评估方法\" class=\"headerlink\" title=\"评估方法\"></a>评估方法</h2><p>通常将包含m样本的数据集划分为训练集和测试集。具体的拆分方法：</p>\n<h3 id=\"留出法\"><a href=\"#留出法\" class=\"headerlink\" title=\"留出法\"></a>留出法</h3><p>直接将数据集划分为两个互斥的集合，分别作为测试集和训练集。</p>\n<p>在训练集中训练出模型后，用测试集来评估测试误差，作为对泛化误差的估计。</p>\n<p>注意：</p>\n<p>数据集的划分要尽可能保持数据分布的一致性（分层采样）。</p>\n<p>一般若干次随机划分、重复进行实验评估后取平均值。</p>\n<p>比例2:1～4:1。</p>\n<h3 id=\"交叉验证法\"><a href=\"#交叉验证法\" class=\"headerlink\" title=\"交叉验证法\"></a>交叉验证法</h3><p>将数据集分层采样划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的子集作为测试集，最终返回k个测试结果的均值，k最常用的取值是10，叫做10折交叉验证。</p>\n<p>其中将数据集划分为k个子集时为了减小因样本划分不同而引入的差别，k折交叉验证通常随机使用不同的划分重复p次，最终结果为p次k折交叉验证结果的均值。</p>\n<p>尤其地，当数据集包含m个样本，令k = m，则得到留一法。它不受随机样本划分的影响（只有一种）。结果较为准确。但是数据大的时候开销难以忍受。</p>\n<h3 id=\"自助法-Bootstrapping\"><a href=\"#自助法-Bootstrapping\" class=\"headerlink\" title=\"自助法(Bootstrapping)\"></a>自助法(Bootstrapping)</h3><p>给定数据集$D$，对它采样生成D’，即每次从随机从D挑选一个样本放入D’，然后再放回D中。重复m次，得到包含m个样本的数据集D’。</p>\n<p>再将D’ 最为训练集，D/D’ 作为测试集（0.37m个样本）。</p>\n<p>数据集小时很有用。但是改变了初始数据集的分布，会引入估计偏差。</p>\n<h2 id=\"调参与验证集\"><a href=\"#调参与验证集\" class=\"headerlink\" title=\"调参与验证集\"></a>调参与验证集</h2><p>参数分为超参和普通参数。</p>\n<p>超参一般为神经网络的层数，神经元个数等等。普通参数一般为权重系数等。</p>\n<p>验证集合时从训练集中再次划分为训练集和验证集，用来选择超参，选择出最优的超参再放到测试集去跑。一般测试集的结果会低一些。</p>\n<h2 id=\"性能度量\"><a href=\"#性能度量\" class=\"headerlink\" title=\"性能度量\"></a>性能度量</h2><p>在回归任务中，性能度量通常使用均方误差（MSE）。</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}){i&space;=&space;1}(f(x_i)&space;-&space;y_i)^2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}){i&space;=&space;1}(f(x_i)&space;-&space;y_i)^2\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}){i = 1}(f(x_i) - y_i)^2\" /></a>\n</center>\n\n<p>在分类任务中，经常用到错误率和精度。</p>\n<p>分类错误率：</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;\\neq&space;y_i)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;\\neq&space;y_i)\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}{i = 1}\\Pi (f(x_i) \\neq y_i)\" /></a>\n</center\n>\n精度：\n\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=1&space;-&space;E(f;D)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?1&space;-&space;E(f;D)\" title=\"1 - E(f;D)\" /></a>\n\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;=&space;y_i)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?E(f;D)&space;=&space;\\frac{1}{m}\\sum^{m}{i&space;=&space;1}\\Pi&space;(f(x_i)&space;=&space;y_i)\" title=\"E(f;D) = \\frac{1}{m}\\sum^{m}{i = 1}\\Pi (f(x_i) = y_i)\" /></a></p>\n</center>\n\n<p>关于混淆矩阵，也是分类任务，对于真实类别为正例时，若预测为正例则为True Positive否则为False Negative。对于真实类别为负例时，若预测为正例则为False Positive否则为True Negative。</p>\n<p>于是给出查准率（Precision）：</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=P&space;=&space;\\frac{TP}{TP&space;&plus;&space;FP}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?P&space;=&space;\\frac{TP}{TP&space;&plus;&space;FP}\" title=\"P = \\frac{TP}{TP + FP}\" /></a>\n</center>\n\n<p>与召回率（Recall）：</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=R&space;=&space;\\frac{TP}{TP&space;&plus;&space;FN}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?R&space;=&space;\\frac{TP}{TP&space;&plus;&space;FN}\" title=\"R = \\frac{TP}{TP + FN}\" /></a>\n</center>\n\n<p>由于二者经常变化矛盾，于是提出P-R曲线。</p>\n<p>除了PR曲线，也可以使用另外两种度量：</p>\n<p>F1度量：</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=F1&space;=&space;\\frac{2\\times&space;P&space;\\times&space;R}{P&space;&plus;&space;R}&space;=&space;\\frac{2&space;\\times&space;TP}{m&space;&plus;&space;TP&space;-&space;TN}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?F1&space;=&space;\\frac{2\\times&space;P&space;\\times&space;R}{P&space;&plus;&space;R}&space;=&space;\\frac{2&space;\\times&space;TP}{m&space;&plus;&space;TP&space;-&space;TN}\" title=\"F1 = \\frac{2\\times P \\times R}{P + R} = \\frac{2 \\times TP}{m + TP - TN}\" /></a>\n</center>\n\n<p>F_{\\beta}度量:</p>\n<center>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=F_{\\beta}&space;=&space;\\frac{(1&space;&plus;&space;{\\beta}^2&space;\\times&space;P&space;\\times&space;R)}{(\\beta&space;^2&space;\\times&space;P)&space;&plus;&space;R}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?F_{\\beta}&space;=&space;\\frac{(1&space;&plus;&space;{\\beta}^2&space;\\times&space;P&space;\\times&space;R)}{(\\beta&space;^2&space;\\times&space;P)&space;&plus;&space;R}\" title=\"F_{\\beta} = \\frac{(1 + {\\beta}^2 \\times P \\times R)}{(\\beta ^2 \\times P) + R}\" /></a>\n</center>\n\n<p>另外，也可以使用宏平均，算出多个二分类混淆矩阵上计算出平均macro-P与macro-R，再去计算F1。也存在微平均，不过是先计算矩阵元素的平均值再去计算P与R。</p>\n<p>为了衡量犯下不同错误的不同的代价，提出了代价敏感错误率。cost01为将0预测为1，cost10为将1预测为0。</p>\n"},{"title":"线性回归","date":"2021-09-23T00:02:40.000Z","_content":"\n线性模型是一种有监督的学习，每个样本都对应有标签。根据我们预测的结果是否为连续值，分为了线性回归和对数几率回归（分类）。前提是输入和输出之前有线性相关关系。\n\n## 回归任务\n\n基本形式：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" title=\"f(x) = W ^T x + b\" /></a>\n\n其中x是样本属性的线性组合，是一个向量。W是对每一个属性的权值。模型的可解释性强（白箱模型）。\n\n对于模型好坏的评估，这里选择的loss function为平方误差。\n\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,&space;b)&space;=&space;\\sum_{i&space;=&space;1}^{100}(y_i&space;-&space;(W&space;x_i&space;&plus;&space;b))^2f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,&space;b)&space;=&space;\\sum_{i&space;=&space;1}^{100}(y_i&space;-&space;(W&space;x_i&space;&plus;&space;b))^2f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" title=\"L(W, b) = \\sum_{i = 1}^{100}(y_i - (W x_i + b))^2f(x) = W ^T x + b\" /></a>\n\n\n下面需要找到一组W，b使得上述的损失函数可以达到最小。方法有两种。一种是最小二乘法，它只对线性回归适用。函数分别对W和b求偏导，让偏导数为0，得到闭式解（closed-form）。\n\n另一种方式是梯度下降（Gradient Descent）。前提是有损失函数，且函数对参数可微。对于W这一个参数的更新来说，它的变化与它当前值所在的位置以及函数梯度相关。\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=W^{'}&space;=&space;W^{0}&space;-&space;lr&space;\\frac{dL}{dW}&space;|_{W&space;=&space;W^0}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?W^{'}&space;=&space;W^{0}&space;-&space;lr&space;\\frac{dL}{dW}&space;|_{W&space;=&space;W^0}\" title=\"W^{'} = W^{0} - lr \\frac{dL}{dW} |_{W = W^0}\" /></a>\n\n其中lr为学习率。面临的问题是很容易收敛到一个极小值点，因此训练的效果也与初值位置的选择相关。\n\n同理、对于W、b两个参数来讲，就是两个参数同时变化。这是可以将它们的梯度用一个向量来描述、叫做这个函数的梯度。\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\bigtriangledown&space;L\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\bigtriangledown&space;L\" title=\"\\bigtriangledown L\" /></a>\n\n值得注意的是，微分为0的点不一定为极小值点，也可能会有鞍点（saddle point）的存在。另外由于一般会设置一个阈值，loss较低时便停止训练，这样也会导致会在平缓下降的位置处停止训练。但是线性回归由于线性的特性所以不会出现这样的问题。\n\n## 二分类任务\n\n现在思考如何将上述线性回归的结果与类别标签联系起来。可以使用单位阶跃函数或者Sigmoid来进行一个映射。\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;Sigmoid(W&space;^T&space;x&space;&plus;&space;b)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?f(x)&space;=&space;Sigmoid(W&space;^T&space;x&space;&plus;&space;b)\" title=\"f(x) = Sigmoid(W ^T x + b)\" /></a>\n\n这样将输出映射到了一个0-1的区间。\n\n对于模型的好坏，首先对于一组W、b产生训练数据的概率为一个似然估计：（x3不属于C1类别）\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,&space;b)&space;=&space;f_{W,b}(x1)&space;f_{W,b}(x2)(1-&space;f_{W,b}(x3))...\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,&space;b)&space;=&space;f_{W,b}(x1)&space;f_{W,b}(x2)(1-&space;f_{W,b}(x3))...\" title=\"L(W, b) = f_{W,b}(x1) f_{W,b}(x2)(1- f_{W,b}(x3))...\" /></a>\n\n那么L(W, b)取值最大是的参数是我们想要的。为了方便优化，我们给上述函数取一负对数。同时对于展开后的每一项进行一个补齐，标签值正确的系数为1，否则系数为0。\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=-lnf_{W,b}(x1)&space;=&space;-(1*ln(fx1)&space;&plus;&space;0*ln(1-f(x1)))\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?-lnf_{W,b}(x1)&space;=&space;-(1*ln(fx1)&space;&plus;&space;0*ln(1-f(x1)))\" title=\"-lnf_{W,b}(x1) = -(1*ln(fx1) + 0*ln(1-f(x1)))\" /></a>\n\n这样就能得到同一化的公式：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,b)&space;=&space;\\sum_{n}-&space;\\left&space;[&space;y_n&space;lnf_{W,b}(x_n)&space;&plus;&space;(1-y_n)ln(1-f_{W,b}(x_n))&space;\\right&space;]\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,b)&space;=&space;\\sum_{n}-&space;\\left&space;[&space;y_n&space;lnf_{W,b}(x_n)&space;&plus;&space;(1-y_n)ln(1-f_{W,b}(x_n))&space;\\right&space;]\" title=\"L(W,b) = \\sum_{n}- \\left [ y_n lnf_{W,b}(x_n) + (1-y_n)ln(1-f_{W,b}(x_n)) \\right ]\" /></a>\n\n这里面其实蕴含了p、q两个分布：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=p(x&space;=&space;1)&space;=&space;y_n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(x&space;=&space;1)&space;=&space;y_n\" title=\"p(x = 1) = y_n\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=p(x&space;=&space;0)&space;-&space;1&space;-&space;y_n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(x&space;=&space;0)&space;-&space;1&space;-&space;y_n\" title=\"p(x = 0) - 1 - y_n\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=q(x&space;=&space;1)&space;=&space;f(x_n)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?q(x&space;=&space;1)&space;=&space;f(x_n)\" title=\"q(x = 1) = f(x_n)\" /></a>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=q(x&space;=&space;0)&space;=&space;1&space;-&space;f(x_n)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?q(x&space;=&space;0)&space;=&space;1&space;-&space;f(x_n)\" title=\"q(x = 0) = 1 - f(x_n)\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=H(p,q)&space;=&space;-\\sum_{x}p(x)ln(q(X))\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?H(p,q)&space;=&space;-\\sum_{x}p(x)ln(q(X))\" title=\"H(p,q) = -\\sum_{x}p(x)ln(q(X))\" /></a>\n\n\n两个分布越接近，交叉熵越小，也就是效果越好。\n","source":"_posts/linear-reg-md.md","raw":"---\ntitle: 线性回归\ndate: 2021-09-23 08:02:40\ntags: 机器学习\n---\n\n线性模型是一种有监督的学习，每个样本都对应有标签。根据我们预测的结果是否为连续值，分为了线性回归和对数几率回归（分类）。前提是输入和输出之前有线性相关关系。\n\n## 回归任务\n\n基本形式：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" title=\"f(x) = W ^T x + b\" /></a>\n\n其中x是样本属性的线性组合，是一个向量。W是对每一个属性的权值。模型的可解释性强（白箱模型）。\n\n对于模型好坏的评估，这里选择的loss function为平方误差。\n\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,&space;b)&space;=&space;\\sum_{i&space;=&space;1}^{100}(y_i&space;-&space;(W&space;x_i&space;&plus;&space;b))^2f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,&space;b)&space;=&space;\\sum_{i&space;=&space;1}^{100}(y_i&space;-&space;(W&space;x_i&space;&plus;&space;b))^2f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" title=\"L(W, b) = \\sum_{i = 1}^{100}(y_i - (W x_i + b))^2f(x) = W ^T x + b\" /></a>\n\n\n下面需要找到一组W，b使得上述的损失函数可以达到最小。方法有两种。一种是最小二乘法，它只对线性回归适用。函数分别对W和b求偏导，让偏导数为0，得到闭式解（closed-form）。\n\n另一种方式是梯度下降（Gradient Descent）。前提是有损失函数，且函数对参数可微。对于W这一个参数的更新来说，它的变化与它当前值所在的位置以及函数梯度相关。\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=W^{'}&space;=&space;W^{0}&space;-&space;lr&space;\\frac{dL}{dW}&space;|_{W&space;=&space;W^0}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?W^{'}&space;=&space;W^{0}&space;-&space;lr&space;\\frac{dL}{dW}&space;|_{W&space;=&space;W^0}\" title=\"W^{'} = W^{0} - lr \\frac{dL}{dW} |_{W = W^0}\" /></a>\n\n其中lr为学习率。面临的问题是很容易收敛到一个极小值点，因此训练的效果也与初值位置的选择相关。\n\n同理、对于W、b两个参数来讲，就是两个参数同时变化。这是可以将它们的梯度用一个向量来描述、叫做这个函数的梯度。\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\bigtriangledown&space;L\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\bigtriangledown&space;L\" title=\"\\bigtriangledown L\" /></a>\n\n值得注意的是，微分为0的点不一定为极小值点，也可能会有鞍点（saddle point）的存在。另外由于一般会设置一个阈值，loss较低时便停止训练，这样也会导致会在平缓下降的位置处停止训练。但是线性回归由于线性的特性所以不会出现这样的问题。\n\n## 二分类任务\n\n现在思考如何将上述线性回归的结果与类别标签联系起来。可以使用单位阶跃函数或者Sigmoid来进行一个映射。\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;Sigmoid(W&space;^T&space;x&space;&plus;&space;b)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?f(x)&space;=&space;Sigmoid(W&space;^T&space;x&space;&plus;&space;b)\" title=\"f(x) = Sigmoid(W ^T x + b)\" /></a>\n\n这样将输出映射到了一个0-1的区间。\n\n对于模型的好坏，首先对于一组W、b产生训练数据的概率为一个似然估计：（x3不属于C1类别）\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,&space;b)&space;=&space;f_{W,b}(x1)&space;f_{W,b}(x2)(1-&space;f_{W,b}(x3))...\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,&space;b)&space;=&space;f_{W,b}(x1)&space;f_{W,b}(x2)(1-&space;f_{W,b}(x3))...\" title=\"L(W, b) = f_{W,b}(x1) f_{W,b}(x2)(1- f_{W,b}(x3))...\" /></a>\n\n那么L(W, b)取值最大是的参数是我们想要的。为了方便优化，我们给上述函数取一负对数。同时对于展开后的每一项进行一个补齐，标签值正确的系数为1，否则系数为0。\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=-lnf_{W,b}(x1)&space;=&space;-(1*ln(fx1)&space;&plus;&space;0*ln(1-f(x1)))\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?-lnf_{W,b}(x1)&space;=&space;-(1*ln(fx1)&space;&plus;&space;0*ln(1-f(x1)))\" title=\"-lnf_{W,b}(x1) = -(1*ln(fx1) + 0*ln(1-f(x1)))\" /></a>\n\n这样就能得到同一化的公式：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,b)&space;=&space;\\sum_{n}-&space;\\left&space;[&space;y_n&space;lnf_{W,b}(x_n)&space;&plus;&space;(1-y_n)ln(1-f_{W,b}(x_n))&space;\\right&space;]\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,b)&space;=&space;\\sum_{n}-&space;\\left&space;[&space;y_n&space;lnf_{W,b}(x_n)&space;&plus;&space;(1-y_n)ln(1-f_{W,b}(x_n))&space;\\right&space;]\" title=\"L(W,b) = \\sum_{n}- \\left [ y_n lnf_{W,b}(x_n) + (1-y_n)ln(1-f_{W,b}(x_n)) \\right ]\" /></a>\n\n这里面其实蕴含了p、q两个分布：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=p(x&space;=&space;1)&space;=&space;y_n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(x&space;=&space;1)&space;=&space;y_n\" title=\"p(x = 1) = y_n\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=p(x&space;=&space;0)&space;-&space;1&space;-&space;y_n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(x&space;=&space;0)&space;-&space;1&space;-&space;y_n\" title=\"p(x = 0) - 1 - y_n\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=q(x&space;=&space;1)&space;=&space;f(x_n)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?q(x&space;=&space;1)&space;=&space;f(x_n)\" title=\"q(x = 1) = f(x_n)\" /></a>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=q(x&space;=&space;0)&space;=&space;1&space;-&space;f(x_n)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?q(x&space;=&space;0)&space;=&space;1&space;-&space;f(x_n)\" title=\"q(x = 0) = 1 - f(x_n)\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=H(p,q)&space;=&space;-\\sum_{x}p(x)ln(q(X))\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?H(p,q)&space;=&space;-\\sum_{x}p(x)ln(q(X))\" title=\"H(p,q) = -\\sum_{x}p(x)ln(q(X))\" /></a>\n\n\n两个分布越接近，交叉熵越小，也就是效果越好。\n","slug":"linear-reg-md","published":1,"updated":"2021-11-05T05:35:09.093Z","_id":"cktxte5ae0000d3oee8undkvv","comments":1,"layout":"post","photos":[],"link":"","content":"<p>线性模型是一种有监督的学习，每个样本都对应有标签。根据我们预测的结果是否为连续值，分为了线性回归和对数几率回归（分类）。前提是输入和输出之前有线性相关关系。</p>\n<h2 id=\"回归任务\"><a href=\"#回归任务\" class=\"headerlink\" title=\"回归任务\"></a>回归任务</h2><p>基本形式：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" title=\"f(x) = W ^T x + b\" /></a></p>\n<p>其中x是样本属性的线性组合，是一个向量。W是对每一个属性的权值。模型的可解释性强（白箱模型）。</p>\n<p>对于模型好坏的评估，这里选择的loss function为平方误差。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,&space;b)&space;=&space;\\sum_{i&space;=&space;1}^{100}(y_i&space;-&space;(W&space;x_i&space;&plus;&space;b))^2f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,&space;b)&space;=&space;\\sum_{i&space;=&space;1}^{100}(y_i&space;-&space;(W&space;x_i&space;&plus;&space;b))^2f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" title=\"L(W, b) = \\sum_{i = 1}^{100}(y_i - (W x_i + b))^2f(x) = W ^T x + b\" /></a></p>\n<p>下面需要找到一组W，b使得上述的损失函数可以达到最小。方法有两种。一种是最小二乘法，它只对线性回归适用。函数分别对W和b求偏导，让偏导数为0，得到闭式解（closed-form）。</p>\n<p>另一种方式是梯度下降（Gradient Descent）。前提是有损失函数，且函数对参数可微。对于W这一个参数的更新来说，它的变化与它当前值所在的位置以及函数梯度相关。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=W^{'}&space;=&space;W^{0}&space;-&space;lr&space;\\frac{dL}{dW}&space;|_{W&space;=&space;W^0}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?W^{'}&space;=&space;W^{0}&space;-&space;lr&space;\\frac{dL}{dW}&space;|_{W&space;=&space;W^0}\" title=\"W^{'} = W^{0} - lr \\frac{dL}{dW} |_{W = W^0}\" /></a></p>\n<p>其中lr为学习率。面临的问题是很容易收敛到一个极小值点，因此训练的效果也与初值位置的选择相关。</p>\n<p>同理、对于W、b两个参数来讲，就是两个参数同时变化。这是可以将它们的梯度用一个向量来描述、叫做这个函数的梯度。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\bigtriangledown&space;L\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\bigtriangledown&space;L\" title=\"\\bigtriangledown L\" /></a></p>\n<p>值得注意的是，微分为0的点不一定为极小值点，也可能会有鞍点（saddle point）的存在。另外由于一般会设置一个阈值，loss较低时便停止训练，这样也会导致会在平缓下降的位置处停止训练。但是线性回归由于线性的特性所以不会出现这样的问题。</p>\n<h2 id=\"二分类任务\"><a href=\"#二分类任务\" class=\"headerlink\" title=\"二分类任务\"></a>二分类任务</h2><p>现在思考如何将上述线性回归的结果与类别标签联系起来。可以使用单位阶跃函数或者Sigmoid来进行一个映射。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;Sigmoid(W&space;^T&space;x&space;&plus;&space;b)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?f(x)&space;=&space;Sigmoid(W&space;^T&space;x&space;&plus;&space;b)\" title=\"f(x) = Sigmoid(W ^T x + b)\" /></a></p>\n<p>这样将输出映射到了一个0-1的区间。</p>\n<p>对于模型的好坏，首先对于一组W、b产生训练数据的概率为一个似然估计：（x3不属于C1类别）</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,&space;b)&space;=&space;f_{W,b}(x1)&space;f_{W,b}(x2)(1-&space;f_{W,b}(x3))...\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,&space;b)&space;=&space;f_{W,b}(x1)&space;f_{W,b}(x2)(1-&space;f_{W,b}(x3))...\" title=\"L(W, b) = f_{W,b}(x1) f_{W,b}(x2)(1- f_{W,b}(x3))...\" /></a></p>\n<p>那么L(W, b)取值最大是的参数是我们想要的。为了方便优化，我们给上述函数取一负对数。同时对于展开后的每一项进行一个补齐，标签值正确的系数为1，否则系数为0。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=-lnf_{W,b}(x1)&space;=&space;-(1*ln(fx1)&space;&plus;&space;0*ln(1-f(x1)))\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?-lnf_{W,b}(x1)&space;=&space;-(1*ln(fx1)&space;&plus;&space;0*ln(1-f(x1)))\" title=\"-lnf_{W,b}(x1) = -(1*ln(fx1) + 0*ln(1-f(x1)))\" /></a></p>\n<p>这样就能得到同一化的公式：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,b)&space;=&space;\\sum_{n}-&space;\\left&space;[&space;y_n&space;lnf_{W,b}(x_n)&space;&plus;&space;(1-y_n)ln(1-f_{W,b}(x_n))&space;\\right&space;]\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,b)&space;=&space;\\sum_{n}-&space;\\left&space;[&space;y_n&space;lnf_{W,b}(x_n)&space;&plus;&space;(1-y_n)ln(1-f_{W,b}(x_n))&space;\\right&space;]\" title=\"L(W,b) = \\sum_{n}- \\left [ y_n lnf_{W,b}(x_n) + (1-y_n)ln(1-f_{W,b}(x_n)) \\right ]\" /></a></p>\n<p>这里面其实蕴含了p、q两个分布：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=p(x&space;=&space;1)&space;=&space;y_n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(x&space;=&space;1)&space;=&space;y_n\" title=\"p(x = 1) = y_n\" /></a></p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=p(x&space;=&space;0)&space;-&space;1&space;-&space;y_n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(x&space;=&space;0)&space;-&space;1&space;-&space;y_n\" title=\"p(x = 0) - 1 - y_n\" /></a></p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=q(x&space;=&space;1)&space;=&space;f(x_n)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?q(x&space;=&space;1)&space;=&space;f(x_n)\" title=\"q(x = 1) = f(x_n)\" /></a><br><a href=\"https://www.codecogs.com/eqnedit.php?latex=q(x&space;=&space;0)&space;=&space;1&space;-&space;f(x_n)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?q(x&space;=&space;0)&space;=&space;1&space;-&space;f(x_n)\" title=\"q(x = 0) = 1 - f(x_n)\" /></a></p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=H(p,q)&space;=&space;-\\sum_{x}p(x)ln(q(X))\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?H(p,q)&space;=&space;-\\sum_{x}p(x)ln(q(X))\" title=\"H(p,q) = -\\sum_{x}p(x)ln(q(X))\" /></a></p>\n<p>两个分布越接近，交叉熵越小，也就是效果越好。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>线性模型是一种有监督的学习，每个样本都对应有标签。根据我们预测的结果是否为连续值，分为了线性回归和对数几率回归（分类）。前提是输入和输出之前有线性相关关系。</p>\n<h2 id=\"回归任务\"><a href=\"#回归任务\" class=\"headerlink\" title=\"回归任务\"></a>回归任务</h2><p>基本形式：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" title=\"f(x) = W ^T x + b\" /></a></p>\n<p>其中x是样本属性的线性组合，是一个向量。W是对每一个属性的权值。模型的可解释性强（白箱模型）。</p>\n<p>对于模型好坏的评估，这里选择的loss function为平方误差。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,&space;b)&space;=&space;\\sum_{i&space;=&space;1}^{100}(y_i&space;-&space;(W&space;x_i&space;&plus;&space;b))^2f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,&space;b)&space;=&space;\\sum_{i&space;=&space;1}^{100}(y_i&space;-&space;(W&space;x_i&space;&plus;&space;b))^2f(x)&space;=&space;W&space;^T&space;x&space;&plus;&space;b\" title=\"L(W, b) = \\sum_{i = 1}^{100}(y_i - (W x_i + b))^2f(x) = W ^T x + b\" /></a></p>\n<p>下面需要找到一组W，b使得上述的损失函数可以达到最小。方法有两种。一种是最小二乘法，它只对线性回归适用。函数分别对W和b求偏导，让偏导数为0，得到闭式解（closed-form）。</p>\n<p>另一种方式是梯度下降（Gradient Descent）。前提是有损失函数，且函数对参数可微。对于W这一个参数的更新来说，它的变化与它当前值所在的位置以及函数梯度相关。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=W^{'}&space;=&space;W^{0}&space;-&space;lr&space;\\frac{dL}{dW}&space;|_{W&space;=&space;W^0}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?W^{'}&space;=&space;W^{0}&space;-&space;lr&space;\\frac{dL}{dW}&space;|_{W&space;=&space;W^0}\" title=\"W^{'} = W^{0} - lr \\frac{dL}{dW} |_{W = W^0}\" /></a></p>\n<p>其中lr为学习率。面临的问题是很容易收敛到一个极小值点，因此训练的效果也与初值位置的选择相关。</p>\n<p>同理、对于W、b两个参数来讲，就是两个参数同时变化。这是可以将它们的梯度用一个向量来描述、叫做这个函数的梯度。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\bigtriangledown&space;L\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\bigtriangledown&space;L\" title=\"\\bigtriangledown L\" /></a></p>\n<p>值得注意的是，微分为0的点不一定为极小值点，也可能会有鞍点（saddle point）的存在。另外由于一般会设置一个阈值，loss较低时便停止训练，这样也会导致会在平缓下降的位置处停止训练。但是线性回归由于线性的特性所以不会出现这样的问题。</p>\n<h2 id=\"二分类任务\"><a href=\"#二分类任务\" class=\"headerlink\" title=\"二分类任务\"></a>二分类任务</h2><p>现在思考如何将上述线性回归的结果与类别标签联系起来。可以使用单位阶跃函数或者Sigmoid来进行一个映射。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;Sigmoid(W&space;^T&space;x&space;&plus;&space;b)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?f(x)&space;=&space;Sigmoid(W&space;^T&space;x&space;&plus;&space;b)\" title=\"f(x) = Sigmoid(W ^T x + b)\" /></a></p>\n<p>这样将输出映射到了一个0-1的区间。</p>\n<p>对于模型的好坏，首先对于一组W、b产生训练数据的概率为一个似然估计：（x3不属于C1类别）</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,&space;b)&space;=&space;f_{W,b}(x1)&space;f_{W,b}(x2)(1-&space;f_{W,b}(x3))...\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,&space;b)&space;=&space;f_{W,b}(x1)&space;f_{W,b}(x2)(1-&space;f_{W,b}(x3))...\" title=\"L(W, b) = f_{W,b}(x1) f_{W,b}(x2)(1- f_{W,b}(x3))...\" /></a></p>\n<p>那么L(W, b)取值最大是的参数是我们想要的。为了方便优化，我们给上述函数取一负对数。同时对于展开后的每一项进行一个补齐，标签值正确的系数为1，否则系数为0。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=-lnf_{W,b}(x1)&space;=&space;-(1*ln(fx1)&space;&plus;&space;0*ln(1-f(x1)))\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?-lnf_{W,b}(x1)&space;=&space;-(1*ln(fx1)&space;&plus;&space;0*ln(1-f(x1)))\" title=\"-lnf_{W,b}(x1) = -(1*ln(fx1) + 0*ln(1-f(x1)))\" /></a></p>\n<p>这样就能得到同一化的公式：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=L(W,b)&space;=&space;\\sum_{n}-&space;\\left&space;[&space;y_n&space;lnf_{W,b}(x_n)&space;&plus;&space;(1-y_n)ln(1-f_{W,b}(x_n))&space;\\right&space;]\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L(W,b)&space;=&space;\\sum_{n}-&space;\\left&space;[&space;y_n&space;lnf_{W,b}(x_n)&space;&plus;&space;(1-y_n)ln(1-f_{W,b}(x_n))&space;\\right&space;]\" title=\"L(W,b) = \\sum_{n}- \\left [ y_n lnf_{W,b}(x_n) + (1-y_n)ln(1-f_{W,b}(x_n)) \\right ]\" /></a></p>\n<p>这里面其实蕴含了p、q两个分布：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=p(x&space;=&space;1)&space;=&space;y_n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(x&space;=&space;1)&space;=&space;y_n\" title=\"p(x = 1) = y_n\" /></a></p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=p(x&space;=&space;0)&space;-&space;1&space;-&space;y_n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(x&space;=&space;0)&space;-&space;1&space;-&space;y_n\" title=\"p(x = 0) - 1 - y_n\" /></a></p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=q(x&space;=&space;1)&space;=&space;f(x_n)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?q(x&space;=&space;1)&space;=&space;f(x_n)\" title=\"q(x = 1) = f(x_n)\" /></a><br><a href=\"https://www.codecogs.com/eqnedit.php?latex=q(x&space;=&space;0)&space;=&space;1&space;-&space;f(x_n)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?q(x&space;=&space;0)&space;=&space;1&space;-&space;f(x_n)\" title=\"q(x = 0) = 1 - f(x_n)\" /></a></p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=H(p,q)&space;=&space;-\\sum_{x}p(x)ln(q(X))\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?H(p,q)&space;=&space;-\\sum_{x}p(x)ln(q(X))\" title=\"H(p,q) = -\\sum_{x}p(x)ln(q(X))\" /></a></p>\n<p>两个分布越接近，交叉熵越小，也就是效果越好。</p>\n"},{"title":"人工智能课程笔记","date":"2021-11-05T06:11:24.000Z","_content":"\n考完AI了，把笔记保存一下吧。描述了一些关键知识的理解（因为抓紧复习所以有些地方写的并不详尽）。\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page1.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page2.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page3.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page4.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page5.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page6.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page7.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page8.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page9.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page10.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page11.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page12.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page13.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page14.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page15.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page16.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page17.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page18.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page19.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page20.png\"/>","source":"_posts/AI-Review-md.md","raw":"---\ntitle: 人工智能课程笔记\ndate: 2021-11-05 14:11:24\ntags: AI Course\n---\n\n考完AI了，把笔记保存一下吧。描述了一些关键知识的理解（因为抓紧复习所以有些地方写的并不详尽）。\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page1.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page2.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page3.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page4.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page5.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page6.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page7.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page8.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page9.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page10.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page11.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page12.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page13.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page14.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page15.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page16.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page17.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page18.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page19.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page20.png\"/>","slug":"AI-Review-md","published":1,"updated":"2021-11-05T09:52:39.339Z","_id":"ckvm4wqog00000hoecx59gf45","comments":1,"layout":"post","photos":[],"link":"","content":"<p>考完AI了，把笔记保存一下吧。描述了一些关键知识的理解（因为抓紧复习所以有些地方写的并不详尽）。</p>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page1.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page2.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page3.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page4.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page5.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page6.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page7.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page8.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page9.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page10.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page11.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page12.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page13.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page14.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page15.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page16.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page17.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page18.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page19.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page20.png\"/>","site":{"data":{}},"excerpt":"","more":"<p>考完AI了，把笔记保存一下吧。描述了一些关键知识的理解（因为抓紧复习所以有些地方写的并不详尽）。</p>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page1.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page2.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page3.png\"/>\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page4.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page5.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page6.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page7.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page8.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page9.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page10.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page11.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page12.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page13.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page14.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page15.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page16.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page17.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page18.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page19.png\"/>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/Page20.png\"/>"},{"title":"积木城堡","date":"2021-11-06T14:10:40.000Z","_content":"\nXC的儿子小XC最喜欢玩的游戏用积木垒漂亮的城堡。城堡是用一些立方体的积木垒成的，城堡的每一层是一块积木。小XC是一个比他爸爸XC还聪明的孩子，他发现垒城堡的时候，如果下面的积木比上面的积木大，那么城堡便不容易倒。所以他在垒城堡的时候总是遵循这样的规则。\n小XC想把自己垒的城堡送给幼儿园里漂亮的女孩子们，这样可以增加他的好感度。为了公平起见，他决定把送给每个女孩子一样高的城堡，这样可以避免女孩子们为了获得更漂亮的城堡而引起争执。可是他发现自己在垒城堡的时候并没有预先考虑到这一点。所以他现在要改造城堡。由于他没有多余的积木了，他灵机一动，想出了一个巧妙的改造方案。他决定从每一个城堡中挪去一些积木，使得最终每座城堡都一样高。为了使他的城堡更雄伟，他觉得应该使最后的城堡都尽可能的高。\n任务：\n请你帮助小XC编一个程序，根据他垒的所有城堡的信息，决定应该移去哪些积木才能获得最佳的效果。\n\n## Input\n第一行是一个整数 N (N≤100)， 表示一共有几座城堡。以下 N 行每行是一系列非负整数，用一个空格分隔，按从下往上的顺序依次给出一座城堡中所有积木的棱长。用-1结束。一座城堡中的积木不超过100块，每块积木的棱长不超过100。\n\n## Output\n一个整数，表示最后城堡的最大可能的高度。如果找不到合适的方案，则输出 0 。\n\n## EX\n2\n2 1 –1\n3 2 1 –1\n\n3\n\n## 思路\n思路就是每次输入一个城堡的高度信息就多一次01背包类型的dp，得到它所能得到的所有高度，保存在dp[i][j]中，这个其实是用一维数组进行dp比较方便，i表示第几个城堡，与dp关系不大，j表示堡垒的最大高度（类比背包size）。\n然后对i组堡垒进行取高度的交集，交集中最大的值就是结果。\n\n## Code\n```cpp\n#include<cstdio>\n\n#include<iostream>\n#include<cmath>\n#include<string>\n\nusing namespace std;\n\nint dp[105][10005],len[105][105],cnt[105];\n\n//len记录每个城堡每块积木的长度，cnt记录每个城堡的积木数\nint n,maxh;\n\nbool judge(int x)//判断高度x是否所有城堡都可到达\n{\n    for(int i=1;i<=n;i++)\n        if(dp[i][x]!=x)\n            return false;\n    return true;\n}\n\n\n\nint main()\n{\n    for(int i = 0;i < 105;i++){\n        for(int j = 0;j < 10005;j++){\n            dp[i][j] = 0;\n        }\n    }\n\n    cin>>n;\n    maxh=0;\n\n    for(int i=1;i<=n;i++) {\n        int sum = 0, tlen;\n        cnt[i] = 0;\n        cin >> tlen;\n        while (tlen > 0) {\n            cnt[i]++;\n            len[i][cnt[i]] = tlen;\n            sum += tlen;\n            cin >> tlen;\n        }\n        if (sum > maxh)\n        {\n            maxh = sum;\n        }\n        for (int j = 1; j <= cnt[i]; j++){\n            for (int k = maxh; k >= len[i][j]; k--) {\n                dp[i][k] = max(dp[i][k], dp[i][k - len[i][j]] + len[i][j]);\n            }\n        }\n    }\n    for(int i=maxh;i>=0;i--)\n    {\n        if(judge(i))\n        {\n            cout<<i<<endl;\n            break;\n        }\n    }\n}\n```","source":"_posts/acm-jimuchengbao-md.md","raw":"---\ntitle: 积木城堡\ndate: 2021-11-06 22:10:40\ntags: ACM\n---\n\nXC的儿子小XC最喜欢玩的游戏用积木垒漂亮的城堡。城堡是用一些立方体的积木垒成的，城堡的每一层是一块积木。小XC是一个比他爸爸XC还聪明的孩子，他发现垒城堡的时候，如果下面的积木比上面的积木大，那么城堡便不容易倒。所以他在垒城堡的时候总是遵循这样的规则。\n小XC想把自己垒的城堡送给幼儿园里漂亮的女孩子们，这样可以增加他的好感度。为了公平起见，他决定把送给每个女孩子一样高的城堡，这样可以避免女孩子们为了获得更漂亮的城堡而引起争执。可是他发现自己在垒城堡的时候并没有预先考虑到这一点。所以他现在要改造城堡。由于他没有多余的积木了，他灵机一动，想出了一个巧妙的改造方案。他决定从每一个城堡中挪去一些积木，使得最终每座城堡都一样高。为了使他的城堡更雄伟，他觉得应该使最后的城堡都尽可能的高。\n任务：\n请你帮助小XC编一个程序，根据他垒的所有城堡的信息，决定应该移去哪些积木才能获得最佳的效果。\n\n## Input\n第一行是一个整数 N (N≤100)， 表示一共有几座城堡。以下 N 行每行是一系列非负整数，用一个空格分隔，按从下往上的顺序依次给出一座城堡中所有积木的棱长。用-1结束。一座城堡中的积木不超过100块，每块积木的棱长不超过100。\n\n## Output\n一个整数，表示最后城堡的最大可能的高度。如果找不到合适的方案，则输出 0 。\n\n## EX\n2\n2 1 –1\n3 2 1 –1\n\n3\n\n## 思路\n思路就是每次输入一个城堡的高度信息就多一次01背包类型的dp，得到它所能得到的所有高度，保存在dp[i][j]中，这个其实是用一维数组进行dp比较方便，i表示第几个城堡，与dp关系不大，j表示堡垒的最大高度（类比背包size）。\n然后对i组堡垒进行取高度的交集，交集中最大的值就是结果。\n\n## Code\n```cpp\n#include<cstdio>\n\n#include<iostream>\n#include<cmath>\n#include<string>\n\nusing namespace std;\n\nint dp[105][10005],len[105][105],cnt[105];\n\n//len记录每个城堡每块积木的长度，cnt记录每个城堡的积木数\nint n,maxh;\n\nbool judge(int x)//判断高度x是否所有城堡都可到达\n{\n    for(int i=1;i<=n;i++)\n        if(dp[i][x]!=x)\n            return false;\n    return true;\n}\n\n\n\nint main()\n{\n    for(int i = 0;i < 105;i++){\n        for(int j = 0;j < 10005;j++){\n            dp[i][j] = 0;\n        }\n    }\n\n    cin>>n;\n    maxh=0;\n\n    for(int i=1;i<=n;i++) {\n        int sum = 0, tlen;\n        cnt[i] = 0;\n        cin >> tlen;\n        while (tlen > 0) {\n            cnt[i]++;\n            len[i][cnt[i]] = tlen;\n            sum += tlen;\n            cin >> tlen;\n        }\n        if (sum > maxh)\n        {\n            maxh = sum;\n        }\n        for (int j = 1; j <= cnt[i]; j++){\n            for (int k = maxh; k >= len[i][j]; k--) {\n                dp[i][k] = max(dp[i][k], dp[i][k - len[i][j]] + len[i][j]);\n            }\n        }\n    }\n    for(int i=maxh;i>=0;i--)\n    {\n        if(judge(i))\n        {\n            cout<<i<<endl;\n            break;\n        }\n    }\n}\n```","slug":"acm-jimuchengbao-md","published":1,"updated":"2021-11-06T16:16:46.543Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckvo0et3l0000t9oe2alhh4k5","content":"<p>XC的儿子小XC最喜欢玩的游戏用积木垒漂亮的城堡。城堡是用一些立方体的积木垒成的，城堡的每一层是一块积木。小XC是一个比他爸爸XC还聪明的孩子，他发现垒城堡的时候，如果下面的积木比上面的积木大，那么城堡便不容易倒。所以他在垒城堡的时候总是遵循这样的规则。<br>小XC想把自己垒的城堡送给幼儿园里漂亮的女孩子们，这样可以增加他的好感度。为了公平起见，他决定把送给每个女孩子一样高的城堡，这样可以避免女孩子们为了获得更漂亮的城堡而引起争执。可是他发现自己在垒城堡的时候并没有预先考虑到这一点。所以他现在要改造城堡。由于他没有多余的积木了，他灵机一动，想出了一个巧妙的改造方案。他决定从每一个城堡中挪去一些积木，使得最终每座城堡都一样高。为了使他的城堡更雄伟，他觉得应该使最后的城堡都尽可能的高。<br>任务：<br>请你帮助小XC编一个程序，根据他垒的所有城堡的信息，决定应该移去哪些积木才能获得最佳的效果。</p>\n<h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><p>第一行是一个整数 N (N≤100)， 表示一共有几座城堡。以下 N 行每行是一系列非负整数，用一个空格分隔，按从下往上的顺序依次给出一座城堡中所有积木的棱长。用-1结束。一座城堡中的积木不超过100块，每块积木的棱长不超过100。</p>\n<h2 id=\"Output\"><a href=\"#Output\" class=\"headerlink\" title=\"Output\"></a>Output</h2><p>一个整数，表示最后城堡的最大可能的高度。如果找不到合适的方案，则输出 0 。</p>\n<h2 id=\"EX\"><a href=\"#EX\" class=\"headerlink\" title=\"EX\"></a>EX</h2><p>2<br>2 1 –1<br>3 2 1 –1</p>\n<p>3</p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>思路就是每次输入一个城堡的高度信息就多一次01背包类型的dp，得到它所能得到的所有高度，保存在dp[i][j]中，这个其实是用一维数组进行dp比较方便，i表示第几个城堡，与dp关系不大，j表示堡垒的最大高度（类比背包size）。<br>然后对i组堡垒进行取高度的交集，交集中最大的值就是结果。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cmath&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;string&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> dp[<span class=\"number\">105</span>][<span class=\"number\">10005</span>],len[<span class=\"number\">105</span>][<span class=\"number\">105</span>],cnt[<span class=\"number\">105</span>];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//len记录每个城堡每块积木的长度，cnt记录每个城堡的积木数</span></span><br><span class=\"line\"><span class=\"keyword\">int</span> n,maxh;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">judge</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span><span class=\"comment\">//判断高度x是否所有城堡都可到达</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;=n;i++)</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(dp[i][x]!=x)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; <span class=\"number\">105</span>;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>;j &lt; <span class=\"number\">10005</span>;j++)&#123;</span><br><span class=\"line\">            dp[i][j] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    cin&gt;&gt;n;</span><br><span class=\"line\">    maxh=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;=n;i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> sum = <span class=\"number\">0</span>, tlen;</span><br><span class=\"line\">        cnt[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        cin &gt;&gt; tlen;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (tlen &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            cnt[i]++;</span><br><span class=\"line\">            len[i][cnt[i]] = tlen;</span><br><span class=\"line\">            sum += tlen;</span><br><span class=\"line\">            cin &gt;&gt; tlen;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sum &gt; maxh)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            maxh = sum;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = <span class=\"number\">1</span>; j &lt;= cnt[i]; j++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = maxh; k &gt;= len[i][j]; k--) &#123;</span><br><span class=\"line\">                dp[i][k] = <span class=\"built_in\">max</span>(dp[i][k], dp[i][k - len[i][j]] + len[i][j]);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=maxh;i&gt;=<span class=\"number\">0</span>;i--)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"built_in\">judge</span>(i))</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            cout&lt;&lt;i&lt;&lt;endl;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>XC的儿子小XC最喜欢玩的游戏用积木垒漂亮的城堡。城堡是用一些立方体的积木垒成的，城堡的每一层是一块积木。小XC是一个比他爸爸XC还聪明的孩子，他发现垒城堡的时候，如果下面的积木比上面的积木大，那么城堡便不容易倒。所以他在垒城堡的时候总是遵循这样的规则。<br>小XC想把自己垒的城堡送给幼儿园里漂亮的女孩子们，这样可以增加他的好感度。为了公平起见，他决定把送给每个女孩子一样高的城堡，这样可以避免女孩子们为了获得更漂亮的城堡而引起争执。可是他发现自己在垒城堡的时候并没有预先考虑到这一点。所以他现在要改造城堡。由于他没有多余的积木了，他灵机一动，想出了一个巧妙的改造方案。他决定从每一个城堡中挪去一些积木，使得最终每座城堡都一样高。为了使他的城堡更雄伟，他觉得应该使最后的城堡都尽可能的高。<br>任务：<br>请你帮助小XC编一个程序，根据他垒的所有城堡的信息，决定应该移去哪些积木才能获得最佳的效果。</p>\n<h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><p>第一行是一个整数 N (N≤100)， 表示一共有几座城堡。以下 N 行每行是一系列非负整数，用一个空格分隔，按从下往上的顺序依次给出一座城堡中所有积木的棱长。用-1结束。一座城堡中的积木不超过100块，每块积木的棱长不超过100。</p>\n<h2 id=\"Output\"><a href=\"#Output\" class=\"headerlink\" title=\"Output\"></a>Output</h2><p>一个整数，表示最后城堡的最大可能的高度。如果找不到合适的方案，则输出 0 。</p>\n<h2 id=\"EX\"><a href=\"#EX\" class=\"headerlink\" title=\"EX\"></a>EX</h2><p>2<br>2 1 –1<br>3 2 1 –1</p>\n<p>3</p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>思路就是每次输入一个城堡的高度信息就多一次01背包类型的dp，得到它所能得到的所有高度，保存在dp[i][j]中，这个其实是用一维数组进行dp比较方便，i表示第几个城堡，与dp关系不大，j表示堡垒的最大高度（类比背包size）。<br>然后对i组堡垒进行取高度的交集，交集中最大的值就是结果。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cmath&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;string&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> dp[<span class=\"number\">105</span>][<span class=\"number\">10005</span>],len[<span class=\"number\">105</span>][<span class=\"number\">105</span>],cnt[<span class=\"number\">105</span>];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//len记录每个城堡每块积木的长度，cnt记录每个城堡的积木数</span></span><br><span class=\"line\"><span class=\"keyword\">int</span> n,maxh;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">judge</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span><span class=\"comment\">//判断高度x是否所有城堡都可到达</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;=n;i++)</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(dp[i][x]!=x)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; <span class=\"number\">105</span>;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>;j &lt; <span class=\"number\">10005</span>;j++)&#123;</span><br><span class=\"line\">            dp[i][j] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    cin&gt;&gt;n;</span><br><span class=\"line\">    maxh=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;=n;i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> sum = <span class=\"number\">0</span>, tlen;</span><br><span class=\"line\">        cnt[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        cin &gt;&gt; tlen;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (tlen &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            cnt[i]++;</span><br><span class=\"line\">            len[i][cnt[i]] = tlen;</span><br><span class=\"line\">            sum += tlen;</span><br><span class=\"line\">            cin &gt;&gt; tlen;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sum &gt; maxh)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            maxh = sum;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = <span class=\"number\">1</span>; j &lt;= cnt[i]; j++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = maxh; k &gt;= len[i][j]; k--) &#123;</span><br><span class=\"line\">                dp[i][k] = <span class=\"built_in\">max</span>(dp[i][k], dp[i][k - len[i][j]] + len[i][j]);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=maxh;i&gt;=<span class=\"number\">0</span>;i--)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"built_in\">judge</span>(i))</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            cout&lt;&lt;i&lt;&lt;endl;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"Warcraft III 守望者的烦恼","date":"2021-11-06T16:19:37.000Z","_content":"\n头脑并不发达的warden最近在思考一个问题，她的闪烁技能是可以升级的，k级的闪烁技能最多可以向前移动k个监狱，一共有n个监狱要视察，她从入口进去，一路上有n个监狱，而且不会往回走，当然她并不用每个监狱都视察，但是她最后一定要到第n个监狱里去，因为监狱的出口在那里，但是她并不一定要到第1个监狱。\n守望者warden现在想知道，她在拥有k级闪烁技能时视察n个监狱一共有多少种方案？\n\n## Input\n第一行是闪烁技能的等级 k (1≤k≤10)\n第二行是监狱的个数 n (1≤n≤231−1)\n\n## Output\n由于方案个数会很多，所以输出它 mod 7777777后的结果就行了\n\n## EX\n2\n4\n\n5\n\n## 思路\n这道题的状态转移很好找到达第n个监狱的方案数位到达n-1,n-2....n-k个监狱的方案数之和，至于为什么到n-k，因为监狱编号再小是无法通过一步直接到n的，也需要先到n-1~n-k中的其中一个再到n，考虑进来就重复了，他们只需要考虑在到达第n-1~n-k个监狱就行了。\n\n但是有个问题是这个数太大了，直接递归恐怕爆了。所以这里引入了矩阵乘法解决递推问题。依据是：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=[a_k,&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]*&space;\\begin{bmatrix}&space;1&space;&1&space;&0&space;&0&space;\\\\&space;1&space;&0&space;&1&space;&0&space;\\\\&space;1&space;&0&space;&0&space;&1&space;\\\\&space;1&space;&0&space;&0&space;&0&space;\\end{bmatrix}&space;=&space;[a_{k&plus;1},&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?[a_k,&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]*&space;\\begin{bmatrix}&space;1&space;&1&space;&0&space;&0&space;\\\\&space;1&space;&0&space;&1&space;&0&space;\\\\&space;1&space;&0&space;&0&space;&1&space;\\\\&space;1&space;&0&space;&0&space;&0&space;\\end{bmatrix}&space;=&space;[a_{k&plus;1},&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]\" title=\"[a_k, a_{k-1}, a_{k - 2},a_{k-3} ]* \\begin{bmatrix} 1 &1 &0 &0 \\\\ 1 &0 &1 &0 \\\\ 1 &0 &0 &1 \\\\ 1 &0 &0 &0 \\end{bmatrix} = [a_{k+1}, a_{k-1}, a_{k - 2},a_{k-3} ]\" /></a>\n\n然后再将得到的结果乘这个矩阵，就可以得到an，也就是想要的结果。\n\n矩阵乘法好写，但是多个矩阵相乘这里直接用的快速幂。\n\n\n## Code\n```cpp\n#include<cstdio>\n#include<iostream>\n#include<cmath>\n#include<string>\nusing namespace std;\n\n#define mod 7777777\n\nint k;\nlong long n;\n\nstruct Matrix{\n    int r,c;\n    long long M[11][11];\n    void init(int r, int c){\n        this->r = r;\n        this->c = c;\n        for(int i = 0;i < r;i++){\n            for(int j = 0;j < c;j++){\n                M[i][j] = 0;\n            }\n        }\n    }\n\n    Matrix operator*(Matrix& B) const{      //重载矩阵乘法\n        Matrix A = *this;\n        Matrix C;\n        C.init(A.r,B.c);\n\n        for(int i = 0;i <C.r;i++){\n            for(int j = 0;j < C.c;j++){\n                for(int q = 0;q < A.c;q++){\n                    C.M[i][j] = (C.M[i][j] + A.M[i][q] * B.M[q][j]) % mod;\n                }\n            }\n        }\n        return C;\n    }\n\n    Matrix Q_pow(long long p){            //矩阵快速幂\n        Matrix tmp = *this;\n        Matrix ans;\n        ans.init(this->r, this->r);\n        for(int i = 0;i < this->r;i++){     //单位矩阵\n            ans.M[i][i] = 1;\n        }\n        while(p){\n            if(p & 1){\n                ans = ans * tmp;\n            }\n            tmp = tmp * tmp;\n            p >>= 1;\n        }\n        return ans;\n    }\n};\n\n\nint main()\n{\n    cin>>k;\n    cin>>n;\n\n    Matrix factor;\n    Matrix base;\n    factor.init(k,k);\n    for(int i = 0;i < k;i++){\n        factor.M[i][0] = 1;\n    }\n    for(int i = 1;i <= k;i++){\n        factor.M[i - 1][i] = 1;\n    }\n    factor = factor.Q_pow(n);\n    base.init(1,k);\n    base.M[0][0] = 1;\n    base = base * factor;\n    cout<<base.M[0][0]<<endl;\n}\n\n```","source":"_posts/acm-shouwangzhe-md.md","raw":"---\ntitle: Warcraft III 守望者的烦恼 \ndate: 2021-11-07 00:19:37\ntags: ACM\n---\n\n头脑并不发达的warden最近在思考一个问题，她的闪烁技能是可以升级的，k级的闪烁技能最多可以向前移动k个监狱，一共有n个监狱要视察，她从入口进去，一路上有n个监狱，而且不会往回走，当然她并不用每个监狱都视察，但是她最后一定要到第n个监狱里去，因为监狱的出口在那里，但是她并不一定要到第1个监狱。\n守望者warden现在想知道，她在拥有k级闪烁技能时视察n个监狱一共有多少种方案？\n\n## Input\n第一行是闪烁技能的等级 k (1≤k≤10)\n第二行是监狱的个数 n (1≤n≤231−1)\n\n## Output\n由于方案个数会很多，所以输出它 mod 7777777后的结果就行了\n\n## EX\n2\n4\n\n5\n\n## 思路\n这道题的状态转移很好找到达第n个监狱的方案数位到达n-1,n-2....n-k个监狱的方案数之和，至于为什么到n-k，因为监狱编号再小是无法通过一步直接到n的，也需要先到n-1~n-k中的其中一个再到n，考虑进来就重复了，他们只需要考虑在到达第n-1~n-k个监狱就行了。\n\n但是有个问题是这个数太大了，直接递归恐怕爆了。所以这里引入了矩阵乘法解决递推问题。依据是：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=[a_k,&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]*&space;\\begin{bmatrix}&space;1&space;&1&space;&0&space;&0&space;\\\\&space;1&space;&0&space;&1&space;&0&space;\\\\&space;1&space;&0&space;&0&space;&1&space;\\\\&space;1&space;&0&space;&0&space;&0&space;\\end{bmatrix}&space;=&space;[a_{k&plus;1},&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?[a_k,&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]*&space;\\begin{bmatrix}&space;1&space;&1&space;&0&space;&0&space;\\\\&space;1&space;&0&space;&1&space;&0&space;\\\\&space;1&space;&0&space;&0&space;&1&space;\\\\&space;1&space;&0&space;&0&space;&0&space;\\end{bmatrix}&space;=&space;[a_{k&plus;1},&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]\" title=\"[a_k, a_{k-1}, a_{k - 2},a_{k-3} ]* \\begin{bmatrix} 1 &1 &0 &0 \\\\ 1 &0 &1 &0 \\\\ 1 &0 &0 &1 \\\\ 1 &0 &0 &0 \\end{bmatrix} = [a_{k+1}, a_{k-1}, a_{k - 2},a_{k-3} ]\" /></a>\n\n然后再将得到的结果乘这个矩阵，就可以得到an，也就是想要的结果。\n\n矩阵乘法好写，但是多个矩阵相乘这里直接用的快速幂。\n\n\n## Code\n```cpp\n#include<cstdio>\n#include<iostream>\n#include<cmath>\n#include<string>\nusing namespace std;\n\n#define mod 7777777\n\nint k;\nlong long n;\n\nstruct Matrix{\n    int r,c;\n    long long M[11][11];\n    void init(int r, int c){\n        this->r = r;\n        this->c = c;\n        for(int i = 0;i < r;i++){\n            for(int j = 0;j < c;j++){\n                M[i][j] = 0;\n            }\n        }\n    }\n\n    Matrix operator*(Matrix& B) const{      //重载矩阵乘法\n        Matrix A = *this;\n        Matrix C;\n        C.init(A.r,B.c);\n\n        for(int i = 0;i <C.r;i++){\n            for(int j = 0;j < C.c;j++){\n                for(int q = 0;q < A.c;q++){\n                    C.M[i][j] = (C.M[i][j] + A.M[i][q] * B.M[q][j]) % mod;\n                }\n            }\n        }\n        return C;\n    }\n\n    Matrix Q_pow(long long p){            //矩阵快速幂\n        Matrix tmp = *this;\n        Matrix ans;\n        ans.init(this->r, this->r);\n        for(int i = 0;i < this->r;i++){     //单位矩阵\n            ans.M[i][i] = 1;\n        }\n        while(p){\n            if(p & 1){\n                ans = ans * tmp;\n            }\n            tmp = tmp * tmp;\n            p >>= 1;\n        }\n        return ans;\n    }\n};\n\n\nint main()\n{\n    cin>>k;\n    cin>>n;\n\n    Matrix factor;\n    Matrix base;\n    factor.init(k,k);\n    for(int i = 0;i < k;i++){\n        factor.M[i][0] = 1;\n    }\n    for(int i = 1;i <= k;i++){\n        factor.M[i - 1][i] = 1;\n    }\n    factor = factor.Q_pow(n);\n    base.init(1,k);\n    base.M[0][0] = 1;\n    base = base * factor;\n    cout<<base.M[0][0]<<endl;\n}\n\n```","slug":"acm-shouwangzhe-md","published":1,"updated":"2021-11-06T16:33:42.640Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckvo10kj30000ymoe4gji7uw6","content":"<p>头脑并不发达的warden最近在思考一个问题，她的闪烁技能是可以升级的，k级的闪烁技能最多可以向前移动k个监狱，一共有n个监狱要视察，她从入口进去，一路上有n个监狱，而且不会往回走，当然她并不用每个监狱都视察，但是她最后一定要到第n个监狱里去，因为监狱的出口在那里，但是她并不一定要到第1个监狱。<br>守望者warden现在想知道，她在拥有k级闪烁技能时视察n个监狱一共有多少种方案？</p>\n<h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><p>第一行是闪烁技能的等级 k (1≤k≤10)<br>第二行是监狱的个数 n (1≤n≤231−1)</p>\n<h2 id=\"Output\"><a href=\"#Output\" class=\"headerlink\" title=\"Output\"></a>Output</h2><p>由于方案个数会很多，所以输出它 mod 7777777后的结果就行了</p>\n<h2 id=\"EX\"><a href=\"#EX\" class=\"headerlink\" title=\"EX\"></a>EX</h2><p>2<br>4</p>\n<p>5</p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>这道题的状态转移很好找到达第n个监狱的方案数位到达n-1,n-2….n-k个监狱的方案数之和，至于为什么到n-k，因为监狱编号再小是无法通过一步直接到n的，也需要先到n-1<del>n-k中的其中一个再到n，考虑进来就重复了，他们只需要考虑在到达第n-1</del>n-k个监狱就行了。</p>\n<p>但是有个问题是这个数太大了，直接递归恐怕爆了。所以这里引入了矩阵乘法解决递推问题。依据是：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=[a_k,&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]*&space;\\begin{bmatrix}&space;1&space;&1&space;&0&space;&0&space;\\\\&space;1&space;&0&space;&1&space;&0&space;\\\\&space;1&space;&0&space;&0&space;&1&space;\\\\&space;1&space;&0&space;&0&space;&0&space;\\end{bmatrix}&space;=&space;[a_{k&plus;1},&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?[a_k,&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]*&space;\\begin{bmatrix}&space;1&space;&1&space;&0&space;&0&space;\\\\&space;1&space;&0&space;&1&space;&0&space;\\\\&space;1&space;&0&space;&0&space;&1&space;\\\\&space;1&space;&0&space;&0&space;&0&space;\\end{bmatrix}&space;=&space;[a_{k&plus;1},&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]\" title=\"[a_k, a_{k-1}, a_{k - 2},a_{k-3} ]* \\begin{bmatrix} 1 &1 &0 &0 \\\\ 1 &0 &1 &0 \\\\ 1 &0 &0 &1 \\\\ 1 &0 &0 &0 \\end{bmatrix} = [a_{k+1}, a_{k-1}, a_{k - 2},a_{k-3} ]\" /></a></p>\n<p>然后再将得到的结果乘这个矩阵，就可以得到an，也就是想要的结果。</p>\n<p>矩阵乘法好写，但是多个矩阵相乘这里直接用的快速幂。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cmath&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;string&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> mod 7777777</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> k;</span><br><span class=\"line\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> n;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">Matrix</span>&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> r,c;</span><br><span class=\"line\">    <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> M[<span class=\"number\">11</span>][<span class=\"number\">11</span>];</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">init</span><span class=\"params\">(<span class=\"keyword\">int</span> r, <span class=\"keyword\">int</span> c)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;r = r;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;c = c;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; r;i++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>;j &lt; c;j++)&#123;</span><br><span class=\"line\">                M[i][j] = <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>*(Matrix&amp; B) <span class=\"keyword\">const</span>&#123;      <span class=\"comment\">//重载矩阵乘法</span></span><br><span class=\"line\">        Matrix A = *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">        Matrix C;</span><br><span class=\"line\">        C.<span class=\"built_in\">init</span>(A.r,B.c);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt;C.r;i++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>;j &lt; C.c;j++)&#123;</span><br><span class=\"line\">                <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> q = <span class=\"number\">0</span>;q &lt; A.c;q++)&#123;</span><br><span class=\"line\">                    C.M[i][j] = (C.M[i][j] + A.M[i][q] * B.M[q][j]) % mod;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> C;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">Q_pow</span><span class=\"params\">(<span class=\"keyword\">long</span> <span class=\"keyword\">long</span> p)</span></span>&#123;            <span class=\"comment\">//矩阵快速幂</span></span><br><span class=\"line\">        Matrix tmp = *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">        Matrix ans;</span><br><span class=\"line\">        ans.<span class=\"built_in\">init</span>(<span class=\"keyword\">this</span>-&gt;r, <span class=\"keyword\">this</span>-&gt;r);</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; <span class=\"keyword\">this</span>-&gt;r;i++)&#123;     <span class=\"comment\">//单位矩阵</span></span><br><span class=\"line\">            ans.M[i][i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(p)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(p &amp; <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">                ans = ans * tmp;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            tmp = tmp * tmp;</span><br><span class=\"line\">            p &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    cin&gt;&gt;k;</span><br><span class=\"line\">    cin&gt;&gt;n;</span><br><span class=\"line\"></span><br><span class=\"line\">    Matrix factor;</span><br><span class=\"line\">    Matrix base;</span><br><span class=\"line\">    factor.<span class=\"built_in\">init</span>(k,k);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; k;i++)&#123;</span><br><span class=\"line\">        factor.M[i][<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>;i &lt;= k;i++)&#123;</span><br><span class=\"line\">        factor.M[i - <span class=\"number\">1</span>][i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    factor = factor.<span class=\"built_in\">Q_pow</span>(n);</span><br><span class=\"line\">    base.<span class=\"built_in\">init</span>(<span class=\"number\">1</span>,k);</span><br><span class=\"line\">    base.M[<span class=\"number\">0</span>][<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    base = base * factor;</span><br><span class=\"line\">    cout&lt;&lt;base.M[<span class=\"number\">0</span>][<span class=\"number\">0</span>]&lt;&lt;endl;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>头脑并不发达的warden最近在思考一个问题，她的闪烁技能是可以升级的，k级的闪烁技能最多可以向前移动k个监狱，一共有n个监狱要视察，她从入口进去，一路上有n个监狱，而且不会往回走，当然她并不用每个监狱都视察，但是她最后一定要到第n个监狱里去，因为监狱的出口在那里，但是她并不一定要到第1个监狱。<br>守望者warden现在想知道，她在拥有k级闪烁技能时视察n个监狱一共有多少种方案？</p>\n<h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><p>第一行是闪烁技能的等级 k (1≤k≤10)<br>第二行是监狱的个数 n (1≤n≤231−1)</p>\n<h2 id=\"Output\"><a href=\"#Output\" class=\"headerlink\" title=\"Output\"></a>Output</h2><p>由于方案个数会很多，所以输出它 mod 7777777后的结果就行了</p>\n<h2 id=\"EX\"><a href=\"#EX\" class=\"headerlink\" title=\"EX\"></a>EX</h2><p>2<br>4</p>\n<p>5</p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>这道题的状态转移很好找到达第n个监狱的方案数位到达n-1,n-2….n-k个监狱的方案数之和，至于为什么到n-k，因为监狱编号再小是无法通过一步直接到n的，也需要先到n-1<del>n-k中的其中一个再到n，考虑进来就重复了，他们只需要考虑在到达第n-1</del>n-k个监狱就行了。</p>\n<p>但是有个问题是这个数太大了，直接递归恐怕爆了。所以这里引入了矩阵乘法解决递推问题。依据是：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=[a_k,&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]*&space;\\begin{bmatrix}&space;1&space;&1&space;&0&space;&0&space;\\\\&space;1&space;&0&space;&1&space;&0&space;\\\\&space;1&space;&0&space;&0&space;&1&space;\\\\&space;1&space;&0&space;&0&space;&0&space;\\end{bmatrix}&space;=&space;[a_{k&plus;1},&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?[a_k,&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]*&space;\\begin{bmatrix}&space;1&space;&1&space;&0&space;&0&space;\\\\&space;1&space;&0&space;&1&space;&0&space;\\\\&space;1&space;&0&space;&0&space;&1&space;\\\\&space;1&space;&0&space;&0&space;&0&space;\\end{bmatrix}&space;=&space;[a_{k&plus;1},&space;a_{k-1},&space;a_{k&space;-&space;2},a_{k-3}&space;]\" title=\"[a_k, a_{k-1}, a_{k - 2},a_{k-3} ]* \\begin{bmatrix} 1 &1 &0 &0 \\\\ 1 &0 &1 &0 \\\\ 1 &0 &0 &1 \\\\ 1 &0 &0 &0 \\end{bmatrix} = [a_{k+1}, a_{k-1}, a_{k - 2},a_{k-3} ]\" /></a></p>\n<p>然后再将得到的结果乘这个矩阵，就可以得到an，也就是想要的结果。</p>\n<p>矩阵乘法好写，但是多个矩阵相乘这里直接用的快速幂。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cmath&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;string&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> mod 7777777</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> k;</span><br><span class=\"line\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> n;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">Matrix</span>&#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> r,c;</span><br><span class=\"line\">    <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> M[<span class=\"number\">11</span>][<span class=\"number\">11</span>];</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">init</span><span class=\"params\">(<span class=\"keyword\">int</span> r, <span class=\"keyword\">int</span> c)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;r = r;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;c = c;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; r;i++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>;j &lt; c;j++)&#123;</span><br><span class=\"line\">                M[i][j] = <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>*(Matrix&amp; B) <span class=\"keyword\">const</span>&#123;      <span class=\"comment\">//重载矩阵乘法</span></span><br><span class=\"line\">        Matrix A = *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">        Matrix C;</span><br><span class=\"line\">        C.<span class=\"built_in\">init</span>(A.r,B.c);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt;C.r;i++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>;j &lt; C.c;j++)&#123;</span><br><span class=\"line\">                <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> q = <span class=\"number\">0</span>;q &lt; A.c;q++)&#123;</span><br><span class=\"line\">                    C.M[i][j] = (C.M[i][j] + A.M[i][q] * B.M[q][j]) % mod;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> C;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">Q_pow</span><span class=\"params\">(<span class=\"keyword\">long</span> <span class=\"keyword\">long</span> p)</span></span>&#123;            <span class=\"comment\">//矩阵快速幂</span></span><br><span class=\"line\">        Matrix tmp = *<span class=\"keyword\">this</span>;</span><br><span class=\"line\">        Matrix ans;</span><br><span class=\"line\">        ans.<span class=\"built_in\">init</span>(<span class=\"keyword\">this</span>-&gt;r, <span class=\"keyword\">this</span>-&gt;r);</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; <span class=\"keyword\">this</span>-&gt;r;i++)&#123;     <span class=\"comment\">//单位矩阵</span></span><br><span class=\"line\">            ans.M[i][i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(p)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(p &amp; <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">                ans = ans * tmp;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            tmp = tmp * tmp;</span><br><span class=\"line\">            p &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    cin&gt;&gt;k;</span><br><span class=\"line\">    cin&gt;&gt;n;</span><br><span class=\"line\"></span><br><span class=\"line\">    Matrix factor;</span><br><span class=\"line\">    Matrix base;</span><br><span class=\"line\">    factor.<span class=\"built_in\">init</span>(k,k);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; k;i++)&#123;</span><br><span class=\"line\">        factor.M[i][<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>;i &lt;= k;i++)&#123;</span><br><span class=\"line\">        factor.M[i - <span class=\"number\">1</span>][i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    factor = factor.<span class=\"built_in\">Q_pow</span>(n);</span><br><span class=\"line\">    base.<span class=\"built_in\">init</span>(<span class=\"number\">1</span>,k);</span><br><span class=\"line\">    base.M[<span class=\"number\">0</span>][<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    base = base * factor;</span><br><span class=\"line\">    cout&lt;&lt;base.M[<span class=\"number\">0</span>][<span class=\"number\">0</span>]&lt;&lt;endl;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>"},{"title":"加分二叉树","date":"2021-11-07T04:15:41.000Z","_content":"\n设一个n个节点的二叉树tree的中序遍历为（l,2,3,…,n），其中数字1,2,3,…,n为节点编号。每个节点都有一个分数（均为正整数），记第i个节点的分数为di，tree及它的每个子树都有一个加分，任一棵子树subtree（也包含tree本身）的加分计算方法如下：\n\n　　subtree的左子树的加分× subtree的右子树的加分＋subtree的根的分数\n\n　　若某个子树为空，规定其加分为1，叶子的加分就是叶节点本身的分数。不考虑它的空子树。\n\n　　试求一棵符合中序遍历为（1,2,3,…,n）且加分最高的二叉树tree。要求输出；\n\n　　（1）tree的最高加分\n\n　　（2）tree的前序遍历\n\n## Input\n第 1 行：一个整数 n (n＜30)， 为节点个数。\n\n第 2 行 ：n 个用空格隔开的整数，为每个节点的分数（分数 ＜100）。\n\n## Output\n第 1 行：一个整数，为最高加分（结果不会超过4,000,000,000）。\n\n第 2 行 ：n 个用空格隔开的整数，为该树的前序遍历。\n\n若存在多种前序遍历均为最高加分，则输出字典序最小的前序遍历\n\n## EX\n5\n5 7 1 2 10\n\n145\n3 1 2 4 5\n\n## 思路\n问题的本质在于寻找到一个最优的二叉树，评估标准是根节点的加分，而跟节点的加分又与它的两个son的加分有关，所以递归方程很好寻找。\n\n可以把中序遍历的序列分为几层，在最外一层寻找最优的根节点，然后在根结点的左右分别找下一层的根结点，每一层的每一个可能根节点的加分都要递归到最后才能算出，所以就能写出来了。\n\n最后要输出前序遍历的序列。这需要用一个变量存下每一层的根结点，然后输出的时候先输出每一层的根结点然后左右层的根结点...以此类推。\n\n\n## Code\n```cpp\n#include<iostream>\n#include<cstdio>\nusing namespace std;\n\nint n,a[40],root[40][40];\nlong long dp[40][40];\n\nlong long dfs(int L,int R){\n    if(L>R) return 1;       //因为要左右子节点相乘，所以如果是空则给1\n\n    if(dp[L][R]) return dp[L][R];  //关键一步，记忆化，以前走过那就直接返回结果\n\n    long long maxn = 0;\n    for(int i=L;i<R;i++){       //递归地寻找加分最大地那个父节点\n        long long t = dfs(L,i-1) * dfs(i+1,R) + a[i];   //寻找左侧和右侧地最大父节点对应的加分\n        if(t > maxn){\n            maxn = t;\n            root[L][R] = i;//L-R的最父节点为i\n        }\n    }\n    return dp[L][R] = maxn;\n}\n\n\nvoid dg(int L,int R){           //前序遍历\n    if(L>R)\n    {\n        return ;\n    }\n    cout<<root[L][R]<<\" \";\n    dg(L,root[L][R]-1);\n    dg(root[L][R]+1,R);\n}\n\nint main(){\n    cin>>n;\n    for(int i=1;i<=n;i++){\n        cin>>a[i];\n        dp[i][i] = a[i];\n        root[i][i] = i;\n    }\n    cout<<dfs(1,n)<<endl;\n    dg(1,n);\n\n    return 0;\n}\n```","source":"_posts/acm-jiafenerchashu-md.md","raw":"---\ntitle: 加分二叉树\ndate: 2021-11-07 12:15:41\ntags: ACM\n---\n\n设一个n个节点的二叉树tree的中序遍历为（l,2,3,…,n），其中数字1,2,3,…,n为节点编号。每个节点都有一个分数（均为正整数），记第i个节点的分数为di，tree及它的每个子树都有一个加分，任一棵子树subtree（也包含tree本身）的加分计算方法如下：\n\n　　subtree的左子树的加分× subtree的右子树的加分＋subtree的根的分数\n\n　　若某个子树为空，规定其加分为1，叶子的加分就是叶节点本身的分数。不考虑它的空子树。\n\n　　试求一棵符合中序遍历为（1,2,3,…,n）且加分最高的二叉树tree。要求输出；\n\n　　（1）tree的最高加分\n\n　　（2）tree的前序遍历\n\n## Input\n第 1 行：一个整数 n (n＜30)， 为节点个数。\n\n第 2 行 ：n 个用空格隔开的整数，为每个节点的分数（分数 ＜100）。\n\n## Output\n第 1 行：一个整数，为最高加分（结果不会超过4,000,000,000）。\n\n第 2 行 ：n 个用空格隔开的整数，为该树的前序遍历。\n\n若存在多种前序遍历均为最高加分，则输出字典序最小的前序遍历\n\n## EX\n5\n5 7 1 2 10\n\n145\n3 1 2 4 5\n\n## 思路\n问题的本质在于寻找到一个最优的二叉树，评估标准是根节点的加分，而跟节点的加分又与它的两个son的加分有关，所以递归方程很好寻找。\n\n可以把中序遍历的序列分为几层，在最外一层寻找最优的根节点，然后在根结点的左右分别找下一层的根结点，每一层的每一个可能根节点的加分都要递归到最后才能算出，所以就能写出来了。\n\n最后要输出前序遍历的序列。这需要用一个变量存下每一层的根结点，然后输出的时候先输出每一层的根结点然后左右层的根结点...以此类推。\n\n\n## Code\n```cpp\n#include<iostream>\n#include<cstdio>\nusing namespace std;\n\nint n,a[40],root[40][40];\nlong long dp[40][40];\n\nlong long dfs(int L,int R){\n    if(L>R) return 1;       //因为要左右子节点相乘，所以如果是空则给1\n\n    if(dp[L][R]) return dp[L][R];  //关键一步，记忆化，以前走过那就直接返回结果\n\n    long long maxn = 0;\n    for(int i=L;i<R;i++){       //递归地寻找加分最大地那个父节点\n        long long t = dfs(L,i-1) * dfs(i+1,R) + a[i];   //寻找左侧和右侧地最大父节点对应的加分\n        if(t > maxn){\n            maxn = t;\n            root[L][R] = i;//L-R的最父节点为i\n        }\n    }\n    return dp[L][R] = maxn;\n}\n\n\nvoid dg(int L,int R){           //前序遍历\n    if(L>R)\n    {\n        return ;\n    }\n    cout<<root[L][R]<<\" \";\n    dg(L,root[L][R]-1);\n    dg(root[L][R]+1,R);\n}\n\nint main(){\n    cin>>n;\n    for(int i=1;i<=n;i++){\n        cin>>a[i];\n        dp[i][i] = a[i];\n        root[i][i] = i;\n    }\n    cout<<dfs(1,n)<<endl;\n    dg(1,n);\n\n    return 0;\n}\n```","slug":"acm-jiafenerchashu-md","published":1,"updated":"2021-11-07T04:36:51.703Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckvoqukk3000043oeh0htc0xa","content":"<p>设一个n个节点的二叉树tree的中序遍历为（l,2,3,…,n），其中数字1,2,3,…,n为节点编号。每个节点都有一个分数（均为正整数），记第i个节点的分数为di，tree及它的每个子树都有一个加分，任一棵子树subtree（也包含tree本身）的加分计算方法如下：</p>\n<p>　　subtree的左子树的加分× subtree的右子树的加分＋subtree的根的分数</p>\n<p>　　若某个子树为空，规定其加分为1，叶子的加分就是叶节点本身的分数。不考虑它的空子树。</p>\n<p>　　试求一棵符合中序遍历为（1,2,3,…,n）且加分最高的二叉树tree。要求输出；</p>\n<p>　　（1）tree的最高加分</p>\n<p>　　（2）tree的前序遍历</p>\n<h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><p>第 1 行：一个整数 n (n＜30)， 为节点个数。</p>\n<p>第 2 行 ：n 个用空格隔开的整数，为每个节点的分数（分数 ＜100）。</p>\n<h2 id=\"Output\"><a href=\"#Output\" class=\"headerlink\" title=\"Output\"></a>Output</h2><p>第 1 行：一个整数，为最高加分（结果不会超过4,000,000,000）。</p>\n<p>第 2 行 ：n 个用空格隔开的整数，为该树的前序遍历。</p>\n<p>若存在多种前序遍历均为最高加分，则输出字典序最小的前序遍历</p>\n<h2 id=\"EX\"><a href=\"#EX\" class=\"headerlink\" title=\"EX\"></a>EX</h2><p>5<br>5 7 1 2 10</p>\n<p>145<br>3 1 2 4 5</p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>问题的本质在于寻找到一个最优的二叉树，评估标准是根节点的加分，而跟节点的加分又与它的两个son的加分有关，所以递归方程很好寻找。</p>\n<p>可以把中序遍历的序列分为几层，在最外一层寻找最优的根节点，然后在根结点的左右分别找下一层的根结点，每一层的每一个可能根节点的加分都要递归到最后才能算出，所以就能写出来了。</p>\n<p>最后要输出前序遍历的序列。这需要用一个变量存下每一层的根结点，然后输出的时候先输出每一层的根结点然后左右层的根结点…以此类推。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> n,a[<span class=\"number\">40</span>],root[<span class=\"number\">40</span>][<span class=\"number\">40</span>];</span><br><span class=\"line\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> dp[<span class=\"number\">40</span>][<span class=\"number\">40</span>];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> <span class=\"title\">dfs</span><span class=\"params\">(<span class=\"keyword\">int</span> L,<span class=\"keyword\">int</span> R)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(L&gt;R) <span class=\"keyword\">return</span> <span class=\"number\">1</span>;       <span class=\"comment\">//因为要左右子节点相乘，所以如果是空则给1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(dp[L][R]) <span class=\"keyword\">return</span> dp[L][R];  <span class=\"comment\">//关键一步，记忆化，以前走过那就直接返回结果</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> maxn = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=L;i&lt;R;i++)&#123;       <span class=\"comment\">//递归地寻找加分最大地那个父节点</span></span><br><span class=\"line\">        <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> t = <span class=\"built_in\">dfs</span>(L,i<span class=\"number\">-1</span>) * <span class=\"built_in\">dfs</span>(i+<span class=\"number\">1</span>,R) + a[i];   <span class=\"comment\">//寻找左侧和右侧地最大父节点对应的加分</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(t &gt; maxn)&#123;</span><br><span class=\"line\">            maxn = t;</span><br><span class=\"line\">            root[L][R] = i;<span class=\"comment\">//L-R的最父节点为i</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[L][R] = maxn;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">dg</span><span class=\"params\">(<span class=\"keyword\">int</span> L,<span class=\"keyword\">int</span> R)</span></span>&#123;           <span class=\"comment\">//前序遍历</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(L&gt;R)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cout&lt;&lt;root[L][R]&lt;&lt;<span class=\"string\">&quot; &quot;</span>;</span><br><span class=\"line\">    <span class=\"built_in\">dg</span>(L,root[L][R]<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"built_in\">dg</span>(root[L][R]+<span class=\"number\">1</span>,R);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cin&gt;&gt;n;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;=n;i++)&#123;</span><br><span class=\"line\">        cin&gt;&gt;a[i];</span><br><span class=\"line\">        dp[i][i] = a[i];</span><br><span class=\"line\">        root[i][i] = i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cout&lt;&lt;<span class=\"built_in\">dfs</span>(<span class=\"number\">1</span>,n)&lt;&lt;endl;</span><br><span class=\"line\">    <span class=\"built_in\">dg</span>(<span class=\"number\">1</span>,n);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>设一个n个节点的二叉树tree的中序遍历为（l,2,3,…,n），其中数字1,2,3,…,n为节点编号。每个节点都有一个分数（均为正整数），记第i个节点的分数为di，tree及它的每个子树都有一个加分，任一棵子树subtree（也包含tree本身）的加分计算方法如下：</p>\n<p>　　subtree的左子树的加分× subtree的右子树的加分＋subtree的根的分数</p>\n<p>　　若某个子树为空，规定其加分为1，叶子的加分就是叶节点本身的分数。不考虑它的空子树。</p>\n<p>　　试求一棵符合中序遍历为（1,2,3,…,n）且加分最高的二叉树tree。要求输出；</p>\n<p>　　（1）tree的最高加分</p>\n<p>　　（2）tree的前序遍历</p>\n<h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><p>第 1 行：一个整数 n (n＜30)， 为节点个数。</p>\n<p>第 2 行 ：n 个用空格隔开的整数，为每个节点的分数（分数 ＜100）。</p>\n<h2 id=\"Output\"><a href=\"#Output\" class=\"headerlink\" title=\"Output\"></a>Output</h2><p>第 1 行：一个整数，为最高加分（结果不会超过4,000,000,000）。</p>\n<p>第 2 行 ：n 个用空格隔开的整数，为该树的前序遍历。</p>\n<p>若存在多种前序遍历均为最高加分，则输出字典序最小的前序遍历</p>\n<h2 id=\"EX\"><a href=\"#EX\" class=\"headerlink\" title=\"EX\"></a>EX</h2><p>5<br>5 7 1 2 10</p>\n<p>145<br>3 1 2 4 5</p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>问题的本质在于寻找到一个最优的二叉树，评估标准是根节点的加分，而跟节点的加分又与它的两个son的加分有关，所以递归方程很好寻找。</p>\n<p>可以把中序遍历的序列分为几层，在最外一层寻找最优的根节点，然后在根结点的左右分别找下一层的根结点，每一层的每一个可能根节点的加分都要递归到最后才能算出，所以就能写出来了。</p>\n<p>最后要输出前序遍历的序列。这需要用一个变量存下每一层的根结点，然后输出的时候先输出每一层的根结点然后左右层的根结点…以此类推。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> n,a[<span class=\"number\">40</span>],root[<span class=\"number\">40</span>][<span class=\"number\">40</span>];</span><br><span class=\"line\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> dp[<span class=\"number\">40</span>][<span class=\"number\">40</span>];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">long</span> <span class=\"keyword\">long</span> <span class=\"title\">dfs</span><span class=\"params\">(<span class=\"keyword\">int</span> L,<span class=\"keyword\">int</span> R)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(L&gt;R) <span class=\"keyword\">return</span> <span class=\"number\">1</span>;       <span class=\"comment\">//因为要左右子节点相乘，所以如果是空则给1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(dp[L][R]) <span class=\"keyword\">return</span> dp[L][R];  <span class=\"comment\">//关键一步，记忆化，以前走过那就直接返回结果</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> maxn = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=L;i&lt;R;i++)&#123;       <span class=\"comment\">//递归地寻找加分最大地那个父节点</span></span><br><span class=\"line\">        <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> t = <span class=\"built_in\">dfs</span>(L,i<span class=\"number\">-1</span>) * <span class=\"built_in\">dfs</span>(i+<span class=\"number\">1</span>,R) + a[i];   <span class=\"comment\">//寻找左侧和右侧地最大父节点对应的加分</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(t &gt; maxn)&#123;</span><br><span class=\"line\">            maxn = t;</span><br><span class=\"line\">            root[L][R] = i;<span class=\"comment\">//L-R的最父节点为i</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[L][R] = maxn;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">dg</span><span class=\"params\">(<span class=\"keyword\">int</span> L,<span class=\"keyword\">int</span> R)</span></span>&#123;           <span class=\"comment\">//前序遍历</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(L&gt;R)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cout&lt;&lt;root[L][R]&lt;&lt;<span class=\"string\">&quot; &quot;</span>;</span><br><span class=\"line\">    <span class=\"built_in\">dg</span>(L,root[L][R]<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"built_in\">dg</span>(root[L][R]+<span class=\"number\">1</span>,R);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cin&gt;&gt;n;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;=n;i++)&#123;</span><br><span class=\"line\">        cin&gt;&gt;a[i];</span><br><span class=\"line\">        dp[i][i] = a[i];</span><br><span class=\"line\">        root[i][i] = i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cout&lt;&lt;<span class=\"built_in\">dfs</span>(<span class=\"number\">1</span>,n)&lt;&lt;endl;</span><br><span class=\"line\">    <span class=\"built_in\">dg</span>(<span class=\"number\">1</span>,n);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"清帝之惑之顺治","date":"2021-11-07T11:22:27.000Z","_content":"## Problem\n顺治喜欢滑雪，这并不奇怪， 因为滑雪的确很刺激。可是为了获得速度，滑的区域必须向下倾斜，而且当你滑到坡底，你不得不再次走上坡或者等待太监们来载你。顺治想知道载一个区域中最长的滑坡。\n\n　　区域由一个二维数组给出。数组的每个数字代表点的高度。下面是一个例子：\n```\n　　 1 2 3 4 5\n　　16 17 18 19 6\n　　15 24 25 20 7\n　　14 23 22 21 8\n　　13 12 11 10 9\n```\n\n　　顺治可以从某个点滑向上下左右相邻四个点之一，当且仅当高度减小。在上面的例子中，一条可滑行的滑坡为24-17-16-1。当然25-24-23-...-3-2-1更长。事实上，这是最长的一条。\n\n## Input\n输入的第一行表示区域的行数 R 和列数 C (1≤R,C≤500) 。下面是 R 行，每行有 C 个整数，代表高度 h,0≤h<103 。\n\n## Output\n输出最长区域的长度。\n\n## EX\n```\n5 5\n1 2 3 4 5\n16 17 18 19 6\n15 24 25 20 7\n14 23 22 21 8\n13 12 11 10 9\n```\n```\n25\n```\n\n## 思路\n很典型的深搜题，rc并不算大，O(n3)不会爆，所以直接dfs每个点的最长序列。注意要记忆化每个点的长度（在dfs内记忆化效果更优，因为能保存下更多点的mem值）。\n\n## Code\n```cpp\n#include<iostream>\n#include<cstdio>\nusing namespace std;\n\nint ans = 0;\nint r,c;\nint height[502][502];\nint mem[502][502] = {0};\nint changex[4] = {-1, 1, 0, 0};\nint changey[4] = {0, 0, -1, 1};\n\nint dfs(int x, int y){\n    if(mem[x][y]){\n        return mem[x][y];\n    }\n    int maxx = 0;\n    for(int i = 0;i < 4;i++){\n        int xx = x + changex[i];\n        int yy = y + changey[i];\n        int hh = 0;\n        if(height[xx][yy] < height[x][y]){\n            hh = dfs(xx,yy);\n        }\n        hh++;\n        if(hh > maxx){\n            maxx = hh;\n        }\n    }\n    mem[x][y] = maxx;\n    return maxx;\n}\n\nint main(){\n    cin>>r>>c;\n    for(int i = 0;i <= r + 1;i++){\n        for(int j = 0;j <= c + 1;j++){\n            height[i][j] = 0x3f3f3f3f;\n            mem[i][j] = 0;\n        }\n    }\n    for(int i = 1;i <= r;i++){\n        for(int j = 1;j <= c;j++){\n            cin>>height[i][j];\n        }\n    }\n    for(int i = 1;i <= r;i++){\n        for(int j = 1;j <= c;j++){\n            int h = dfs(i,j);\n            if(h > ans){\n                ans = h;\n            }\n        }\n    }\n    cout<<ans<<endl;\n    return 0;\n}\n```\n","source":"_posts/acm-qingdi-md.md","raw":"---\ntitle:  清帝之惑之顺治\ndate: 2021-11-07 19:22:27\ntags: ACM\n---\n## Problem\n顺治喜欢滑雪，这并不奇怪， 因为滑雪的确很刺激。可是为了获得速度，滑的区域必须向下倾斜，而且当你滑到坡底，你不得不再次走上坡或者等待太监们来载你。顺治想知道载一个区域中最长的滑坡。\n\n　　区域由一个二维数组给出。数组的每个数字代表点的高度。下面是一个例子：\n```\n　　 1 2 3 4 5\n　　16 17 18 19 6\n　　15 24 25 20 7\n　　14 23 22 21 8\n　　13 12 11 10 9\n```\n\n　　顺治可以从某个点滑向上下左右相邻四个点之一，当且仅当高度减小。在上面的例子中，一条可滑行的滑坡为24-17-16-1。当然25-24-23-...-3-2-1更长。事实上，这是最长的一条。\n\n## Input\n输入的第一行表示区域的行数 R 和列数 C (1≤R,C≤500) 。下面是 R 行，每行有 C 个整数，代表高度 h,0≤h<103 。\n\n## Output\n输出最长区域的长度。\n\n## EX\n```\n5 5\n1 2 3 4 5\n16 17 18 19 6\n15 24 25 20 7\n14 23 22 21 8\n13 12 11 10 9\n```\n```\n25\n```\n\n## 思路\n很典型的深搜题，rc并不算大，O(n3)不会爆，所以直接dfs每个点的最长序列。注意要记忆化每个点的长度（在dfs内记忆化效果更优，因为能保存下更多点的mem值）。\n\n## Code\n```cpp\n#include<iostream>\n#include<cstdio>\nusing namespace std;\n\nint ans = 0;\nint r,c;\nint height[502][502];\nint mem[502][502] = {0};\nint changex[4] = {-1, 1, 0, 0};\nint changey[4] = {0, 0, -1, 1};\n\nint dfs(int x, int y){\n    if(mem[x][y]){\n        return mem[x][y];\n    }\n    int maxx = 0;\n    for(int i = 0;i < 4;i++){\n        int xx = x + changex[i];\n        int yy = y + changey[i];\n        int hh = 0;\n        if(height[xx][yy] < height[x][y]){\n            hh = dfs(xx,yy);\n        }\n        hh++;\n        if(hh > maxx){\n            maxx = hh;\n        }\n    }\n    mem[x][y] = maxx;\n    return maxx;\n}\n\nint main(){\n    cin>>r>>c;\n    for(int i = 0;i <= r + 1;i++){\n        for(int j = 0;j <= c + 1;j++){\n            height[i][j] = 0x3f3f3f3f;\n            mem[i][j] = 0;\n        }\n    }\n    for(int i = 1;i <= r;i++){\n        for(int j = 1;j <= c;j++){\n            cin>>height[i][j];\n        }\n    }\n    for(int i = 1;i <= r;i++){\n        for(int j = 1;j <= c;j++){\n            int h = dfs(i,j);\n            if(h > ans){\n                ans = h;\n            }\n        }\n    }\n    cout<<ans<<endl;\n    return 0;\n}\n```\n","slug":"acm-qingdi-md","published":1,"updated":"2021-11-07T11:26:41.795Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckvp5hm0z0000a9oecjff2kwp","content":"<h2 id=\"Problem\"><a href=\"#Problem\" class=\"headerlink\" title=\"Problem\"></a>Problem</h2><p>顺治喜欢滑雪，这并不奇怪， 因为滑雪的确很刺激。可是为了获得速度，滑的区域必须向下倾斜，而且当你滑到坡底，你不得不再次走上坡或者等待太监们来载你。顺治想知道载一个区域中最长的滑坡。</p>\n<p>　　区域由一个二维数组给出。数组的每个数字代表点的高度。下面是一个例子：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">　　 1 2 3 4 5</span><br><span class=\"line\">　　16 17 18 19 6</span><br><span class=\"line\">　　15 24 25 20 7</span><br><span class=\"line\">　　14 23 22 21 8</span><br><span class=\"line\">　　13 12 11 10 9</span><br></pre></td></tr></table></figure>\n\n<p>　　顺治可以从某个点滑向上下左右相邻四个点之一，当且仅当高度减小。在上面的例子中，一条可滑行的滑坡为24-17-16-1。当然25-24-23-…-3-2-1更长。事实上，这是最长的一条。</p>\n<h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><p>输入的第一行表示区域的行数 R 和列数 C (1≤R,C≤500) 。下面是 R 行，每行有 C 个整数，代表高度 h,0≤h&lt;103 。</p>\n<h2 id=\"Output\"><a href=\"#Output\" class=\"headerlink\" title=\"Output\"></a>Output</h2><p>输出最长区域的长度。</p>\n<h2 id=\"EX\"><a href=\"#EX\" class=\"headerlink\" title=\"EX\"></a>EX</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">5 5</span><br><span class=\"line\">1 2 3 4 5</span><br><span class=\"line\">16 17 18 19 6</span><br><span class=\"line\">15 24 25 20 7</span><br><span class=\"line\">14 23 22 21 8</span><br><span class=\"line\">13 12 11 10 9</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">25</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>很典型的深搜题，rc并不算大，O(n3)不会爆，所以直接dfs每个点的最长序列。注意要记忆化每个点的长度（在dfs内记忆化效果更优，因为能保存下更多点的mem值）。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> ans = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> r,c;</span><br><span class=\"line\"><span class=\"keyword\">int</span> height[<span class=\"number\">502</span>][<span class=\"number\">502</span>];</span><br><span class=\"line\"><span class=\"keyword\">int</span> mem[<span class=\"number\">502</span>][<span class=\"number\">502</span>] = &#123;<span class=\"number\">0</span>&#125;;</span><br><span class=\"line\"><span class=\"keyword\">int</span> changex[<span class=\"number\">4</span>] = &#123;<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>&#125;;</span><br><span class=\"line\"><span class=\"keyword\">int</span> changey[<span class=\"number\">4</span>] = &#123;<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">-1</span>, <span class=\"number\">1</span>&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">dfs</span><span class=\"params\">(<span class=\"keyword\">int</span> x, <span class=\"keyword\">int</span> y)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(mem[x][y])&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mem[x][y];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> maxx = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; <span class=\"number\">4</span>;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> xx = x + changex[i];</span><br><span class=\"line\">        <span class=\"keyword\">int</span> yy = y + changey[i];</span><br><span class=\"line\">        <span class=\"keyword\">int</span> hh = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(height[xx][yy] &lt; height[x][y])&#123;</span><br><span class=\"line\">            hh = <span class=\"built_in\">dfs</span>(xx,yy);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        hh++;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(hh &gt; maxx)&#123;</span><br><span class=\"line\">            maxx = hh;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    mem[x][y] = maxx;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> maxx;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cin&gt;&gt;r&gt;&gt;c;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt;= r + <span class=\"number\">1</span>;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>;j &lt;= c + <span class=\"number\">1</span>;j++)&#123;</span><br><span class=\"line\">            height[i][j] = <span class=\"number\">0x3f3f3f3f</span>;</span><br><span class=\"line\">            mem[i][j] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>;i &lt;= r;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">1</span>;j &lt;= c;j++)&#123;</span><br><span class=\"line\">            cin&gt;&gt;height[i][j];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>;i &lt;= r;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">1</span>;j &lt;= c;j++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> h = <span class=\"built_in\">dfs</span>(i,j);</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(h &gt; ans)&#123;</span><br><span class=\"line\">                ans = h;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Problem\"><a href=\"#Problem\" class=\"headerlink\" title=\"Problem\"></a>Problem</h2><p>顺治喜欢滑雪，这并不奇怪， 因为滑雪的确很刺激。可是为了获得速度，滑的区域必须向下倾斜，而且当你滑到坡底，你不得不再次走上坡或者等待太监们来载你。顺治想知道载一个区域中最长的滑坡。</p>\n<p>　　区域由一个二维数组给出。数组的每个数字代表点的高度。下面是一个例子：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">　　 1 2 3 4 5</span><br><span class=\"line\">　　16 17 18 19 6</span><br><span class=\"line\">　　15 24 25 20 7</span><br><span class=\"line\">　　14 23 22 21 8</span><br><span class=\"line\">　　13 12 11 10 9</span><br></pre></td></tr></table></figure>\n\n<p>　　顺治可以从某个点滑向上下左右相邻四个点之一，当且仅当高度减小。在上面的例子中，一条可滑行的滑坡为24-17-16-1。当然25-24-23-…-3-2-1更长。事实上，这是最长的一条。</p>\n<h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><p>输入的第一行表示区域的行数 R 和列数 C (1≤R,C≤500) 。下面是 R 行，每行有 C 个整数，代表高度 h,0≤h&lt;103 。</p>\n<h2 id=\"Output\"><a href=\"#Output\" class=\"headerlink\" title=\"Output\"></a>Output</h2><p>输出最长区域的长度。</p>\n<h2 id=\"EX\"><a href=\"#EX\" class=\"headerlink\" title=\"EX\"></a>EX</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">5 5</span><br><span class=\"line\">1 2 3 4 5</span><br><span class=\"line\">16 17 18 19 6</span><br><span class=\"line\">15 24 25 20 7</span><br><span class=\"line\">14 23 22 21 8</span><br><span class=\"line\">13 12 11 10 9</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">25</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>很典型的深搜题，rc并不算大，O(n3)不会爆，所以直接dfs每个点的最长序列。注意要记忆化每个点的长度（在dfs内记忆化效果更优，因为能保存下更多点的mem值）。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> ans = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> r,c;</span><br><span class=\"line\"><span class=\"keyword\">int</span> height[<span class=\"number\">502</span>][<span class=\"number\">502</span>];</span><br><span class=\"line\"><span class=\"keyword\">int</span> mem[<span class=\"number\">502</span>][<span class=\"number\">502</span>] = &#123;<span class=\"number\">0</span>&#125;;</span><br><span class=\"line\"><span class=\"keyword\">int</span> changex[<span class=\"number\">4</span>] = &#123;<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>&#125;;</span><br><span class=\"line\"><span class=\"keyword\">int</span> changey[<span class=\"number\">4</span>] = &#123;<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">-1</span>, <span class=\"number\">1</span>&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">dfs</span><span class=\"params\">(<span class=\"keyword\">int</span> x, <span class=\"keyword\">int</span> y)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(mem[x][y])&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mem[x][y];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> maxx = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; <span class=\"number\">4</span>;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> xx = x + changex[i];</span><br><span class=\"line\">        <span class=\"keyword\">int</span> yy = y + changey[i];</span><br><span class=\"line\">        <span class=\"keyword\">int</span> hh = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(height[xx][yy] &lt; height[x][y])&#123;</span><br><span class=\"line\">            hh = <span class=\"built_in\">dfs</span>(xx,yy);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        hh++;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(hh &gt; maxx)&#123;</span><br><span class=\"line\">            maxx = hh;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    mem[x][y] = maxx;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> maxx;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cin&gt;&gt;r&gt;&gt;c;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt;= r + <span class=\"number\">1</span>;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>;j &lt;= c + <span class=\"number\">1</span>;j++)&#123;</span><br><span class=\"line\">            height[i][j] = <span class=\"number\">0x3f3f3f3f</span>;</span><br><span class=\"line\">            mem[i][j] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>;i &lt;= r;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">1</span>;j &lt;= c;j++)&#123;</span><br><span class=\"line\">            cin&gt;&gt;height[i][j];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>;i &lt;= r;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">1</span>;j &lt;= c;j++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> h = <span class=\"built_in\">dfs</span>(i,j);</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(h &gt; ans)&#123;</span><br><span class=\"line\">                ans = h;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n"},{"title":"集合划分","date":"2021-11-07T12:39:42.000Z","_content":"## Problem\nn个元素的集合{1,2,..., n }可以划分为若干个非空子集。例如，当n=4 时，集合{1，2，3，4}可以划分为15 个不同的非空子集如下：\n```\n{{1}，{2}，{3}，{4}}，\n\n{{1，2}，{3}，{4}}，\n\n{{1，3}，{2}，{4}}，\n\n{{1，4}，{2}，{3}}，\n\n{{2，3}，{1}，{4}}，\n\n{{2，4}，{1}，{3}}，\n\n{{3，4}，{1}，{2}}，\n\n{{1，2}，{3，4}}，\n\n{{1，3}，{2，4}}，\n\n{{1，4}，{2，3}}，\n\n{{1，2，3}，{4}}，\n\n{{1，2，4}，{3}}，\n\n{{1，3，4}，{2}}，\n\n{{2，3，4}，{1}}，\n\n{{1，2，3，4}}\n```\n给定正整数n，计算出n 个元素的集合{1,2,..., n }可以划分为多少个不同的非空子集。\n\n## Input\n多组输入(<=10组数据，读入以EOF结尾) 每组一行输入一个数字，n(0< n <=18)\n\n## Output\n每组输出一行结果。\n\n## EX\n```\n4\n```\n\n```\n15\n```\n\n## 思路\n这道题一开始从整体上分析无从下手，但是不难体会到它是存在递推关系的，复杂问题可以将其分解为一步一步的小问题。因此可以从划分为一组开始讨论，每增加一个分组后划情况数会经历怎样的改变？（这个改变必须于上一个状态的划分数相关才可构成递推关系），\n\n可以发现一种加法是将其中一个元素拿出，将该元素加入到n-1个元素的分组中，但是这样少考虑了该元素单独在一个组的情况，因此还需要加上这种情况的个数，也就是n-1个元素划分为m-1个set的个数。这样就构成了递推关系，可以递推到元素越来越少的情况，直到return1。\n\n## Code\n```cpp\n#include<iostream>\n#include<cstdio>\nusing namespace std;\n\nint a[20];\n\nint f(int i, int n){\n    if(i == 1 || i >= n){\n        return 1;\n    }\n    else{\n        return f(i, n - 1)*i + f(i - 1, n - 1);\n    }\n}\n\nint main(){\n    int n;\n    while(cin>>n){\n        int ans = 0;\n        for(int i = 1;i <= n;i++) { //对划分块数遍历\n            ans += f(i, n);\n        }\n        cout<<ans<<endl;\n    }\n    return 0;\n}\n```","source":"_posts/acm-jihehuafen-md.md","raw":"---\ntitle: 集合划分\ndate: 2021-11-07 20:39:42\ntags: ACM\n---\n## Problem\nn个元素的集合{1,2,..., n }可以划分为若干个非空子集。例如，当n=4 时，集合{1，2，3，4}可以划分为15 个不同的非空子集如下：\n```\n{{1}，{2}，{3}，{4}}，\n\n{{1，2}，{3}，{4}}，\n\n{{1，3}，{2}，{4}}，\n\n{{1，4}，{2}，{3}}，\n\n{{2，3}，{1}，{4}}，\n\n{{2，4}，{1}，{3}}，\n\n{{3，4}，{1}，{2}}，\n\n{{1，2}，{3，4}}，\n\n{{1，3}，{2，4}}，\n\n{{1，4}，{2，3}}，\n\n{{1，2，3}，{4}}，\n\n{{1，2，4}，{3}}，\n\n{{1，3，4}，{2}}，\n\n{{2，3，4}，{1}}，\n\n{{1，2，3，4}}\n```\n给定正整数n，计算出n 个元素的集合{1,2,..., n }可以划分为多少个不同的非空子集。\n\n## Input\n多组输入(<=10组数据，读入以EOF结尾) 每组一行输入一个数字，n(0< n <=18)\n\n## Output\n每组输出一行结果。\n\n## EX\n```\n4\n```\n\n```\n15\n```\n\n## 思路\n这道题一开始从整体上分析无从下手，但是不难体会到它是存在递推关系的，复杂问题可以将其分解为一步一步的小问题。因此可以从划分为一组开始讨论，每增加一个分组后划情况数会经历怎样的改变？（这个改变必须于上一个状态的划分数相关才可构成递推关系），\n\n可以发现一种加法是将其中一个元素拿出，将该元素加入到n-1个元素的分组中，但是这样少考虑了该元素单独在一个组的情况，因此还需要加上这种情况的个数，也就是n-1个元素划分为m-1个set的个数。这样就构成了递推关系，可以递推到元素越来越少的情况，直到return1。\n\n## Code\n```cpp\n#include<iostream>\n#include<cstdio>\nusing namespace std;\n\nint a[20];\n\nint f(int i, int n){\n    if(i == 1 || i >= n){\n        return 1;\n    }\n    else{\n        return f(i, n - 1)*i + f(i - 1, n - 1);\n    }\n}\n\nint main(){\n    int n;\n    while(cin>>n){\n        int ans = 0;\n        for(int i = 1;i <= n;i++) { //对划分块数遍历\n            ans += f(i, n);\n        }\n        cout<<ans<<endl;\n    }\n    return 0;\n}\n```","slug":"acm-jihehuafen-md","published":1,"updated":"2021-11-07T12:57:33.062Z","_id":"ckvp8ovv100003coeel4dgcv8","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Problem\"><a href=\"#Problem\" class=\"headerlink\" title=\"Problem\"></a>Problem</h2><p>n个元素的集合{1,2,…, n }可以划分为若干个非空子集。例如，当n=4 时，集合{1，2，3，4}可以划分为15 个不同的非空子集如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;&#123;1&#125;，&#123;2&#125;，&#123;3&#125;，&#123;4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，2&#125;，&#123;3&#125;，&#123;4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，3&#125;，&#123;2&#125;，&#123;4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，4&#125;，&#123;2&#125;，&#123;3&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;2，3&#125;，&#123;1&#125;，&#123;4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;2，4&#125;，&#123;1&#125;，&#123;3&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;3，4&#125;，&#123;1&#125;，&#123;2&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，2&#125;，&#123;3，4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，3&#125;，&#123;2，4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，4&#125;，&#123;2，3&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，2，3&#125;，&#123;4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，2，4&#125;，&#123;3&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，3，4&#125;，&#123;2&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;2，3，4&#125;，&#123;1&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，2，3，4&#125;&#125;</span><br></pre></td></tr></table></figure>\n<p>给定正整数n，计算出n 个元素的集合{1,2,…, n }可以划分为多少个不同的非空子集。</p>\n<h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><p>多组输入(&lt;=10组数据，读入以EOF结尾) 每组一行输入一个数字，n(0&lt; n &lt;=18)</p>\n<h2 id=\"Output\"><a href=\"#Output\" class=\"headerlink\" title=\"Output\"></a>Output</h2><p>每组输出一行结果。</p>\n<h2 id=\"EX\"><a href=\"#EX\" class=\"headerlink\" title=\"EX\"></a>EX</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">15</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>这道题一开始从整体上分析无从下手，但是不难体会到它是存在递推关系的，复杂问题可以将其分解为一步一步的小问题。因此可以从划分为一组开始讨论，每增加一个分组后划情况数会经历怎样的改变？（这个改变必须于上一个状态的划分数相关才可构成递推关系），</p>\n<p>可以发现一种加法是将其中一个元素拿出，将该元素加入到n-1个元素的分组中，但是这样少考虑了该元素单独在一个组的情况，因此还需要加上这种情况的个数，也就是n-1个元素划分为m-1个set的个数。这样就构成了递推关系，可以递推到元素越来越少的情况，直到return1。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> a[<span class=\"number\">20</span>];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">f</span><span class=\"params\">(<span class=\"keyword\">int</span> i, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i == <span class=\"number\">1</span> || i &gt;= n)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">f</span>(i, n - <span class=\"number\">1</span>)*i + <span class=\"built_in\">f</span>(i - <span class=\"number\">1</span>, n - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> n;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cin&gt;&gt;n)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> ans = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>;i &lt;= n;i++) &#123; <span class=\"comment\">//对划分块数遍历</span></span><br><span class=\"line\">            ans += <span class=\"built_in\">f</span>(i, n);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Problem\"><a href=\"#Problem\" class=\"headerlink\" title=\"Problem\"></a>Problem</h2><p>n个元素的集合{1,2,…, n }可以划分为若干个非空子集。例如，当n=4 时，集合{1，2，3，4}可以划分为15 个不同的非空子集如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;&#123;1&#125;，&#123;2&#125;，&#123;3&#125;，&#123;4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，2&#125;，&#123;3&#125;，&#123;4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，3&#125;，&#123;2&#125;，&#123;4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，4&#125;，&#123;2&#125;，&#123;3&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;2，3&#125;，&#123;1&#125;，&#123;4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;2，4&#125;，&#123;1&#125;，&#123;3&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;3，4&#125;，&#123;1&#125;，&#123;2&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，2&#125;，&#123;3，4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，3&#125;，&#123;2，4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，4&#125;，&#123;2，3&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，2，3&#125;，&#123;4&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，2，4&#125;，&#123;3&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，3，4&#125;，&#123;2&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;2，3，4&#125;，&#123;1&#125;&#125;，</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&#123;1，2，3，4&#125;&#125;</span><br></pre></td></tr></table></figure>\n<p>给定正整数n，计算出n 个元素的集合{1,2,…, n }可以划分为多少个不同的非空子集。</p>\n<h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><p>多组输入(&lt;=10组数据，读入以EOF结尾) 每组一行输入一个数字，n(0&lt; n &lt;=18)</p>\n<h2 id=\"Output\"><a href=\"#Output\" class=\"headerlink\" title=\"Output\"></a>Output</h2><p>每组输出一行结果。</p>\n<h2 id=\"EX\"><a href=\"#EX\" class=\"headerlink\" title=\"EX\"></a>EX</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">15</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>这道题一开始从整体上分析无从下手，但是不难体会到它是存在递推关系的，复杂问题可以将其分解为一步一步的小问题。因此可以从划分为一组开始讨论，每增加一个分组后划情况数会经历怎样的改变？（这个改变必须于上一个状态的划分数相关才可构成递推关系），</p>\n<p>可以发现一种加法是将其中一个元素拿出，将该元素加入到n-1个元素的分组中，但是这样少考虑了该元素单独在一个组的情况，因此还需要加上这种情况的个数，也就是n-1个元素划分为m-1个set的个数。这样就构成了递推关系，可以递推到元素越来越少的情况，直到return1。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> a[<span class=\"number\">20</span>];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">f</span><span class=\"params\">(<span class=\"keyword\">int</span> i, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i == <span class=\"number\">1</span> || i &gt;= n)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">f</span>(i, n - <span class=\"number\">1</span>)*i + <span class=\"built_in\">f</span>(i - <span class=\"number\">1</span>, n - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> n;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cin&gt;&gt;n)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> ans = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>;i &lt;= n;i++) &#123; <span class=\"comment\">//对划分块数遍历</span></span><br><span class=\"line\">            ans += <span class=\"built_in\">f</span>(i, n);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"三层感知机-step by step","date":"2021-11-09T15:12:33.000Z","_content":"\n## 实现内容：\n1. 实现一个三层感知机\n2. 对手写数字数据集进行分类\n3. 绘制损失值变化曲线\n4. 完成kaggle MNIST手写数字分类任务，根据给定的超参数训练模型，完成表格的填写\n\n## 实现\n\n数据集使用手写数字集。并且40%作测试集，60%做训练集。\n\n\n```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom time import time\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(load_digits()['data'], load_digits()['target'], test_size = 0.4, random_state = 32)\n```\n\n接下来是数据预处理，神经网络的训练方法一般是基于梯度的优化算法，如梯度下降，为了让这类算法能更好的优化神经网络，我们往往需要对数据集进行归一化，这里我们选择对数据进行标准化。\n\n减去均值可以让数据以0为中心，除以标准差可以让数据缩放到一个较小的范围内。这样可以使得梯度的下降方向更多样，同时缩小梯度的数量级，让学习变得稳定。  \n\n首先需要对训练集进行标准化，针对每个特征求出其均值和标准差，然后用训练集的每个样本减去均值除以标准差，就得到了新的训练集。然后用测试集的每个样本，减去训练集的均值，除以训练集的标准差，完成对测试集的标准化。\n```py\ntrainY_mat = np.zeros((len(trainY), 10))\ntrainY_mat[np.arange(0, len(trainY), 1), trainY] = 1\n\ntestY_mat = np.zeros((len(testY), 10))\ntestY_mat[np.arange(0, len(testY), 1), testY] = 1\n```\n下面是参数的初始化。\n```py\ndef initialize(h, K):\n    '''\n    参数初始化\n    \n    Parameters\n    ----------\n    h: int: 隐藏层单元个数\n    \n    K: int: 输出层单元个数\n    \n    Returns\n    ----------\n    parameters: dict，参数，键是\"W1\", \"b1\", \"W2\", \"b2\"\n    \n    '''\n    np.random.seed(32)\n    W_1 = np.random.normal(size = (trainX.shape[1], h)) * 0.01\n    b_1 = np.zeros((1, h))\n    \n    np.random.seed(32)\n    W_2 = np.random.normal(size = (h, K)) * 0.01\n    b_2 = np.zeros((1, K))\n    \n    parameters = {'W1': W_1, 'b1': b_1, 'W2': W_2, 'b2': b_2}\n    \n    return parameters\n```\n向前传播，这里具体指的就是依据公式向前计算值。\n\n这里有一点要注意，矩阵的点乘是使用```np.dot()```进行的，否则py会默认为元素乘。\n```py\ndef linear_combination(X, W, b):\n    '''\n    计算Z，Z = XW + b\n    \n    Parameters\n    ----------\n    X: np.ndarray, shape = (n, m)，输入的数据\n    \n    W: np.ndarray, shape = (m, h)，权重\n    \n    b: np.ndarray, shape = (1, h)，偏置\n    \n    Returns\n    ----------\n    Z: np.ndarray, shape = (n, h)，线性组合后的值\n    \n    '''\n    \n    # Z = XW + b\n    # YOUR CODE HERE\n    Z = np.dot(X,W) + b\n    \n    return Z\n```\n每一线性层的输出都要经过一个activate，隐藏层的activate为ReLu。\n```py\ndef ReLU(X):\n    '''\n    ReLU激活函数\n    \n    Parameters\n    ----------\n    X: np.ndarray，待激活的矩阵\n    \n    Returns\n    ----------\n    activations: np.ndarray, 激活后的矩阵\n    \n    '''\n    \n    # YOUR CODE HERE\n    X[X < 0] = 0\n    activations = X\n    \n    return activations\n```\n输出层要经过softmax找到每一个label的概率大小。这里值得注意的是，O矩阵的求和是对每一行的各个元素求和，而不是对所有元素求和，所以要有```axis=1```，对行进行sum操作，并保持维度。\n\n前一个```my_softmax(O)```会导致对于较小值的output，会导致分母为0的情况，所以要对其进行一些处理，让O的每一个元素减去该行的最大值，这样能保证取exp后至少一个元素为1，所以不会出现NaN的情况。\n```py\ndef my_softmax(O):\n    '''\n    softmax激活\n    '''\n    # YOUR CODE HERE\n    return np.exp(O) / np.sum(np.exp(O), axis = 1, keepdims = True)\n\ndef softmax(O):\n    '''\n    softmax激活函数\n    \n    Parameters\n    ----------\n    O: np.ndarray，待激活的矩阵\n    \n    Returns\n    ----------\n    activations: np.ndarray, 激活后的矩阵\n    \n    '''\n    \n    # YOUR CODE HEER\n    O = O - np.max(O, axis=1, keepdims=True)\n    activations = my_softmax(O)\n    return activations\n```\n接下来是实现损失函数，交叉熵损失函数：\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathrm{loss}&space;=&space;-&space;\\frac{1}{n}&space;\\sum_n&space;\\sum^{K}_{k=1}&space;y_k&space;\\log{(\\hat{y_k})}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathrm{loss}&space;=&space;-&space;\\frac{1}{n}&space;\\sum_n&space;\\sum^{K}_{k=1}&space;y_k&space;\\log{(\\hat{y_k})}\" title=\"\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}\" /></a>\n这里又会出一个问题，交叉熵损失函数中，我们需要对softmax的激活值取对数，也就是log\\haty，这就要求我们的激活值全都是大于0的数，不能等于0，但是我们实现的softmax在有些时候确实会输出0。这就使得在计算loss的时候会出现问题，解决这个问题的方法是log softmax。所谓log softmax，就是将交叉熵中的对数运算与softmax结合起来，避开为0的情况。\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\log{\\frac{\\exp{(O_i)}}{\\sum_K&space;\\exp{(O_k)}}}&space;&=&space;\\log{\\frac{\\exp{(O_i&space;-&space;\\mathrm{max}(O))}}{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}}\\\\&space;&=&space;O_i&space;-&space;\\mathrm{max}(O)&space;-&space;\\log{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\log{\\frac{\\exp{(O_i)}}{\\sum_K&space;\\exp{(O_k)}}}&space;&=&space;\\log{\\frac{\\exp{(O_i&space;-&space;\\mathrm{max}(O))}}{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}}\\\\&space;&=&space;O_i&space;-&space;\\mathrm{max}(O)&space;-&space;\\log{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}&space;\\end{aligned}\" title=\"\\begin{aligned} \\log{\\frac{\\exp{(O_i)}}{\\sum_K \\exp{(O_k)}}} &= \\log{\\frac{\\exp{(O_i - \\mathrm{max}(O))}}{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}}}\\\\ &= O_i - \\mathrm{max}(O) - \\log{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}} \\end{aligned}\" /></a>\n\n这样我们再计算loss的时候就可以把输出层的输出直接放到log softmax中计算，不用先激活，再取对数了。\n```py\ndef log_softmax(x):\n    '''\n    log softmax\n    \n    Parameters\n    ----------\n    x: np.ndarray，待激活的矩阵\n    \n    Returns\n    ----------\n    log_activations: np.ndarray, 激活后取了对数的矩阵\n    \n    '''\n    # YOUR CODE HERE\n    log_activations = x - np.max(x) - np.log( np.sum(np.exp(x - np.max(x)), axis = 1, keepdims = True) )\n    \n    return log_activations\n```\n然后编写`cross_entropy_with_softmax`。函数内容不再赘述。\n```py\ndef cross_entropy_with_softmax(y_true, O):\n    '''\n    求解交叉熵损失函数，这里需要使用log softmax，所以参数分别是真值和未经softmax激活的输出值\n\n    Parameters\n    ----------\n    y_true: np.ndarray，shape = (n, K), 真值\n    \n    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值\n    \n    Returns\n    ----------\n    loss: float, 平均的交叉熵损失值\n    \n    '''\n    \n    # 平均交叉熵损失\n    # YOUR CODE HERE\n    loss = - 1/len(y_true) * np.sum(np.sum(y_true * log_softmax(O)))    # 这里是元素乘\n    \n    return loss\n```\n正是因为softmax激活与交叉熵损失会有这样的问题，所以在很多深度学习框架中，交叉熵损失函数就直接带有了激活的功能，所以我们在实现前向传播计算的时候，就不要加softmax激活函数了。\n```py\ndef forward(X, parameters):\n    '''\n    前向传播，从输入一直到输出层softmax激活前的值\n    \n    Parameters\n    ----------\n    X: np.ndarray, shape = (n, m)，输入的数据\n    \n    parameters: dict，参数\n    \n    Returns\n    ----------\n    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值\n    \n    '''\n    # 输入层到隐藏层\n    # YOUR CODE HERE\n    Z = np.dot(X, parameters['W1']) + parameters['b1']\n    \n    # 隐藏层的激活\n    # YOUR CODE HERE\n    H = ReLU(Z)\n    \n    # 隐藏层到输出层\n    # YOUR CODE HERE\n    O = np.dot(H, parameters['W2']) + parameters['b2']\n\n    return O\n```\n下面是反向传播，也是本篇blog的重点。首先是偏导的推导，细节不再赘述，使用链式求导法则认真推导即可。\n\nforward公式：最后一层的输出，使用softmax函数激活，得到神经网络计算出的各类的概率值。\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=Z&space;=&space;XW_1&space;&plus;&space;b_1\\\\&space;H_1&space;=&space;\\mathrm{ReLU}(Z)\\\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?Z&space;=&space;XW_1&space;&plus;&space;b_1\\\\&space;H_1&space;=&space;\\mathrm{ReLU}(Z)\\\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2\" title=\"Z = XW_1 + b_1\\\\ H_1 = \\mathrm{ReLU}(Z)\\\\ O = H_1 W_2 + b_2\" /></a>\n\n损失函数对参数W_2和b_2的偏导数：\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;[{H_1}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)]&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;[{H_1}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)]&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial W_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{1}{n} [{H_1}^\\mathrm{T} (\\hat{y} - y)] \\end{aligned}\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;\\sum^n_{i=1}&space;(\\hat{y_i}&space;-&space;y_i)&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;\\sum^n_{i=1}&space;(\\hat{y_i}&space;-&space;y_i)&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial b_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{1}{n} \\sum^n_{i=1} (\\hat{y_i} - y_i) \\end{aligned}\" /></a>\n\n求得loss对W_1和b_1的偏导数：\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;[(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}]\\\\&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;[(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}]\\\\&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial W_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\ & = \\frac{1}{n} {X}^\\mathrm{T} [(\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}]\\\\ \\end{aligned}\" /></a>\n\nReLu的偏导数：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{\\partial&space;\\mathrm{ReLU(x)}}{\\partial&space;x}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;x&space;<&space;0\\\\&space;1&space;&&space;\\text{if&space;}&space;x&space;\\geq&space;0&space;\\end{cases}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial&space;\\mathrm{ReLU(x)}}{\\partial&space;x}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;x&space;<&space;0\\\\&space;1&space;&&space;\\text{if&space;}&space;x&space;\\geq&space;0&space;\\end{cases}\" title=\"\\frac{\\partial \\mathrm{ReLU(x)}}{\\partial x} = \\begin{cases} 0 & \\text{if } x < 0\\\\ 1 & \\text{if } x \\geq 0 \\end{cases}\" /></a>\n\n从而：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{\\partial&space;loss}{\\partial&space;{W_1}_{ij}}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial&space;loss}{\\partial&space;{W_1}_{ij}}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}\" title=\"\\frac{\\partial loss}{\\partial {W_1}_{ij}} = \\begin{cases} 0 & \\text{if } {Z}_{ij} < 0\\\\ \\frac{1}{n} {X}^\\mathrm{T} (\\hat{y} - y) {W_2}^\\mathrm{T} & \\text{if } {Z}_{ij} \\geq 0 \\end{cases}\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}\\\\&space;&&space;=&space;\\begin{cases}&space;0&space;&\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;\\sum_n&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}\\\\&space;&&space;=&space;\\begin{cases}&space;0&space;&\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;\\sum_n&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial b_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\ & = \\frac{1}{n} (\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}\\\\ & = \\begin{cases} 0 &\\text{if } {Z}_{ij} < 0\\\\ \\frac{1}{n} \\sum_n (\\hat{y} - y) {W_2}^\\mathrm{T} &\\text{if } {Z}_{ij} \\geq 0 \\end{cases} \\end{aligned}\" /></a>\n\n描述完公式后下面来用代码实现，首先dW2和db2的代码是很显然的。对于dW1，这里涉及到的ReLu的偏导数，很显然如果hidden层的值小于零对应ReLu为0时，定义其偏导为0，那么如何确定dW1中的那些值是由该定义得到的呢。如果我们眼光狭窄只分析dW2公式的最后结果必然很难分析出来，因为最终的dW2是(hidden, output)维度的，而relu_regard是(n, hidden)维度的，直接对它们进行关联显然不现实。那么需要追根溯源，深入了解这个dW2的来由。\n\n在dW2分段函数的前一步，它的结果是XT点积后面的一堆，其中H对Z的偏导其实就是ReLu的偏导，是在这里决定了dW2的值，再来分析一下维度1/n可以broadcast不用管，后面是```(n, input)T · [(n, output) · (hidden, output)T * (n, hidden)]```这样的维度关系。这里尤其要注意最后一个运算，我一开始卡在这里好久，因为这里涉及到了元素乘，```(n, hidden) * (n, hidden)```，这里决定了哪个计算位置的值来自于ReLu的0，元素乘后再与X的转置计算。\n\ndb2同理。\n\n```py\ndef compute_gradient(y_true, y_pred, H, Z, X, parameters):\n    '''\n    计算梯度\n    \n    Parameters\n    ----------\n    y_true: np.ndarray，shape = (n, K), 真值\n    \n    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值\n    \n    H: np.ndarray, shape = (n, h)，隐藏层激活后的值\n    \n    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值\n    \n    X: np.ndarray, shape = (n, m)，输入的原始数据\n    \n    parameters: dict，参数\n    \n    Returns\n    ----------\n    grads: dict, 梯度\n    \n    '''\n    \n    # 计算W2的梯度\n    # YOUR CODE HERE\n    dW2 = (1/len(y_true)) * np.dot(np.transpose(H), (y_pred - y_true))\n    \n    # 计算b2的梯度\n    # YOUR CODE HERE\n    db2 = (1/len(y_true)) * np.sum(y_pred - y_true, axis=0)\n    \n    # 计算ReLU的梯度\n    relu_grad = Z.copy()\n    relu_grad[relu_grad >= 0] = 1\n    relu_grad[relu_grad < 0] = 0\n    \n    # 计算W1的梯度\n    # YOUR CODE HERE\n    dW1 = 1/len(y_true) * np.dot(np.transpose(X), np.dot((y_pred - y_true), np.transpose(parameters['W2'])) * relu_grad  )\n    # 计算b1的梯度\n    # YOUR CODE HERE\n    db1 = 1/len(y_true) * np.sum(np.dot((y_pred - y_true), np.transpose(parameters['W2'])) * relu_grad, axis=0)\n    \n    grads = {'dW2': dW2, 'db2': db2, 'dW1': dW1, 'db1': db1}\n    \n    return grads\n```\n梯度下降，反向传播，参数更新。\n```py\ndef update(parameters, grads, learning_rate):\n    '''\n    参数更新\n    \n    Parameters\n    ----------\n    parameters: dict，参数\n    \n    grads: dict, 梯度\n    \n    learning_rate: float, 学习率\n    \n    '''\n    parameters['W2'] -= learning_rate * grads['dW2']\n    parameters['b2'] -= learning_rate * grads['db2']\n    parameters['W1'] -= learning_rate * grads['dW1']\n    parameters['b1'] -= learning_rate * grads['db1']\n```\n```py\ndef backward(y_true, y_pred, H, Z, X, parameters, learning_rate):\n    '''\n    计算梯度，参数更新\n    \n    Parameters\n    ----------\n    y_true: np.ndarray，shape = (n, K), 真值\n    \n    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值\n    \n    H: np.ndarray, shape = (n, h)，隐藏层激活后的值\n    \n    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值\n    \n    X: np.ndarray, shape = (n, m)，输入的原始数据\n    \n    parameters: dict，参数\n    \n    learning_rate: float, 学习率\n    \n    '''\n    # 计算梯度\n    # YOUR CODE HERE\n    grads = compute_gradient(y_true, y_pred, H, Z, X, parameters)\n    \n    # 更新参数\n    # YOUR CODE HERE\n    update(parameters, grads, learning_rate)\n```\n训练。\n```py\ndef train(trainX, trainY, testX, testY, parameters, epochs, learning_rate = 0.01, verbose = False):\n    '''\n    训练\n    \n    Parameters\n    ----------\n    Parameters\n    ----------\n    trainX: np.ndarray, shape = (n, m), 训练集\n    \n    trainY: np.ndarray, shape = (n, K), 训练集标记\n    \n    testX: np.ndarray, shape = (n_test, m)，测试集\n    \n    testY: np.ndarray, shape = (n_test, K)，测试集的标记\n    \n    parameters: dict，参数\n    \n    epochs: int, 要迭代的轮数\n    \n    learning_rate: float, default 0.01，学习率\n    \n    verbose: boolean, default False，是否打印损失值\n    \n    Returns\n    ----------\n    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n    \n    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n    \n    '''\n    # 存储损失值\n    training_loss_list = []\n    testing_loss_list = []\n    \n    for i in range(epochs):\n        \n        # 这里要计算出Z和H，因为后面反向传播计算梯度的时候需要这两个矩阵\n        Z = linear_combination(trainX, parameters['W1'], parameters['b1'])\n        H = ReLU(Z)\n        train_O = linear_combination(H, parameters['W2'], parameters['b2'])\n        train_y_pred = softmax(train_O)\n        training_loss = cross_entropy_with_softmax(trainY, train_O)\n        \n        test_O = forward(testX, parameters)\n        testing_loss = cross_entropy_with_softmax(testY, test_O)\n        \n        if verbose == True:\n            print('epoch %s, training loss:%s'%(i + 1, training_loss))\n            print('epoch %s, testing loss:%s'%(i + 1, testing_loss))\n            print()\n        \n        training_loss_list.append(training_loss)\n        testing_loss_list.append(testing_loss)\n        \n        backward(trainY, train_y_pred, H, Z, trainX, parameters, learning_rate)\n    return training_loss_list, testing_loss_list\n```\n绘制loss随epoch的变化曲线。\n```py\ndef plot_loss_curve(training_loss_list, testing_loss_list):\n    '''\n    绘制损失值变化曲线\n    \n    Parameters\n    ----------\n    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n    \n    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n    \n    '''\n    plt.figure(figsize = (10, 6))\n    plt.plot(training_loss_list, label = 'training loss')\n    plt.plot(testing_loss_list, label = 'testing loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend()\n```\n预测\n```py\ndef predict(X, parameters):\n    '''\n    预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，取每行最大的那个数的下标作为标记\n    \n    Parameters\n    ----------\n    X: np.ndarray, shape = (n, m), 训练集\n    \n    parameters: dict，参数\n    \n    Returns\n    ----------\n    prediction: np.ndarray, shape = (n, 1)，预测的标记\n    \n    '''\n    # 用forward函数得到softmax激活前的值\n    # YOUR CODE HERE\n    O = forward(X, parameters)\n    \n    # 计算softmax激活后的值\n    # YOUR CODE HERE\n    y_pred = softmax(O)\n    \n    # 取每行最大的元素对应的下标\n    # YOUR CODE HERE\n    prediction = np.argmax(y_pred, axis=1)\n    \n    return prediction\n```\n训练一个不算特别优秀的3-layer-perceptron。\n```py\nfrom sklearn.metrics import accuracy_score\nstart_time = time()\n\nh = 50\nK = 10\nparameters = initialize(h, K)\ntraining_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters, 1000, 0.03, False)\n\nend_time = time()\nprint('training time: %s s'%(end_time - start_time))\nprediction = predict(testX, parameters)\nprint(accuracy_score(prediction, testY))\nplot_loss_curve(training_loss_list, testing_loss_list)\n``` \n到这里就结束了，其实不算复杂，数值计算的细节比较重要，以前经常用pytorch来写BP、RNN之类的，但是很少从底层去实现过，这还是一个简单的感知机模型，较复杂的基础模型涉及到的内容可能更复杂。只能说我企图学会吧。","source":"_posts/3-layer-MLP-md.md","raw":"---\ntitle: 三层感知机-step by step\ndate: 2021-11-09 23:12:33\ntags: 机器学习\n---\n\n## 实现内容：\n1. 实现一个三层感知机\n2. 对手写数字数据集进行分类\n3. 绘制损失值变化曲线\n4. 完成kaggle MNIST手写数字分类任务，根据给定的超参数训练模型，完成表格的填写\n\n## 实现\n\n数据集使用手写数字集。并且40%作测试集，60%做训练集。\n\n\n```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom time import time\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(load_digits()['data'], load_digits()['target'], test_size = 0.4, random_state = 32)\n```\n\n接下来是数据预处理，神经网络的训练方法一般是基于梯度的优化算法，如梯度下降，为了让这类算法能更好的优化神经网络，我们往往需要对数据集进行归一化，这里我们选择对数据进行标准化。\n\n减去均值可以让数据以0为中心，除以标准差可以让数据缩放到一个较小的范围内。这样可以使得梯度的下降方向更多样，同时缩小梯度的数量级，让学习变得稳定。  \n\n首先需要对训练集进行标准化，针对每个特征求出其均值和标准差，然后用训练集的每个样本减去均值除以标准差，就得到了新的训练集。然后用测试集的每个样本，减去训练集的均值，除以训练集的标准差，完成对测试集的标准化。\n```py\ntrainY_mat = np.zeros((len(trainY), 10))\ntrainY_mat[np.arange(0, len(trainY), 1), trainY] = 1\n\ntestY_mat = np.zeros((len(testY), 10))\ntestY_mat[np.arange(0, len(testY), 1), testY] = 1\n```\n下面是参数的初始化。\n```py\ndef initialize(h, K):\n    '''\n    参数初始化\n    \n    Parameters\n    ----------\n    h: int: 隐藏层单元个数\n    \n    K: int: 输出层单元个数\n    \n    Returns\n    ----------\n    parameters: dict，参数，键是\"W1\", \"b1\", \"W2\", \"b2\"\n    \n    '''\n    np.random.seed(32)\n    W_1 = np.random.normal(size = (trainX.shape[1], h)) * 0.01\n    b_1 = np.zeros((1, h))\n    \n    np.random.seed(32)\n    W_2 = np.random.normal(size = (h, K)) * 0.01\n    b_2 = np.zeros((1, K))\n    \n    parameters = {'W1': W_1, 'b1': b_1, 'W2': W_2, 'b2': b_2}\n    \n    return parameters\n```\n向前传播，这里具体指的就是依据公式向前计算值。\n\n这里有一点要注意，矩阵的点乘是使用```np.dot()```进行的，否则py会默认为元素乘。\n```py\ndef linear_combination(X, W, b):\n    '''\n    计算Z，Z = XW + b\n    \n    Parameters\n    ----------\n    X: np.ndarray, shape = (n, m)，输入的数据\n    \n    W: np.ndarray, shape = (m, h)，权重\n    \n    b: np.ndarray, shape = (1, h)，偏置\n    \n    Returns\n    ----------\n    Z: np.ndarray, shape = (n, h)，线性组合后的值\n    \n    '''\n    \n    # Z = XW + b\n    # YOUR CODE HERE\n    Z = np.dot(X,W) + b\n    \n    return Z\n```\n每一线性层的输出都要经过一个activate，隐藏层的activate为ReLu。\n```py\ndef ReLU(X):\n    '''\n    ReLU激活函数\n    \n    Parameters\n    ----------\n    X: np.ndarray，待激活的矩阵\n    \n    Returns\n    ----------\n    activations: np.ndarray, 激活后的矩阵\n    \n    '''\n    \n    # YOUR CODE HERE\n    X[X < 0] = 0\n    activations = X\n    \n    return activations\n```\n输出层要经过softmax找到每一个label的概率大小。这里值得注意的是，O矩阵的求和是对每一行的各个元素求和，而不是对所有元素求和，所以要有```axis=1```，对行进行sum操作，并保持维度。\n\n前一个```my_softmax(O)```会导致对于较小值的output，会导致分母为0的情况，所以要对其进行一些处理，让O的每一个元素减去该行的最大值，这样能保证取exp后至少一个元素为1，所以不会出现NaN的情况。\n```py\ndef my_softmax(O):\n    '''\n    softmax激活\n    '''\n    # YOUR CODE HERE\n    return np.exp(O) / np.sum(np.exp(O), axis = 1, keepdims = True)\n\ndef softmax(O):\n    '''\n    softmax激活函数\n    \n    Parameters\n    ----------\n    O: np.ndarray，待激活的矩阵\n    \n    Returns\n    ----------\n    activations: np.ndarray, 激活后的矩阵\n    \n    '''\n    \n    # YOUR CODE HEER\n    O = O - np.max(O, axis=1, keepdims=True)\n    activations = my_softmax(O)\n    return activations\n```\n接下来是实现损失函数，交叉熵损失函数：\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathrm{loss}&space;=&space;-&space;\\frac{1}{n}&space;\\sum_n&space;\\sum^{K}_{k=1}&space;y_k&space;\\log{(\\hat{y_k})}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathrm{loss}&space;=&space;-&space;\\frac{1}{n}&space;\\sum_n&space;\\sum^{K}_{k=1}&space;y_k&space;\\log{(\\hat{y_k})}\" title=\"\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}\" /></a>\n这里又会出一个问题，交叉熵损失函数中，我们需要对softmax的激活值取对数，也就是log\\haty，这就要求我们的激活值全都是大于0的数，不能等于0，但是我们实现的softmax在有些时候确实会输出0。这就使得在计算loss的时候会出现问题，解决这个问题的方法是log softmax。所谓log softmax，就是将交叉熵中的对数运算与softmax结合起来，避开为0的情况。\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\log{\\frac{\\exp{(O_i)}}{\\sum_K&space;\\exp{(O_k)}}}&space;&=&space;\\log{\\frac{\\exp{(O_i&space;-&space;\\mathrm{max}(O))}}{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}}\\\\&space;&=&space;O_i&space;-&space;\\mathrm{max}(O)&space;-&space;\\log{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\log{\\frac{\\exp{(O_i)}}{\\sum_K&space;\\exp{(O_k)}}}&space;&=&space;\\log{\\frac{\\exp{(O_i&space;-&space;\\mathrm{max}(O))}}{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}}\\\\&space;&=&space;O_i&space;-&space;\\mathrm{max}(O)&space;-&space;\\log{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}&space;\\end{aligned}\" title=\"\\begin{aligned} \\log{\\frac{\\exp{(O_i)}}{\\sum_K \\exp{(O_k)}}} &= \\log{\\frac{\\exp{(O_i - \\mathrm{max}(O))}}{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}}}\\\\ &= O_i - \\mathrm{max}(O) - \\log{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}} \\end{aligned}\" /></a>\n\n这样我们再计算loss的时候就可以把输出层的输出直接放到log softmax中计算，不用先激活，再取对数了。\n```py\ndef log_softmax(x):\n    '''\n    log softmax\n    \n    Parameters\n    ----------\n    x: np.ndarray，待激活的矩阵\n    \n    Returns\n    ----------\n    log_activations: np.ndarray, 激活后取了对数的矩阵\n    \n    '''\n    # YOUR CODE HERE\n    log_activations = x - np.max(x) - np.log( np.sum(np.exp(x - np.max(x)), axis = 1, keepdims = True) )\n    \n    return log_activations\n```\n然后编写`cross_entropy_with_softmax`。函数内容不再赘述。\n```py\ndef cross_entropy_with_softmax(y_true, O):\n    '''\n    求解交叉熵损失函数，这里需要使用log softmax，所以参数分别是真值和未经softmax激活的输出值\n\n    Parameters\n    ----------\n    y_true: np.ndarray，shape = (n, K), 真值\n    \n    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值\n    \n    Returns\n    ----------\n    loss: float, 平均的交叉熵损失值\n    \n    '''\n    \n    # 平均交叉熵损失\n    # YOUR CODE HERE\n    loss = - 1/len(y_true) * np.sum(np.sum(y_true * log_softmax(O)))    # 这里是元素乘\n    \n    return loss\n```\n正是因为softmax激活与交叉熵损失会有这样的问题，所以在很多深度学习框架中，交叉熵损失函数就直接带有了激活的功能，所以我们在实现前向传播计算的时候，就不要加softmax激活函数了。\n```py\ndef forward(X, parameters):\n    '''\n    前向传播，从输入一直到输出层softmax激活前的值\n    \n    Parameters\n    ----------\n    X: np.ndarray, shape = (n, m)，输入的数据\n    \n    parameters: dict，参数\n    \n    Returns\n    ----------\n    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值\n    \n    '''\n    # 输入层到隐藏层\n    # YOUR CODE HERE\n    Z = np.dot(X, parameters['W1']) + parameters['b1']\n    \n    # 隐藏层的激活\n    # YOUR CODE HERE\n    H = ReLU(Z)\n    \n    # 隐藏层到输出层\n    # YOUR CODE HERE\n    O = np.dot(H, parameters['W2']) + parameters['b2']\n\n    return O\n```\n下面是反向传播，也是本篇blog的重点。首先是偏导的推导，细节不再赘述，使用链式求导法则认真推导即可。\n\nforward公式：最后一层的输出，使用softmax函数激活，得到神经网络计算出的各类的概率值。\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=Z&space;=&space;XW_1&space;&plus;&space;b_1\\\\&space;H_1&space;=&space;\\mathrm{ReLU}(Z)\\\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?Z&space;=&space;XW_1&space;&plus;&space;b_1\\\\&space;H_1&space;=&space;\\mathrm{ReLU}(Z)\\\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2\" title=\"Z = XW_1 + b_1\\\\ H_1 = \\mathrm{ReLU}(Z)\\\\ O = H_1 W_2 + b_2\" /></a>\n\n损失函数对参数W_2和b_2的偏导数：\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;[{H_1}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)]&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;[{H_1}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)]&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial W_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{1}{n} [{H_1}^\\mathrm{T} (\\hat{y} - y)] \\end{aligned}\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;\\sum^n_{i=1}&space;(\\hat{y_i}&space;-&space;y_i)&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;\\sum^n_{i=1}&space;(\\hat{y_i}&space;-&space;y_i)&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial b_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{1}{n} \\sum^n_{i=1} (\\hat{y_i} - y_i) \\end{aligned}\" /></a>\n\n求得loss对W_1和b_1的偏导数：\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;[(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}]\\\\&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;[(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}]\\\\&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial W_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\ & = \\frac{1}{n} {X}^\\mathrm{T} [(\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}]\\\\ \\end{aligned}\" /></a>\n\nReLu的偏导数：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{\\partial&space;\\mathrm{ReLU(x)}}{\\partial&space;x}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;x&space;<&space;0\\\\&space;1&space;&&space;\\text{if&space;}&space;x&space;\\geq&space;0&space;\\end{cases}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial&space;\\mathrm{ReLU(x)}}{\\partial&space;x}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;x&space;<&space;0\\\\&space;1&space;&&space;\\text{if&space;}&space;x&space;\\geq&space;0&space;\\end{cases}\" title=\"\\frac{\\partial \\mathrm{ReLU(x)}}{\\partial x} = \\begin{cases} 0 & \\text{if } x < 0\\\\ 1 & \\text{if } x \\geq 0 \\end{cases}\" /></a>\n\n从而：\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{\\partial&space;loss}{\\partial&space;{W_1}_{ij}}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial&space;loss}{\\partial&space;{W_1}_{ij}}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}\" title=\"\\frac{\\partial loss}{\\partial {W_1}_{ij}} = \\begin{cases} 0 & \\text{if } {Z}_{ij} < 0\\\\ \\frac{1}{n} {X}^\\mathrm{T} (\\hat{y} - y) {W_2}^\\mathrm{T} & \\text{if } {Z}_{ij} \\geq 0 \\end{cases}\" /></a>\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}\\\\&space;&&space;=&space;\\begin{cases}&space;0&space;&\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;\\sum_n&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}\\\\&space;&&space;=&space;\\begin{cases}&space;0&space;&\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;\\sum_n&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial b_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\ & = \\frac{1}{n} (\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}\\\\ & = \\begin{cases} 0 &\\text{if } {Z}_{ij} < 0\\\\ \\frac{1}{n} \\sum_n (\\hat{y} - y) {W_2}^\\mathrm{T} &\\text{if } {Z}_{ij} \\geq 0 \\end{cases} \\end{aligned}\" /></a>\n\n描述完公式后下面来用代码实现，首先dW2和db2的代码是很显然的。对于dW1，这里涉及到的ReLu的偏导数，很显然如果hidden层的值小于零对应ReLu为0时，定义其偏导为0，那么如何确定dW1中的那些值是由该定义得到的呢。如果我们眼光狭窄只分析dW2公式的最后结果必然很难分析出来，因为最终的dW2是(hidden, output)维度的，而relu_regard是(n, hidden)维度的，直接对它们进行关联显然不现实。那么需要追根溯源，深入了解这个dW2的来由。\n\n在dW2分段函数的前一步，它的结果是XT点积后面的一堆，其中H对Z的偏导其实就是ReLu的偏导，是在这里决定了dW2的值，再来分析一下维度1/n可以broadcast不用管，后面是```(n, input)T · [(n, output) · (hidden, output)T * (n, hidden)]```这样的维度关系。这里尤其要注意最后一个运算，我一开始卡在这里好久，因为这里涉及到了元素乘，```(n, hidden) * (n, hidden)```，这里决定了哪个计算位置的值来自于ReLu的0，元素乘后再与X的转置计算。\n\ndb2同理。\n\n```py\ndef compute_gradient(y_true, y_pred, H, Z, X, parameters):\n    '''\n    计算梯度\n    \n    Parameters\n    ----------\n    y_true: np.ndarray，shape = (n, K), 真值\n    \n    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值\n    \n    H: np.ndarray, shape = (n, h)，隐藏层激活后的值\n    \n    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值\n    \n    X: np.ndarray, shape = (n, m)，输入的原始数据\n    \n    parameters: dict，参数\n    \n    Returns\n    ----------\n    grads: dict, 梯度\n    \n    '''\n    \n    # 计算W2的梯度\n    # YOUR CODE HERE\n    dW2 = (1/len(y_true)) * np.dot(np.transpose(H), (y_pred - y_true))\n    \n    # 计算b2的梯度\n    # YOUR CODE HERE\n    db2 = (1/len(y_true)) * np.sum(y_pred - y_true, axis=0)\n    \n    # 计算ReLU的梯度\n    relu_grad = Z.copy()\n    relu_grad[relu_grad >= 0] = 1\n    relu_grad[relu_grad < 0] = 0\n    \n    # 计算W1的梯度\n    # YOUR CODE HERE\n    dW1 = 1/len(y_true) * np.dot(np.transpose(X), np.dot((y_pred - y_true), np.transpose(parameters['W2'])) * relu_grad  )\n    # 计算b1的梯度\n    # YOUR CODE HERE\n    db1 = 1/len(y_true) * np.sum(np.dot((y_pred - y_true), np.transpose(parameters['W2'])) * relu_grad, axis=0)\n    \n    grads = {'dW2': dW2, 'db2': db2, 'dW1': dW1, 'db1': db1}\n    \n    return grads\n```\n梯度下降，反向传播，参数更新。\n```py\ndef update(parameters, grads, learning_rate):\n    '''\n    参数更新\n    \n    Parameters\n    ----------\n    parameters: dict，参数\n    \n    grads: dict, 梯度\n    \n    learning_rate: float, 学习率\n    \n    '''\n    parameters['W2'] -= learning_rate * grads['dW2']\n    parameters['b2'] -= learning_rate * grads['db2']\n    parameters['W1'] -= learning_rate * grads['dW1']\n    parameters['b1'] -= learning_rate * grads['db1']\n```\n```py\ndef backward(y_true, y_pred, H, Z, X, parameters, learning_rate):\n    '''\n    计算梯度，参数更新\n    \n    Parameters\n    ----------\n    y_true: np.ndarray，shape = (n, K), 真值\n    \n    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值\n    \n    H: np.ndarray, shape = (n, h)，隐藏层激活后的值\n    \n    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值\n    \n    X: np.ndarray, shape = (n, m)，输入的原始数据\n    \n    parameters: dict，参数\n    \n    learning_rate: float, 学习率\n    \n    '''\n    # 计算梯度\n    # YOUR CODE HERE\n    grads = compute_gradient(y_true, y_pred, H, Z, X, parameters)\n    \n    # 更新参数\n    # YOUR CODE HERE\n    update(parameters, grads, learning_rate)\n```\n训练。\n```py\ndef train(trainX, trainY, testX, testY, parameters, epochs, learning_rate = 0.01, verbose = False):\n    '''\n    训练\n    \n    Parameters\n    ----------\n    Parameters\n    ----------\n    trainX: np.ndarray, shape = (n, m), 训练集\n    \n    trainY: np.ndarray, shape = (n, K), 训练集标记\n    \n    testX: np.ndarray, shape = (n_test, m)，测试集\n    \n    testY: np.ndarray, shape = (n_test, K)，测试集的标记\n    \n    parameters: dict，参数\n    \n    epochs: int, 要迭代的轮数\n    \n    learning_rate: float, default 0.01，学习率\n    \n    verbose: boolean, default False，是否打印损失值\n    \n    Returns\n    ----------\n    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n    \n    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n    \n    '''\n    # 存储损失值\n    training_loss_list = []\n    testing_loss_list = []\n    \n    for i in range(epochs):\n        \n        # 这里要计算出Z和H，因为后面反向传播计算梯度的时候需要这两个矩阵\n        Z = linear_combination(trainX, parameters['W1'], parameters['b1'])\n        H = ReLU(Z)\n        train_O = linear_combination(H, parameters['W2'], parameters['b2'])\n        train_y_pred = softmax(train_O)\n        training_loss = cross_entropy_with_softmax(trainY, train_O)\n        \n        test_O = forward(testX, parameters)\n        testing_loss = cross_entropy_with_softmax(testY, test_O)\n        \n        if verbose == True:\n            print('epoch %s, training loss:%s'%(i + 1, training_loss))\n            print('epoch %s, testing loss:%s'%(i + 1, testing_loss))\n            print()\n        \n        training_loss_list.append(training_loss)\n        testing_loss_list.append(testing_loss)\n        \n        backward(trainY, train_y_pred, H, Z, trainX, parameters, learning_rate)\n    return training_loss_list, testing_loss_list\n```\n绘制loss随epoch的变化曲线。\n```py\ndef plot_loss_curve(training_loss_list, testing_loss_list):\n    '''\n    绘制损失值变化曲线\n    \n    Parameters\n    ----------\n    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n    \n    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n    \n    '''\n    plt.figure(figsize = (10, 6))\n    plt.plot(training_loss_list, label = 'training loss')\n    plt.plot(testing_loss_list, label = 'testing loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend()\n```\n预测\n```py\ndef predict(X, parameters):\n    '''\n    预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，取每行最大的那个数的下标作为标记\n    \n    Parameters\n    ----------\n    X: np.ndarray, shape = (n, m), 训练集\n    \n    parameters: dict，参数\n    \n    Returns\n    ----------\n    prediction: np.ndarray, shape = (n, 1)，预测的标记\n    \n    '''\n    # 用forward函数得到softmax激活前的值\n    # YOUR CODE HERE\n    O = forward(X, parameters)\n    \n    # 计算softmax激活后的值\n    # YOUR CODE HERE\n    y_pred = softmax(O)\n    \n    # 取每行最大的元素对应的下标\n    # YOUR CODE HERE\n    prediction = np.argmax(y_pred, axis=1)\n    \n    return prediction\n```\n训练一个不算特别优秀的3-layer-perceptron。\n```py\nfrom sklearn.metrics import accuracy_score\nstart_time = time()\n\nh = 50\nK = 10\nparameters = initialize(h, K)\ntraining_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters, 1000, 0.03, False)\n\nend_time = time()\nprint('training time: %s s'%(end_time - start_time))\nprediction = predict(testX, parameters)\nprint(accuracy_score(prediction, testY))\nplot_loss_curve(training_loss_list, testing_loss_list)\n``` \n到这里就结束了，其实不算复杂，数值计算的细节比较重要，以前经常用pytorch来写BP、RNN之类的，但是很少从底层去实现过，这还是一个简单的感知机模型，较复杂的基础模型涉及到的内容可能更复杂。只能说我企图学会吧。","slug":"3-layer-MLP-md","published":1,"updated":"2021-11-09T16:32:36.164Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckvsbawxx0000r5oe89koez8m","content":"<h2 id=\"实现内容：\"><a href=\"#实现内容：\" class=\"headerlink\" title=\"实现内容：\"></a>实现内容：</h2><ol>\n<li>实现一个三层感知机</li>\n<li>对手写数字数据集进行分类</li>\n<li>绘制损失值变化曲线</li>\n<li>完成kaggle MNIST手写数字分类任务，根据给定的超参数训练模型，完成表格的填写</li>\n</ol>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>数据集使用手写数字集。并且40%作测试集，60%做训练集。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">from</span> time <span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\">trainX, testX, trainY, testY = train_test_split(load_digits()[<span class=\"string\">&#x27;data&#x27;</span>], load_digits()[<span class=\"string\">&#x27;target&#x27;</span>], test_size = <span class=\"number\">0.4</span>, random_state = <span class=\"number\">32</span>)</span><br></pre></td></tr></table></figure>\n\n<p>接下来是数据预处理，神经网络的训练方法一般是基于梯度的优化算法，如梯度下降，为了让这类算法能更好的优化神经网络，我们往往需要对数据集进行归一化，这里我们选择对数据进行标准化。</p>\n<p>减去均值可以让数据以0为中心，除以标准差可以让数据缩放到一个较小的范围内。这样可以使得梯度的下降方向更多样，同时缩小梯度的数量级，让学习变得稳定。  </p>\n<p>首先需要对训练集进行标准化，针对每个特征求出其均值和标准差，然后用训练集的每个样本减去均值除以标准差，就得到了新的训练集。然后用测试集的每个样本，减去训练集的均值，除以训练集的标准差，完成对测试集的标准化。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trainY_mat = np.zeros((<span class=\"built_in\">len</span>(trainY), <span class=\"number\">10</span>))</span><br><span class=\"line\">trainY_mat[np.arange(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(trainY), <span class=\"number\">1</span>), trainY] = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">testY_mat = np.zeros((<span class=\"built_in\">len</span>(testY), <span class=\"number\">10</span>))</span><br><span class=\"line\">testY_mat[np.arange(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(testY), <span class=\"number\">1</span>), testY] = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<p>下面是参数的初始化。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize</span>(<span class=\"params\">h, K</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    参数初始化</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    h: int: 隐藏层单元个数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    K: int: 输出层单元个数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数，键是&quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">32</span>)</span><br><span class=\"line\">    W_1 = np.random.normal(size = (trainX.shape[<span class=\"number\">1</span>], h)) * <span class=\"number\">0.01</span></span><br><span class=\"line\">    b_1 = np.zeros((<span class=\"number\">1</span>, h))</span><br><span class=\"line\">    </span><br><span class=\"line\">    np.random.seed(<span class=\"number\">32</span>)</span><br><span class=\"line\">    W_2 = np.random.normal(size = (h, K)) * <span class=\"number\">0.01</span></span><br><span class=\"line\">    b_2 = np.zeros((<span class=\"number\">1</span>, K))</span><br><span class=\"line\">    </span><br><span class=\"line\">    parameters = &#123;<span class=\"string\">&#x27;W1&#x27;</span>: W_1, <span class=\"string\">&#x27;b1&#x27;</span>: b_1, <span class=\"string\">&#x27;W2&#x27;</span>: W_2, <span class=\"string\">&#x27;b2&#x27;</span>: b_2&#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<p>向前传播，这里具体指的就是依据公式向前计算值。</p>\n<p>这里有一点要注意，矩阵的点乘是使用<code>np.dot()</code>进行的，否则py会默认为元素乘。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_combination</span>(<span class=\"params\">X, W, b</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    计算Z，Z = XW + b</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    W: np.ndarray, shape = (m, h)，权重</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    b: np.ndarray, shape = (1, h)，偏置</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    Z: np.ndarray, shape = (n, h)，线性组合后的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Z = XW + b</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    Z = np.dot(X,W) + b</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> Z</span><br></pre></td></tr></table></figure>\n<p>每一线性层的输出都要经过一个activate，隐藏层的activate为ReLu。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ReLU</span>(<span class=\"params\">X</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    ReLU激活函数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray，待激活的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    activations: np.ndarray, 激活后的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    X[X &lt; <span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    activations = X</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> activations</span><br></pre></td></tr></table></figure>\n<p>输出层要经过softmax找到每一个label的概率大小。这里值得注意的是，O矩阵的求和是对每一行的各个元素求和，而不是对所有元素求和，所以要有<code>axis=1</code>，对行进行sum操作，并保持维度。</p>\n<p>前一个<code>my_softmax(O)</code>会导致对于较小值的output，会导致分母为0的情况，所以要对其进行一些处理，让O的每一个元素减去该行的最大值，这样能保证取exp后至少一个元素为1，所以不会出现NaN的情况。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">my_softmax</span>(<span class=\"params\">O</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    softmax激活</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.exp(O) / np.<span class=\"built_in\">sum</span>(np.exp(O), axis = <span class=\"number\">1</span>, keepdims = <span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">softmax</span>(<span class=\"params\">O</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    softmax激活函数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    O: np.ndarray，待激活的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    activations: np.ndarray, 激活后的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HEER</span></span><br><span class=\"line\">    O = O - np.<span class=\"built_in\">max</span>(O, axis=<span class=\"number\">1</span>, keepdims=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    activations = my_softmax(O)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> activations</span><br></pre></td></tr></table></figure>\n<p>接下来是实现损失函数，交叉熵损失函数：<br><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathrm{loss}&space;=&space;-&space;\\frac{1}{n}&space;\\sum_n&space;\\sum^{K}_{k=1}&space;y_k&space;\\log{(\\hat{y_k})}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathrm{loss}&space;=&space;-&space;\\frac{1}{n}&space;\\sum_n&space;\\sum^{K}_{k=1}&space;y_k&space;\\log{(\\hat{y_k})}\" title=\"\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}\" /></a><br>这里又会出一个问题，交叉熵损失函数中，我们需要对softmax的激活值取对数，也就是log\\haty，这就要求我们的激活值全都是大于0的数，不能等于0，但是我们实现的softmax在有些时候确实会输出0。这就使得在计算loss的时候会出现问题，解决这个问题的方法是log softmax。所谓log softmax，就是将交叉熵中的对数运算与softmax结合起来，避开为0的情况。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\log{\\frac{\\exp{(O_i)}}{\\sum_K&space;\\exp{(O_k)}}}&space;&=&space;\\log{\\frac{\\exp{(O_i&space;-&space;\\mathrm{max}(O))}}{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}}\\\\&space;&=&space;O_i&space;-&space;\\mathrm{max}(O)&space;-&space;\\log{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\log{\\frac{\\exp{(O_i)}}{\\sum_K&space;\\exp{(O_k)}}}&space;&=&space;\\log{\\frac{\\exp{(O_i&space;-&space;\\mathrm{max}(O))}}{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}}\\\\&space;&=&space;O_i&space;-&space;\\mathrm{max}(O)&space;-&space;\\log{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}&space;\\end{aligned}\" title=\"\\begin{aligned} \\log{\\frac{\\exp{(O_i)}}{\\sum_K \\exp{(O_k)}}} &= \\log{\\frac{\\exp{(O_i - \\mathrm{max}(O))}}{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}}}\\\\ &= O_i - \\mathrm{max}(O) - \\log{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}} \\end{aligned}\" /></a></p>\n<p>这样我们再计算loss的时候就可以把输出层的输出直接放到log softmax中计算，不用先激活，再取对数了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">log_softmax</span>(<span class=\"params\">x</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    log softmax</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    x: np.ndarray，待激活的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    log_activations: np.ndarray, 激活后取了对数的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    log_activations = x - np.<span class=\"built_in\">max</span>(x) - np.log( np.<span class=\"built_in\">sum</span>(np.exp(x - np.<span class=\"built_in\">max</span>(x)), axis = <span class=\"number\">1</span>, keepdims = <span class=\"literal\">True</span>) )</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> log_activations</span><br></pre></td></tr></table></figure>\n<p>然后编写<code>cross_entropy_with_softmax</code>。函数内容不再赘述。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cross_entropy_with_softmax</span>(<span class=\"params\">y_true, O</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    求解交叉熵损失函数，这里需要使用log softmax，所以参数分别是真值和未经softmax激活的输出值</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    loss: float, 平均的交叉熵损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 平均交叉熵损失</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    loss = - <span class=\"number\">1</span>/<span class=\"built_in\">len</span>(y_true) * np.<span class=\"built_in\">sum</span>(np.<span class=\"built_in\">sum</span>(y_true * log_softmax(O)))    <span class=\"comment\"># 这里是元素乘</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br></pre></td></tr></table></figure>\n<p>正是因为softmax激活与交叉熵损失会有这样的问题，所以在很多深度学习框架中，交叉熵损失函数就直接带有了激活的功能，所以我们在实现前向传播计算的时候，就不要加softmax激活函数了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">X, parameters</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    前向传播，从输入一直到输出层softmax激活前的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 输入层到隐藏层</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    Z = np.dot(X, parameters[<span class=\"string\">&#x27;W1&#x27;</span>]) + parameters[<span class=\"string\">&#x27;b1&#x27;</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 隐藏层的激活</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    H = ReLU(Z)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 隐藏层到输出层</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    O = np.dot(H, parameters[<span class=\"string\">&#x27;W2&#x27;</span>]) + parameters[<span class=\"string\">&#x27;b2&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> O</span><br></pre></td></tr></table></figure>\n<p>下面是反向传播，也是本篇blog的重点。首先是偏导的推导，细节不再赘述，使用链式求导法则认真推导即可。</p>\n<p>forward公式：最后一层的输出，使用softmax函数激活，得到神经网络计算出的各类的概率值。<br><a href=\"https://www.codecogs.com/eqnedit.php?latex=Z&space;=&space;XW_1&space;&plus;&space;b_1\\\\&space;H_1&space;=&space;\\mathrm{ReLU}(Z)\\\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?Z&space;=&space;XW_1&space;&plus;&space;b_1\\\\&space;H_1&space;=&space;\\mathrm{ReLU}(Z)\\\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2\" title=\"Z = XW_1 + b_1\\\\ H_1 = \\mathrm{ReLU}(Z)\\\\ O = H_1 W_2 + b_2\" /></a></p>\n<p>损失函数对参数W_2和b_2的偏导数：<br><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;[{H_1}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)]&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;[{H_1}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)]&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial W_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{1}{n} [{H_1}^\\mathrm{T} (\\hat{y} - y)] \\end{aligned}\" /></a></p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;\\sum^n_{i=1}&space;(\\hat{y_i}&space;-&space;y_i)&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;\\sum^n_{i=1}&space;(\\hat{y_i}&space;-&space;y_i)&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial b_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{1}{n} \\sum^n_{i=1} (\\hat{y_i} - y_i) \\end{aligned}\" /></a></p>\n<p>求得loss对W_1和b_1的偏导数：<br><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;[(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}]\\\\&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;[(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}]\\\\&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial W_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\ & = \\frac{1}{n} {X}^\\mathrm{T} [(\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}]\\\\ \\end{aligned}\" /></a></p>\n<p>ReLu的偏导数：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{\\partial&space;\\mathrm{ReLU(x)}}{\\partial&space;x}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;x&space;<&space;0\\\\&space;1&space;&&space;\\text{if&space;}&space;x&space;\\geq&space;0&space;\\end{cases}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial&space;\\mathrm{ReLU(x)}}{\\partial&space;x}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;x&space;<&space;0\\\\&space;1&space;&&space;\\text{if&space;}&space;x&space;\\geq&space;0&space;\\end{cases}\" title=\"\\frac{\\partial \\mathrm{ReLU(x)}}{\\partial x} = \\begin{cases} 0 & \\text{if } x < 0\\\\ 1 & \\text{if } x \\geq 0 \\end{cases}\" /></a></p>\n<p>从而：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{\\partial&space;loss}{\\partial&space;{W_1}_{ij}}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial&space;loss}{\\partial&space;{W_1}_{ij}}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}\" title=\"\\frac{\\partial loss}{\\partial {W_1}_{ij}} = \\begin{cases} 0 & \\text{if } {Z}_{ij} < 0\\\\ \\frac{1}{n} {X}^\\mathrm{T} (\\hat{y} - y) {W_2}^\\mathrm{T} & \\text{if } {Z}_{ij} \\geq 0 \\end{cases}\" /></a></p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}\\\\&space;&&space;=&space;\\begin{cases}&space;0&space;&\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;\\sum_n&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}\\\\&space;&&space;=&space;\\begin{cases}&space;0&space;&\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;\\sum_n&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial b_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\ & = \\frac{1}{n} (\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}\\\\ & = \\begin{cases} 0 &\\text{if } {Z}_{ij} < 0\\\\ \\frac{1}{n} \\sum_n (\\hat{y} - y) {W_2}^\\mathrm{T} &\\text{if } {Z}_{ij} \\geq 0 \\end{cases} \\end{aligned}\" /></a></p>\n<p>描述完公式后下面来用代码实现，首先dW2和db2的代码是很显然的。对于dW1，这里涉及到的ReLu的偏导数，很显然如果hidden层的值小于零对应ReLu为0时，定义其偏导为0，那么如何确定dW1中的那些值是由该定义得到的呢。如果我们眼光狭窄只分析dW2公式的最后结果必然很难分析出来，因为最终的dW2是(hidden, output)维度的，而relu_regard是(n, hidden)维度的，直接对它们进行关联显然不现实。那么需要追根溯源，深入了解这个dW2的来由。</p>\n<p>在dW2分段函数的前一步，它的结果是XT点积后面的一堆，其中H对Z的偏导其实就是ReLu的偏导，是在这里决定了dW2的值，再来分析一下维度1/n可以broadcast不用管，后面是<code>(n, input)T · [(n, output) · (hidden, output)T * (n, hidden)]</code>这样的维度关系。这里尤其要注意最后一个运算，我一开始卡在这里好久，因为这里涉及到了元素乘，<code>(n, hidden) * (n, hidden)</code>，这里决定了哪个计算位置的值来自于ReLu的0，元素乘后再与X的转置计算。</p>\n<p>db2同理。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_gradient</span>(<span class=\"params\">y_true, y_pred, H, Z, X, parameters</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    计算梯度</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    H: np.ndarray, shape = (n, h)，隐藏层激活后的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray, shape = (n, m)，输入的原始数据</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    grads: dict, 梯度</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算W2的梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    dW2 = (<span class=\"number\">1</span>/<span class=\"built_in\">len</span>(y_true)) * np.dot(np.transpose(H), (y_pred - y_true))</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算b2的梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    db2 = (<span class=\"number\">1</span>/<span class=\"built_in\">len</span>(y_true)) * np.<span class=\"built_in\">sum</span>(y_pred - y_true, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算ReLU的梯度</span></span><br><span class=\"line\">    relu_grad = Z.copy()</span><br><span class=\"line\">    relu_grad[relu_grad &gt;= <span class=\"number\">0</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">    relu_grad[relu_grad &lt; <span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算W1的梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    dW1 = <span class=\"number\">1</span>/<span class=\"built_in\">len</span>(y_true) * np.dot(np.transpose(X), np.dot((y_pred - y_true), np.transpose(parameters[<span class=\"string\">&#x27;W2&#x27;</span>])) * relu_grad  )</span><br><span class=\"line\">    <span class=\"comment\"># 计算b1的梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    db1 = <span class=\"number\">1</span>/<span class=\"built_in\">len</span>(y_true) * np.<span class=\"built_in\">sum</span>(np.dot((y_pred - y_true), np.transpose(parameters[<span class=\"string\">&#x27;W2&#x27;</span>])) * relu_grad, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    grads = &#123;<span class=\"string\">&#x27;dW2&#x27;</span>: dW2, <span class=\"string\">&#x27;db2&#x27;</span>: db2, <span class=\"string\">&#x27;dW1&#x27;</span>: dW1, <span class=\"string\">&#x27;db1&#x27;</span>: db1&#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> grads</span><br></pre></td></tr></table></figure>\n<p>梯度下降，反向传播，参数更新。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update</span>(<span class=\"params\">parameters, grads, learning_rate</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    参数更新</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    grads: dict, 梯度</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    learning_rate: float, 学习率</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    parameters[<span class=\"string\">&#x27;W2&#x27;</span>] -= learning_rate * grads[<span class=\"string\">&#x27;dW2&#x27;</span>]</span><br><span class=\"line\">    parameters[<span class=\"string\">&#x27;b2&#x27;</span>] -= learning_rate * grads[<span class=\"string\">&#x27;db2&#x27;</span>]</span><br><span class=\"line\">    parameters[<span class=\"string\">&#x27;W1&#x27;</span>] -= learning_rate * grads[<span class=\"string\">&#x27;dW1&#x27;</span>]</span><br><span class=\"line\">    parameters[<span class=\"string\">&#x27;b1&#x27;</span>] -= learning_rate * grads[<span class=\"string\">&#x27;db1&#x27;</span>]</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backward</span>(<span class=\"params\">y_true, y_pred, H, Z, X, parameters, learning_rate</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    计算梯度，参数更新</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    H: np.ndarray, shape = (n, h)，隐藏层激活后的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray, shape = (n, m)，输入的原始数据</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    learning_rate: float, 学习率</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    grads = compute_gradient(y_true, y_pred, H, Z, X, parameters)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 更新参数</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    update(parameters, grads, learning_rate)</span><br></pre></td></tr></table></figure>\n<p>训练。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span>(<span class=\"params\">trainX, trainY, testX, testY, parameters, epochs, learning_rate = <span class=\"number\">0.01</span>, verbose = <span class=\"literal\">False</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    训练</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    trainX: np.ndarray, shape = (n, m), 训练集</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    trainY: np.ndarray, shape = (n, K), 训练集标记</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    testX: np.ndarray, shape = (n_test, m)，测试集</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    testY: np.ndarray, shape = (n_test, K)，测试集的标记</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    epochs: int, 要迭代的轮数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    learning_rate: float, default 0.01，学习率</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    verbose: boolean, default False，是否打印损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 存储损失值</span></span><br><span class=\"line\">    training_loss_list = []</span><br><span class=\"line\">    testing_loss_list = []</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 这里要计算出Z和H，因为后面反向传播计算梯度的时候需要这两个矩阵</span></span><br><span class=\"line\">        Z = linear_combination(trainX, parameters[<span class=\"string\">&#x27;W1&#x27;</span>], parameters[<span class=\"string\">&#x27;b1&#x27;</span>])</span><br><span class=\"line\">        H = ReLU(Z)</span><br><span class=\"line\">        train_O = linear_combination(H, parameters[<span class=\"string\">&#x27;W2&#x27;</span>], parameters[<span class=\"string\">&#x27;b2&#x27;</span>])</span><br><span class=\"line\">        train_y_pred = softmax(train_O)</span><br><span class=\"line\">        training_loss = cross_entropy_with_softmax(trainY, train_O)</span><br><span class=\"line\">        </span><br><span class=\"line\">        test_O = forward(testX, parameters)</span><br><span class=\"line\">        testing_loss = cross_entropy_with_softmax(testY, test_O)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span> verbose == <span class=\"literal\">True</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;epoch %s, training loss:%s&#x27;</span>%(i + <span class=\"number\">1</span>, training_loss))</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;epoch %s, testing loss:%s&#x27;</span>%(i + <span class=\"number\">1</span>, testing_loss))</span><br><span class=\"line\">            <span class=\"built_in\">print</span>()</span><br><span class=\"line\">        </span><br><span class=\"line\">        training_loss_list.append(training_loss)</span><br><span class=\"line\">        testing_loss_list.append(testing_loss)</span><br><span class=\"line\">        </span><br><span class=\"line\">        backward(trainY, train_y_pred, H, Z, trainX, parameters, learning_rate)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> training_loss_list, testing_loss_list</span><br></pre></td></tr></table></figure>\n<p>绘制loss随epoch的变化曲线。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_loss_curve</span>(<span class=\"params\">training_loss_list, testing_loss_list</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    绘制损失值变化曲线</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    plt.figure(figsize = (<span class=\"number\">10</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\">    plt.plot(training_loss_list, label = <span class=\"string\">&#x27;training loss&#x27;</span>)</span><br><span class=\"line\">    plt.plot(testing_loss_list, label = <span class=\"string\">&#x27;testing loss&#x27;</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">&#x27;epoch&#x27;</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">&#x27;loss&#x27;</span>)</span><br><span class=\"line\">    plt.legend()</span><br></pre></td></tr></table></figure>\n<p>预测</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span>(<span class=\"params\">X, parameters</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，取每行最大的那个数的下标作为标记</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray, shape = (n, m), 训练集</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    prediction: np.ndarray, shape = (n, 1)，预测的标记</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 用forward函数得到softmax激活前的值</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    O = forward(X, parameters)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算softmax激活后的值</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    y_pred = softmax(O)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 取每行最大的元素对应的下标</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    prediction = np.argmax(y_pred, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> prediction</span><br></pre></td></tr></table></figure>\n<p>训练一个不算特别优秀的3-layer-perceptron。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> accuracy_score</span><br><span class=\"line\">start_time = time()</span><br><span class=\"line\"></span><br><span class=\"line\">h = <span class=\"number\">50</span></span><br><span class=\"line\">K = <span class=\"number\">10</span></span><br><span class=\"line\">parameters = initialize(h, K)</span><br><span class=\"line\">training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters, <span class=\"number\">1000</span>, <span class=\"number\">0.03</span>, <span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">end_time = time()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;training time: %s s&#x27;</span>%(end_time - start_time))</span><br><span class=\"line\">prediction = predict(testX, parameters)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(accuracy_score(prediction, testY))</span><br><span class=\"line\">plot_loss_curve(training_loss_list, testing_loss_list)</span><br></pre></td></tr></table></figure>\n<p>到这里就结束了，其实不算复杂，数值计算的细节比较重要，以前经常用pytorch来写BP、RNN之类的，但是很少从底层去实现过，这还是一个简单的感知机模型，较复杂的基础模型涉及到的内容可能更复杂。只能说我企图学会吧。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"实现内容：\"><a href=\"#实现内容：\" class=\"headerlink\" title=\"实现内容：\"></a>实现内容：</h2><ol>\n<li>实现一个三层感知机</li>\n<li>对手写数字数据集进行分类</li>\n<li>绘制损失值变化曲线</li>\n<li>完成kaggle MNIST手写数字分类任务，根据给定的超参数训练模型，完成表格的填写</li>\n</ol>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>数据集使用手写数字集。并且40%作测试集，60%做训练集。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">from</span> time <span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\">trainX, testX, trainY, testY = train_test_split(load_digits()[<span class=\"string\">&#x27;data&#x27;</span>], load_digits()[<span class=\"string\">&#x27;target&#x27;</span>], test_size = <span class=\"number\">0.4</span>, random_state = <span class=\"number\">32</span>)</span><br></pre></td></tr></table></figure>\n\n<p>接下来是数据预处理，神经网络的训练方法一般是基于梯度的优化算法，如梯度下降，为了让这类算法能更好的优化神经网络，我们往往需要对数据集进行归一化，这里我们选择对数据进行标准化。</p>\n<p>减去均值可以让数据以0为中心，除以标准差可以让数据缩放到一个较小的范围内。这样可以使得梯度的下降方向更多样，同时缩小梯度的数量级，让学习变得稳定。  </p>\n<p>首先需要对训练集进行标准化，针对每个特征求出其均值和标准差，然后用训练集的每个样本减去均值除以标准差，就得到了新的训练集。然后用测试集的每个样本，减去训练集的均值，除以训练集的标准差，完成对测试集的标准化。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trainY_mat = np.zeros((<span class=\"built_in\">len</span>(trainY), <span class=\"number\">10</span>))</span><br><span class=\"line\">trainY_mat[np.arange(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(trainY), <span class=\"number\">1</span>), trainY] = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">testY_mat = np.zeros((<span class=\"built_in\">len</span>(testY), <span class=\"number\">10</span>))</span><br><span class=\"line\">testY_mat[np.arange(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(testY), <span class=\"number\">1</span>), testY] = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<p>下面是参数的初始化。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize</span>(<span class=\"params\">h, K</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    参数初始化</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    h: int: 隐藏层单元个数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    K: int: 输出层单元个数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数，键是&quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">32</span>)</span><br><span class=\"line\">    W_1 = np.random.normal(size = (trainX.shape[<span class=\"number\">1</span>], h)) * <span class=\"number\">0.01</span></span><br><span class=\"line\">    b_1 = np.zeros((<span class=\"number\">1</span>, h))</span><br><span class=\"line\">    </span><br><span class=\"line\">    np.random.seed(<span class=\"number\">32</span>)</span><br><span class=\"line\">    W_2 = np.random.normal(size = (h, K)) * <span class=\"number\">0.01</span></span><br><span class=\"line\">    b_2 = np.zeros((<span class=\"number\">1</span>, K))</span><br><span class=\"line\">    </span><br><span class=\"line\">    parameters = &#123;<span class=\"string\">&#x27;W1&#x27;</span>: W_1, <span class=\"string\">&#x27;b1&#x27;</span>: b_1, <span class=\"string\">&#x27;W2&#x27;</span>: W_2, <span class=\"string\">&#x27;b2&#x27;</span>: b_2&#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<p>向前传播，这里具体指的就是依据公式向前计算值。</p>\n<p>这里有一点要注意，矩阵的点乘是使用<code>np.dot()</code>进行的，否则py会默认为元素乘。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_combination</span>(<span class=\"params\">X, W, b</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    计算Z，Z = XW + b</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    W: np.ndarray, shape = (m, h)，权重</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    b: np.ndarray, shape = (1, h)，偏置</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    Z: np.ndarray, shape = (n, h)，线性组合后的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Z = XW + b</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    Z = np.dot(X,W) + b</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> Z</span><br></pre></td></tr></table></figure>\n<p>每一线性层的输出都要经过一个activate，隐藏层的activate为ReLu。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ReLU</span>(<span class=\"params\">X</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    ReLU激活函数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray，待激活的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    activations: np.ndarray, 激活后的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    X[X &lt; <span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    activations = X</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> activations</span><br></pre></td></tr></table></figure>\n<p>输出层要经过softmax找到每一个label的概率大小。这里值得注意的是，O矩阵的求和是对每一行的各个元素求和，而不是对所有元素求和，所以要有<code>axis=1</code>，对行进行sum操作，并保持维度。</p>\n<p>前一个<code>my_softmax(O)</code>会导致对于较小值的output，会导致分母为0的情况，所以要对其进行一些处理，让O的每一个元素减去该行的最大值，这样能保证取exp后至少一个元素为1，所以不会出现NaN的情况。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">my_softmax</span>(<span class=\"params\">O</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    softmax激活</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.exp(O) / np.<span class=\"built_in\">sum</span>(np.exp(O), axis = <span class=\"number\">1</span>, keepdims = <span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">softmax</span>(<span class=\"params\">O</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    softmax激活函数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    O: np.ndarray，待激活的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    activations: np.ndarray, 激活后的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HEER</span></span><br><span class=\"line\">    O = O - np.<span class=\"built_in\">max</span>(O, axis=<span class=\"number\">1</span>, keepdims=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    activations = my_softmax(O)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> activations</span><br></pre></td></tr></table></figure>\n<p>接下来是实现损失函数，交叉熵损失函数：<br><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathrm{loss}&space;=&space;-&space;\\frac{1}{n}&space;\\sum_n&space;\\sum^{K}_{k=1}&space;y_k&space;\\log{(\\hat{y_k})}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathrm{loss}&space;=&space;-&space;\\frac{1}{n}&space;\\sum_n&space;\\sum^{K}_{k=1}&space;y_k&space;\\log{(\\hat{y_k})}\" title=\"\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}\" /></a><br>这里又会出一个问题，交叉熵损失函数中，我们需要对softmax的激活值取对数，也就是log\\haty，这就要求我们的激活值全都是大于0的数，不能等于0，但是我们实现的softmax在有些时候确实会输出0。这就使得在计算loss的时候会出现问题，解决这个问题的方法是log softmax。所谓log softmax，就是将交叉熵中的对数运算与softmax结合起来，避开为0的情况。</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\log{\\frac{\\exp{(O_i)}}{\\sum_K&space;\\exp{(O_k)}}}&space;&=&space;\\log{\\frac{\\exp{(O_i&space;-&space;\\mathrm{max}(O))}}{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}}\\\\&space;&=&space;O_i&space;-&space;\\mathrm{max}(O)&space;-&space;\\log{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\log{\\frac{\\exp{(O_i)}}{\\sum_K&space;\\exp{(O_k)}}}&space;&=&space;\\log{\\frac{\\exp{(O_i&space;-&space;\\mathrm{max}(O))}}{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}}\\\\&space;&=&space;O_i&space;-&space;\\mathrm{max}(O)&space;-&space;\\log{\\sum_K&space;\\exp{(O_k&space;-&space;\\mathrm{max}(O))}}&space;\\end{aligned}\" title=\"\\begin{aligned} \\log{\\frac{\\exp{(O_i)}}{\\sum_K \\exp{(O_k)}}} &= \\log{\\frac{\\exp{(O_i - \\mathrm{max}(O))}}{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}}}\\\\ &= O_i - \\mathrm{max}(O) - \\log{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}} \\end{aligned}\" /></a></p>\n<p>这样我们再计算loss的时候就可以把输出层的输出直接放到log softmax中计算，不用先激活，再取对数了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">log_softmax</span>(<span class=\"params\">x</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    log softmax</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    x: np.ndarray，待激活的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    log_activations: np.ndarray, 激活后取了对数的矩阵</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    log_activations = x - np.<span class=\"built_in\">max</span>(x) - np.log( np.<span class=\"built_in\">sum</span>(np.exp(x - np.<span class=\"built_in\">max</span>(x)), axis = <span class=\"number\">1</span>, keepdims = <span class=\"literal\">True</span>) )</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> log_activations</span><br></pre></td></tr></table></figure>\n<p>然后编写<code>cross_entropy_with_softmax</code>。函数内容不再赘述。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cross_entropy_with_softmax</span>(<span class=\"params\">y_true, O</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    求解交叉熵损失函数，这里需要使用log softmax，所以参数分别是真值和未经softmax激活的输出值</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    loss: float, 平均的交叉熵损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 平均交叉熵损失</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    loss = - <span class=\"number\">1</span>/<span class=\"built_in\">len</span>(y_true) * np.<span class=\"built_in\">sum</span>(np.<span class=\"built_in\">sum</span>(y_true * log_softmax(O)))    <span class=\"comment\"># 这里是元素乘</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br></pre></td></tr></table></figure>\n<p>正是因为softmax激活与交叉熵损失会有这样的问题，所以在很多深度学习框架中，交叉熵损失函数就直接带有了激活的功能，所以我们在实现前向传播计算的时候，就不要加softmax激活函数了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">X, parameters</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    前向传播，从输入一直到输出层softmax激活前的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 输入层到隐藏层</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    Z = np.dot(X, parameters[<span class=\"string\">&#x27;W1&#x27;</span>]) + parameters[<span class=\"string\">&#x27;b1&#x27;</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 隐藏层的激活</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    H = ReLU(Z)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 隐藏层到输出层</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    O = np.dot(H, parameters[<span class=\"string\">&#x27;W2&#x27;</span>]) + parameters[<span class=\"string\">&#x27;b2&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> O</span><br></pre></td></tr></table></figure>\n<p>下面是反向传播，也是本篇blog的重点。首先是偏导的推导，细节不再赘述，使用链式求导法则认真推导即可。</p>\n<p>forward公式：最后一层的输出，使用softmax函数激活，得到神经网络计算出的各类的概率值。<br><a href=\"https://www.codecogs.com/eqnedit.php?latex=Z&space;=&space;XW_1&space;&plus;&space;b_1\\\\&space;H_1&space;=&space;\\mathrm{ReLU}(Z)\\\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?Z&space;=&space;XW_1&space;&plus;&space;b_1\\\\&space;H_1&space;=&space;\\mathrm{ReLU}(Z)\\\\&space;O&space;=&space;H_1&space;W_2&space;&plus;&space;b_2\" title=\"Z = XW_1 + b_1\\\\ H_1 = \\mathrm{ReLU}(Z)\\\\ O = H_1 W_2 + b_2\" /></a></p>\n<p>损失函数对参数W_2和b_2的偏导数：<br><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;[{H_1}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)]&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;W_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;[{H_1}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)]&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial W_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial W_2}\\\\ & = \\frac{1}{n} [{H_1}^\\mathrm{T} (\\hat{y} - y)] \\end{aligned}\" /></a></p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;\\sum^n_{i=1}&space;(\\hat{y_i}&space;-&space;y_i)&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_2}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;\\frac{\\partial&space;O}{\\partial&space;b_2}\\\\&space;&&space;=&space;\\frac{1}{n}&space;\\sum^n_{i=1}&space;(\\hat{y_i}&space;-&space;y_i)&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial b_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial b_2}\\\\ & = \\frac{1}{n} \\sum^n_{i=1} (\\hat{y_i} - y_i) \\end{aligned}\" /></a></p>\n<p>求得loss对W_1和b_1的偏导数：<br><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;[(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}]\\\\&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;W_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;W_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;[(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}]\\\\&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial W_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\ & = \\frac{1}{n} {X}^\\mathrm{T} [(\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}]\\\\ \\end{aligned}\" /></a></p>\n<p>ReLu的偏导数：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{\\partial&space;\\mathrm{ReLU(x)}}{\\partial&space;x}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;x&space;<&space;0\\\\&space;1&space;&&space;\\text{if&space;}&space;x&space;\\geq&space;0&space;\\end{cases}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial&space;\\mathrm{ReLU(x)}}{\\partial&space;x}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;x&space;<&space;0\\\\&space;1&space;&&space;\\text{if&space;}&space;x&space;\\geq&space;0&space;\\end{cases}\" title=\"\\frac{\\partial \\mathrm{ReLU(x)}}{\\partial x} = \\begin{cases} 0 & \\text{if } x < 0\\\\ 1 & \\text{if } x \\geq 0 \\end{cases}\" /></a></p>\n<p>从而：</p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{\\partial&space;loss}{\\partial&space;{W_1}_{ij}}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial&space;loss}{\\partial&space;{W_1}_{ij}}&space;=&space;\\begin{cases}&space;0&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;{X}^\\mathrm{T}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&&space;\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}\" title=\"\\frac{\\partial loss}{\\partial {W_1}_{ij}} = \\begin{cases} 0 & \\text{if } {Z}_{ij} < 0\\\\ \\frac{1}{n} {X}^\\mathrm{T} (\\hat{y} - y) {W_2}^\\mathrm{T} & \\text{if } {Z}_{ij} \\geq 0 \\end{cases}\" /></a></p>\n<p><a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}\\\\&space;&&space;=&space;\\begin{cases}&space;0&space;&\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;\\sum_n&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}&space;\\end{aligned}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{aligned}&space;\\frac{\\partial&space;loss}{\\partial&space;b_1}&space;&&space;=&space;\\frac{\\partial&space;\\mathrm{loss}}{\\partial&space;\\hat{y}}&space;\\frac{\\partial&space;\\hat{y}}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{\\partial&space;loss}{\\partial&space;O}&space;\\frac{\\partial&space;O}{\\partial&space;H_1}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}&space;\\frac{\\partial&space;Z}{\\partial&space;b_1}\\\\&space;&&space;=&space;\\frac{1}{n}&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;\\frac{\\partial&space;H_1}{\\partial&space;Z}\\\\&space;&&space;=&space;\\begin{cases}&space;0&space;&\\text{if&space;}&space;{Z}_{ij}&space;<&space;0\\\\&space;\\frac{1}{n}&space;\\sum_n&space;(\\hat{y}&space;-&space;y)&space;{W_2}^\\mathrm{T}&space;&\\text{if&space;}&space;{Z}_{ij}&space;\\geq&space;0&space;\\end{cases}&space;\\end{aligned}\" title=\"\\begin{aligned} \\frac{\\partial loss}{\\partial b_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\ & = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\ & = \\frac{1}{n} (\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}\\\\ & = \\begin{cases} 0 &\\text{if } {Z}_{ij} < 0\\\\ \\frac{1}{n} \\sum_n (\\hat{y} - y) {W_2}^\\mathrm{T} &\\text{if } {Z}_{ij} \\geq 0 \\end{cases} \\end{aligned}\" /></a></p>\n<p>描述完公式后下面来用代码实现，首先dW2和db2的代码是很显然的。对于dW1，这里涉及到的ReLu的偏导数，很显然如果hidden层的值小于零对应ReLu为0时，定义其偏导为0，那么如何确定dW1中的那些值是由该定义得到的呢。如果我们眼光狭窄只分析dW2公式的最后结果必然很难分析出来，因为最终的dW2是(hidden, output)维度的，而relu_regard是(n, hidden)维度的，直接对它们进行关联显然不现实。那么需要追根溯源，深入了解这个dW2的来由。</p>\n<p>在dW2分段函数的前一步，它的结果是XT点积后面的一堆，其中H对Z的偏导其实就是ReLu的偏导，是在这里决定了dW2的值，再来分析一下维度1/n可以broadcast不用管，后面是<code>(n, input)T · [(n, output) · (hidden, output)T * (n, hidden)]</code>这样的维度关系。这里尤其要注意最后一个运算，我一开始卡在这里好久，因为这里涉及到了元素乘，<code>(n, hidden) * (n, hidden)</code>，这里决定了哪个计算位置的值来自于ReLu的0，元素乘后再与X的转置计算。</p>\n<p>db2同理。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_gradient</span>(<span class=\"params\">y_true, y_pred, H, Z, X, parameters</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    计算梯度</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    H: np.ndarray, shape = (n, h)，隐藏层激活后的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray, shape = (n, m)，输入的原始数据</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    grads: dict, 梯度</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算W2的梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    dW2 = (<span class=\"number\">1</span>/<span class=\"built_in\">len</span>(y_true)) * np.dot(np.transpose(H), (y_pred - y_true))</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算b2的梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    db2 = (<span class=\"number\">1</span>/<span class=\"built_in\">len</span>(y_true)) * np.<span class=\"built_in\">sum</span>(y_pred - y_true, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算ReLU的梯度</span></span><br><span class=\"line\">    relu_grad = Z.copy()</span><br><span class=\"line\">    relu_grad[relu_grad &gt;= <span class=\"number\">0</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">    relu_grad[relu_grad &lt; <span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算W1的梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    dW1 = <span class=\"number\">1</span>/<span class=\"built_in\">len</span>(y_true) * np.dot(np.transpose(X), np.dot((y_pred - y_true), np.transpose(parameters[<span class=\"string\">&#x27;W2&#x27;</span>])) * relu_grad  )</span><br><span class=\"line\">    <span class=\"comment\"># 计算b1的梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    db1 = <span class=\"number\">1</span>/<span class=\"built_in\">len</span>(y_true) * np.<span class=\"built_in\">sum</span>(np.dot((y_pred - y_true), np.transpose(parameters[<span class=\"string\">&#x27;W2&#x27;</span>])) * relu_grad, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    grads = &#123;<span class=\"string\">&#x27;dW2&#x27;</span>: dW2, <span class=\"string\">&#x27;db2&#x27;</span>: db2, <span class=\"string\">&#x27;dW1&#x27;</span>: dW1, <span class=\"string\">&#x27;db1&#x27;</span>: db1&#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> grads</span><br></pre></td></tr></table></figure>\n<p>梯度下降，反向传播，参数更新。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update</span>(<span class=\"params\">parameters, grads, learning_rate</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    参数更新</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    grads: dict, 梯度</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    learning_rate: float, 学习率</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    parameters[<span class=\"string\">&#x27;W2&#x27;</span>] -= learning_rate * grads[<span class=\"string\">&#x27;dW2&#x27;</span>]</span><br><span class=\"line\">    parameters[<span class=\"string\">&#x27;b2&#x27;</span>] -= learning_rate * grads[<span class=\"string\">&#x27;db2&#x27;</span>]</span><br><span class=\"line\">    parameters[<span class=\"string\">&#x27;W1&#x27;</span>] -= learning_rate * grads[<span class=\"string\">&#x27;dW1&#x27;</span>]</span><br><span class=\"line\">    parameters[<span class=\"string\">&#x27;b1&#x27;</span>] -= learning_rate * grads[<span class=\"string\">&#x27;db1&#x27;</span>]</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backward</span>(<span class=\"params\">y_true, y_pred, H, Z, X, parameters, learning_rate</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    计算梯度，参数更新</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    H: np.ndarray, shape = (n, h)，隐藏层激活后的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray, shape = (n, m)，输入的原始数据</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    learning_rate: float, 学习率</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    grads = compute_gradient(y_true, y_pred, H, Z, X, parameters)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 更新参数</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    update(parameters, grads, learning_rate)</span><br></pre></td></tr></table></figure>\n<p>训练。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span>(<span class=\"params\">trainX, trainY, testX, testY, parameters, epochs, learning_rate = <span class=\"number\">0.01</span>, verbose = <span class=\"literal\">False</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    训练</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    trainX: np.ndarray, shape = (n, m), 训练集</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    trainY: np.ndarray, shape = (n, K), 训练集标记</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    testX: np.ndarray, shape = (n_test, m)，测试集</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    testY: np.ndarray, shape = (n_test, K)，测试集的标记</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    epochs: int, 要迭代的轮数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    learning_rate: float, default 0.01，学习率</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    verbose: boolean, default False，是否打印损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 存储损失值</span></span><br><span class=\"line\">    training_loss_list = []</span><br><span class=\"line\">    testing_loss_list = []</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 这里要计算出Z和H，因为后面反向传播计算梯度的时候需要这两个矩阵</span></span><br><span class=\"line\">        Z = linear_combination(trainX, parameters[<span class=\"string\">&#x27;W1&#x27;</span>], parameters[<span class=\"string\">&#x27;b1&#x27;</span>])</span><br><span class=\"line\">        H = ReLU(Z)</span><br><span class=\"line\">        train_O = linear_combination(H, parameters[<span class=\"string\">&#x27;W2&#x27;</span>], parameters[<span class=\"string\">&#x27;b2&#x27;</span>])</span><br><span class=\"line\">        train_y_pred = softmax(train_O)</span><br><span class=\"line\">        training_loss = cross_entropy_with_softmax(trainY, train_O)</span><br><span class=\"line\">        </span><br><span class=\"line\">        test_O = forward(testX, parameters)</span><br><span class=\"line\">        testing_loss = cross_entropy_with_softmax(testY, test_O)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span> verbose == <span class=\"literal\">True</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;epoch %s, training loss:%s&#x27;</span>%(i + <span class=\"number\">1</span>, training_loss))</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;epoch %s, testing loss:%s&#x27;</span>%(i + <span class=\"number\">1</span>, testing_loss))</span><br><span class=\"line\">            <span class=\"built_in\">print</span>()</span><br><span class=\"line\">        </span><br><span class=\"line\">        training_loss_list.append(training_loss)</span><br><span class=\"line\">        testing_loss_list.append(testing_loss)</span><br><span class=\"line\">        </span><br><span class=\"line\">        backward(trainY, train_y_pred, H, Z, trainX, parameters, learning_rate)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> training_loss_list, testing_loss_list</span><br></pre></td></tr></table></figure>\n<p>绘制loss随epoch的变化曲线。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_loss_curve</span>(<span class=\"params\">training_loss_list, testing_loss_list</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    绘制损失值变化曲线</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    plt.figure(figsize = (<span class=\"number\">10</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\">    plt.plot(training_loss_list, label = <span class=\"string\">&#x27;training loss&#x27;</span>)</span><br><span class=\"line\">    plt.plot(testing_loss_list, label = <span class=\"string\">&#x27;testing loss&#x27;</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">&#x27;epoch&#x27;</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">&#x27;loss&#x27;</span>)</span><br><span class=\"line\">    plt.legend()</span><br></pre></td></tr></table></figure>\n<p>预测</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span>(<span class=\"params\">X, parameters</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，取每行最大的那个数的下标作为标记</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X: np.ndarray, shape = (n, m), 训练集</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    parameters: dict，参数</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    prediction: np.ndarray, shape = (n, 1)，预测的标记</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 用forward函数得到softmax激活前的值</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    O = forward(X, parameters)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算softmax激活后的值</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    y_pred = softmax(O)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 取每行最大的元素对应的下标</span></span><br><span class=\"line\">    <span class=\"comment\"># YOUR CODE HERE</span></span><br><span class=\"line\">    prediction = np.argmax(y_pred, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> prediction</span><br></pre></td></tr></table></figure>\n<p>训练一个不算特别优秀的3-layer-perceptron。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> accuracy_score</span><br><span class=\"line\">start_time = time()</span><br><span class=\"line\"></span><br><span class=\"line\">h = <span class=\"number\">50</span></span><br><span class=\"line\">K = <span class=\"number\">10</span></span><br><span class=\"line\">parameters = initialize(h, K)</span><br><span class=\"line\">training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters, <span class=\"number\">1000</span>, <span class=\"number\">0.03</span>, <span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">end_time = time()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;training time: %s s&#x27;</span>%(end_time - start_time))</span><br><span class=\"line\">prediction = predict(testX, parameters)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(accuracy_score(prediction, testY))</span><br><span class=\"line\">plot_loss_curve(training_loss_list, testing_loss_list)</span><br></pre></td></tr></table></figure>\n<p>到这里就结束了，其实不算复杂，数值计算的细节比较重要，以前经常用pytorch来写BP、RNN之类的，但是很少从底层去实现过，这还是一个简单的感知机模型，较复杂的基础模型涉及到的内容可能更复杂。只能说我企图学会吧。</p>\n"},{"title":"acm-zhengshumicifang.md","date":"2021-11-08T06:55:12.000Z","_content":"","source":"_posts/acm-zhengshumicifang-md.md","raw":"---\ntitle: acm-zhengshumicifang.md\ndate: 2021-11-08 14:55:12\ntags:\n---\n","slug":"acm-zhengshumicifang-md","published":1,"updated":"2021-11-08T06:55:12.204Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckvsbawy20002r5oecb4r00lu","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Adaboost","date":"2021-12-20T17:27:40.000Z","_content":"\n## 关于Adaboost\n\nAdaboost算法是针对二分类问题提出的集成学习算法，是boosting类算法最著名的代表。当一个学习器的学习的正确率仅比随机猜测的正确率略高，那么就称它是弱学习器，当一个学习期的学习的正确率很高，那么就称它是强学习器。而且发现弱学习器算法要容易得多，这样就需要将弱学习器提升为强学习器。Adaboost的做法是首先选择一个弱学习器，然后进行多轮的训练，但是每一轮训练过后，都要根据当前的错误率去调整训练样本的权重，让预测正确的样本权重降低，预测错误的样本权重增加，从而达到每次训练都是针对上一次预测结果较差的部分进行的，从而训练出一个较强的学习器。\n\n## 实现\n\n首先展示理论算法描述。\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/IMG_ED06002BA2CA-1.jpeg\">\n\n按照上述步骤开始用代码实现。\n\n首先是初始化样本权重，初始他们的权重都是相等的，都是样本数分之一。代码如下所示。\n```python\nn_train, n_test = len(X_train), len(X_test)\nW = np.ones(n_train) / n_train  # 样本权重初始化\n```\n然后在规定的训练轮数下，在相应的样本权重下对弱分类器进行训练。\n```python\nWeak_clf.fit(X_train, Y_train, sample_weight=W)\n```\n然后在测试集进行预测，并且计算出不正确的样本数。\n```python\n # 预测不正确的样本数，计算精度\nmiss = [int(x) for x in (pred_train_i != Y_train)]\n# 在当前权重下计算错误率\nmiss_w = np.dot(W, miss)\n```\n下面就是根据预测的结果，对预测正确的样本权重进行削弱，对预测错误的样本权重进行加强，从而对样本对权重进行更新，用于下一次学习器的训练，公式代码如下所示。\n```python\n# 计算alpha\nalpha = 0.5 * np.log(float(1 - miss_w) / float(miss_w + 0.01))\n# 权重的系数\nfactor = [x if x == 1 else -1 for x in miss]\n# 更新样本权重\nW = np.multiply(W, np.exp([float(x) * alpha for x in factor]))\nW = W / sum(W)  # normalization\n```\n最终输出的H(x)要对每个也测结果乘alhpa然后加入到结果的列表中。\n```python\n# predict\npred_train_i = [1 if x == 1 else -1 for x in pred_train_i]\npred_test_i = [1 if x == 1 else -1 for x in pred_test_i]\npred_test = pred_test + np.multiply(alpha, pred_test_i)\n```\n最后对于列表中大于0 的值认为它预测为标签值1，小于0的值认为它预测为比标签值0。\n```python\npred_test = (pred_test > 0) * 1\n# pred = (pred > 0) * 1\nreturn pred_test\n```\n从而完成了Adaboost的一次训练过程。\n下面阐述一下我自己实现的十折交叉验证，这里使用的是sklearn的KFold，来对数据集进行十次划分，让每次划分的十个部分轮流做测试集，在同一弱学习器下训练十次，最终预测结果指标是这十次的求和取平均，代码如下所示。\n```python\nweak_clf = DecisionTreeClassifier(criterion='entropy', max_depth=2)\n# 十折交叉验证\n    acc = []\n    pre = []\n    rec = []\n    f1 = []\n    Data = data.copy()\n    kf = KFold(n_splits=10, shuffle=True, random_state=0)  # 10折\n    for train_index, test_index in tqdm(kf.split(Data)):  # 将数据划分为10折\n        train_data = Data[train_index]  # 选取的训练集数据下标\n        test_data = Data[test_index]  # 选取的测试集数据下标\n        x_train = train_data[:, :8]\n        y_train = train_data[:, 8]\n        x_test = test_data[:, :8]\n        y_test = test_data[:, 8]\n        scaler = StandardScaler()  # 标准化转换\n        scaler.fit(x_train)  # 训练标准化对象\n        x_train = scaler.transform(x_train)\n        scaler.fit(x_test)  # 训练标准化对象\n        x_test = scaler.transform(x_test)\n\n        pred_test = my_adaboost(weak_clf, x_train, x_test, y_train, y_test, epoch)\n        acc.append(accuracy_score(y_test, pred_test))\n        pre.append(precision_score(y_test, pred_test))\n        rec.append(recall_score(y_test, pred_test))\n        f1.append(f1_score(y_test, pred_test))\n\n    # 计算测试集的精度，查准率，查全率，F1\n    print(\"My Adaboost outcome in test set with {} epoch:\".format(epoch))\n    print(\"ACC:\", sum(acc) / 10)\n    print(\"PRE: \", sum(pre) / 10)\n    print(\"REC: \", sum(rec) / 10)\n    print(\"F1: \", sum(f1) / 10)\n```\n\n整体的代码如下：\n```python\n# 自己实现的adaboost\ndef my_adaboost(Weak_clf, X_train, X_test, Y_train, Y_test, Epoch):\n    \"\"\"    :param Weak_clf:    :param X_train:    :param X_test:    :param Y_train:    :param Y_test:    :param Epoch:    :return:    \"\"\"\n    n_train, n_test = len(X_train), len(X_test)\n    W = np.ones(n_train) / n_train  # 样本权重初始化\n    # W = np.ones(n) / n\n    pred_train, pred_test = [np.zeros(n_train), np.zeros(n_test)]\n    # pred = [np.zeros(n)]\n    for i in range(Epoch):\n        # 用特定权重训练分类器\n        Weak_clf.fit(X_train, Y_train, sample_weight=W)\n        pred_train_i = weak_clf.predict(X_train)\n        pred_test_i = weak_clf.predict(X_test)\n        # pred_i = cross_val_predict(Weak_clf, X, Y, cv=10)\n        # 预测不正确的样本数，计算精度\n        miss = [int(x) for x in (pred_train_i != Y_train)]\n        # 在当前权重下计算错误率\n        miss_w = np.dot(W, miss)\n        # 计算alpha\n        alpha = 0.5 * np.log(float(1 - miss_w) / float(miss_w + 0.01))\n        # 权重的系数\n        factor = [x if x == 1 else -1 for x in miss]\n        # 更新样本权重\n        W = np.multiply(W, np.exp([float(x) * alpha for x in factor]))\n        W = W / sum(W)  # normalization\n        # predict\n        pred_train_i = [1 if x == 1 else -1 for x in pred_train_i]\n        # pred_i = [1 if x == 1 else -1 for x in pred_i]\n        pred_test_i = [1 if x == 1 else -1 for x in pred_test_i]\n        pred_test = pred_test + np.multiply(alpha, pred_test_i)\n        pred_train = pred_train + np.multiply(alpha, pred_train_i)\n        # pred = pred + np.multiply(alpha, pred_i)\n    pred_train = (pred_train > 0) * 1\n    pred_test = (pred_test > 0) * 1\n    # pred = (pred > 0) * 1\n    return pred_test\n```","source":"_posts/Adaboost-md.md","raw":"---\ntitle: Adaboost\ndate: 2021-12-21 01:27:40\ntags: 机器学习\n---\n\n## 关于Adaboost\n\nAdaboost算法是针对二分类问题提出的集成学习算法，是boosting类算法最著名的代表。当一个学习器的学习的正确率仅比随机猜测的正确率略高，那么就称它是弱学习器，当一个学习期的学习的正确率很高，那么就称它是强学习器。而且发现弱学习器算法要容易得多，这样就需要将弱学习器提升为强学习器。Adaboost的做法是首先选择一个弱学习器，然后进行多轮的训练，但是每一轮训练过后，都要根据当前的错误率去调整训练样本的权重，让预测正确的样本权重降低，预测错误的样本权重增加，从而达到每次训练都是针对上一次预测结果较差的部分进行的，从而训练出一个较强的学习器。\n\n## 实现\n\n首先展示理论算法描述。\n\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/IMG_ED06002BA2CA-1.jpeg\">\n\n按照上述步骤开始用代码实现。\n\n首先是初始化样本权重，初始他们的权重都是相等的，都是样本数分之一。代码如下所示。\n```python\nn_train, n_test = len(X_train), len(X_test)\nW = np.ones(n_train) / n_train  # 样本权重初始化\n```\n然后在规定的训练轮数下，在相应的样本权重下对弱分类器进行训练。\n```python\nWeak_clf.fit(X_train, Y_train, sample_weight=W)\n```\n然后在测试集进行预测，并且计算出不正确的样本数。\n```python\n # 预测不正确的样本数，计算精度\nmiss = [int(x) for x in (pred_train_i != Y_train)]\n# 在当前权重下计算错误率\nmiss_w = np.dot(W, miss)\n```\n下面就是根据预测的结果，对预测正确的样本权重进行削弱，对预测错误的样本权重进行加强，从而对样本对权重进行更新，用于下一次学习器的训练，公式代码如下所示。\n```python\n# 计算alpha\nalpha = 0.5 * np.log(float(1 - miss_w) / float(miss_w + 0.01))\n# 权重的系数\nfactor = [x if x == 1 else -1 for x in miss]\n# 更新样本权重\nW = np.multiply(W, np.exp([float(x) * alpha for x in factor]))\nW = W / sum(W)  # normalization\n```\n最终输出的H(x)要对每个也测结果乘alhpa然后加入到结果的列表中。\n```python\n# predict\npred_train_i = [1 if x == 1 else -1 for x in pred_train_i]\npred_test_i = [1 if x == 1 else -1 for x in pred_test_i]\npred_test = pred_test + np.multiply(alpha, pred_test_i)\n```\n最后对于列表中大于0 的值认为它预测为标签值1，小于0的值认为它预测为比标签值0。\n```python\npred_test = (pred_test > 0) * 1\n# pred = (pred > 0) * 1\nreturn pred_test\n```\n从而完成了Adaboost的一次训练过程。\n下面阐述一下我自己实现的十折交叉验证，这里使用的是sklearn的KFold，来对数据集进行十次划分，让每次划分的十个部分轮流做测试集，在同一弱学习器下训练十次，最终预测结果指标是这十次的求和取平均，代码如下所示。\n```python\nweak_clf = DecisionTreeClassifier(criterion='entropy', max_depth=2)\n# 十折交叉验证\n    acc = []\n    pre = []\n    rec = []\n    f1 = []\n    Data = data.copy()\n    kf = KFold(n_splits=10, shuffle=True, random_state=0)  # 10折\n    for train_index, test_index in tqdm(kf.split(Data)):  # 将数据划分为10折\n        train_data = Data[train_index]  # 选取的训练集数据下标\n        test_data = Data[test_index]  # 选取的测试集数据下标\n        x_train = train_data[:, :8]\n        y_train = train_data[:, 8]\n        x_test = test_data[:, :8]\n        y_test = test_data[:, 8]\n        scaler = StandardScaler()  # 标准化转换\n        scaler.fit(x_train)  # 训练标准化对象\n        x_train = scaler.transform(x_train)\n        scaler.fit(x_test)  # 训练标准化对象\n        x_test = scaler.transform(x_test)\n\n        pred_test = my_adaboost(weak_clf, x_train, x_test, y_train, y_test, epoch)\n        acc.append(accuracy_score(y_test, pred_test))\n        pre.append(precision_score(y_test, pred_test))\n        rec.append(recall_score(y_test, pred_test))\n        f1.append(f1_score(y_test, pred_test))\n\n    # 计算测试集的精度，查准率，查全率，F1\n    print(\"My Adaboost outcome in test set with {} epoch:\".format(epoch))\n    print(\"ACC:\", sum(acc) / 10)\n    print(\"PRE: \", sum(pre) / 10)\n    print(\"REC: \", sum(rec) / 10)\n    print(\"F1: \", sum(f1) / 10)\n```\n\n整体的代码如下：\n```python\n# 自己实现的adaboost\ndef my_adaboost(Weak_clf, X_train, X_test, Y_train, Y_test, Epoch):\n    \"\"\"    :param Weak_clf:    :param X_train:    :param X_test:    :param Y_train:    :param Y_test:    :param Epoch:    :return:    \"\"\"\n    n_train, n_test = len(X_train), len(X_test)\n    W = np.ones(n_train) / n_train  # 样本权重初始化\n    # W = np.ones(n) / n\n    pred_train, pred_test = [np.zeros(n_train), np.zeros(n_test)]\n    # pred = [np.zeros(n)]\n    for i in range(Epoch):\n        # 用特定权重训练分类器\n        Weak_clf.fit(X_train, Y_train, sample_weight=W)\n        pred_train_i = weak_clf.predict(X_train)\n        pred_test_i = weak_clf.predict(X_test)\n        # pred_i = cross_val_predict(Weak_clf, X, Y, cv=10)\n        # 预测不正确的样本数，计算精度\n        miss = [int(x) for x in (pred_train_i != Y_train)]\n        # 在当前权重下计算错误率\n        miss_w = np.dot(W, miss)\n        # 计算alpha\n        alpha = 0.5 * np.log(float(1 - miss_w) / float(miss_w + 0.01))\n        # 权重的系数\n        factor = [x if x == 1 else -1 for x in miss]\n        # 更新样本权重\n        W = np.multiply(W, np.exp([float(x) * alpha for x in factor]))\n        W = W / sum(W)  # normalization\n        # predict\n        pred_train_i = [1 if x == 1 else -1 for x in pred_train_i]\n        # pred_i = [1 if x == 1 else -1 for x in pred_i]\n        pred_test_i = [1 if x == 1 else -1 for x in pred_test_i]\n        pred_test = pred_test + np.multiply(alpha, pred_test_i)\n        pred_train = pred_train + np.multiply(alpha, pred_train_i)\n        # pred = pred + np.multiply(alpha, pred_i)\n    pred_train = (pred_train > 0) * 1\n    pred_test = (pred_test > 0) * 1\n    # pred = (pred > 0) * 1\n    return pred_test\n```","slug":"Adaboost-md","published":1,"updated":"2021-12-20T17:47:11.453Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxez0owb0000ltoehais4djc","content":"<h2 id=\"关于Adaboost\"><a href=\"#关于Adaboost\" class=\"headerlink\" title=\"关于Adaboost\"></a>关于Adaboost</h2><p>Adaboost算法是针对二分类问题提出的集成学习算法，是boosting类算法最著名的代表。当一个学习器的学习的正确率仅比随机猜测的正确率略高，那么就称它是弱学习器，当一个学习期的学习的正确率很高，那么就称它是强学习器。而且发现弱学习器算法要容易得多，这样就需要将弱学习器提升为强学习器。Adaboost的做法是首先选择一个弱学习器，然后进行多轮的训练，但是每一轮训练过后，都要根据当前的错误率去调整训练样本的权重，让预测正确的样本权重降低，预测错误的样本权重增加，从而达到每次训练都是针对上一次预测结果较差的部分进行的，从而训练出一个较强的学习器。</p>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>首先展示理论算法描述。</p>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/IMG_ED06002BA2CA-1.jpeg\">\n\n<p>按照上述步骤开始用代码实现。</p>\n<p>首先是初始化样本权重，初始他们的权重都是相等的，都是样本数分之一。代码如下所示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n_train, n_test = <span class=\"built_in\">len</span>(X_train), <span class=\"built_in\">len</span>(X_test)</span><br><span class=\"line\">W = np.ones(n_train) / n_train  <span class=\"comment\"># 样本权重初始化</span></span><br></pre></td></tr></table></figure>\n<p>然后在规定的训练轮数下，在相应的样本权重下对弱分类器进行训练。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Weak_clf.fit(X_train, Y_train, sample_weight=W)</span><br></pre></td></tr></table></figure>\n<p>然后在测试集进行预测，并且计算出不正确的样本数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"comment\"># 预测不正确的样本数，计算精度</span></span><br><span class=\"line\">miss = [<span class=\"built_in\">int</span>(x) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> (pred_train_i != Y_train)]</span><br><span class=\"line\"><span class=\"comment\"># 在当前权重下计算错误率</span></span><br><span class=\"line\">miss_w = np.dot(W, miss)</span><br></pre></td></tr></table></figure>\n<p>下面就是根据预测的结果，对预测正确的样本权重进行削弱，对预测错误的样本权重进行加强，从而对样本对权重进行更新，用于下一次学习器的训练，公式代码如下所示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算alpha</span></span><br><span class=\"line\">alpha = <span class=\"number\">0.5</span> * np.log(<span class=\"built_in\">float</span>(<span class=\"number\">1</span> - miss_w) / <span class=\"built_in\">float</span>(miss_w + <span class=\"number\">0.01</span>))</span><br><span class=\"line\"><span class=\"comment\"># 权重的系数</span></span><br><span class=\"line\">factor = [x <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> miss]</span><br><span class=\"line\"><span class=\"comment\"># 更新样本权重</span></span><br><span class=\"line\">W = np.multiply(W, np.exp([<span class=\"built_in\">float</span>(x) * alpha <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> factor]))</span><br><span class=\"line\">W = W / <span class=\"built_in\">sum</span>(W)  <span class=\"comment\"># normalization</span></span><br></pre></td></tr></table></figure>\n<p>最终输出的H(x)要对每个也测结果乘alhpa然后加入到结果的列表中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># predict</span></span><br><span class=\"line\">pred_train_i = [<span class=\"number\">1</span> <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> pred_train_i]</span><br><span class=\"line\">pred_test_i = [<span class=\"number\">1</span> <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> pred_test_i]</span><br><span class=\"line\">pred_test = pred_test + np.multiply(alpha, pred_test_i)</span><br></pre></td></tr></table></figure>\n<p>最后对于列表中大于0 的值认为它预测为标签值1，小于0的值认为它预测为比标签值0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pred_test = (pred_test &gt; <span class=\"number\">0</span>) * <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"comment\"># pred = (pred &gt; 0) * 1</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> pred_test</span><br></pre></td></tr></table></figure>\n<p>从而完成了Adaboost的一次训练过程。<br>下面阐述一下我自己实现的十折交叉验证，这里使用的是sklearn的KFold，来对数据集进行十次划分，让每次划分的十个部分轮流做测试集，在同一弱学习器下训练十次，最终预测结果指标是这十次的求和取平均，代码如下所示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weak_clf = DecisionTreeClassifier(criterion=<span class=\"string\">&#x27;entropy&#x27;</span>, max_depth=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"comment\"># 十折交叉验证</span></span><br><span class=\"line\">    acc = []</span><br><span class=\"line\">    pre = []</span><br><span class=\"line\">    rec = []</span><br><span class=\"line\">    f1 = []</span><br><span class=\"line\">    Data = data.copy()</span><br><span class=\"line\">    kf = KFold(n_splits=<span class=\"number\">10</span>, shuffle=<span class=\"literal\">True</span>, random_state=<span class=\"number\">0</span>)  <span class=\"comment\"># 10折</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> train_index, test_index <span class=\"keyword\">in</span> tqdm(kf.split(Data)):  <span class=\"comment\"># 将数据划分为10折</span></span><br><span class=\"line\">        train_data = Data[train_index]  <span class=\"comment\"># 选取的训练集数据下标</span></span><br><span class=\"line\">        test_data = Data[test_index]  <span class=\"comment\"># 选取的测试集数据下标</span></span><br><span class=\"line\">        x_train = train_data[:, :<span class=\"number\">8</span>]</span><br><span class=\"line\">        y_train = train_data[:, <span class=\"number\">8</span>]</span><br><span class=\"line\">        x_test = test_data[:, :<span class=\"number\">8</span>]</span><br><span class=\"line\">        y_test = test_data[:, <span class=\"number\">8</span>]</span><br><span class=\"line\">        scaler = StandardScaler()  <span class=\"comment\"># 标准化转换</span></span><br><span class=\"line\">        scaler.fit(x_train)  <span class=\"comment\"># 训练标准化对象</span></span><br><span class=\"line\">        x_train = scaler.transform(x_train)</span><br><span class=\"line\">        scaler.fit(x_test)  <span class=\"comment\"># 训练标准化对象</span></span><br><span class=\"line\">        x_test = scaler.transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">        pred_test = my_adaboost(weak_clf, x_train, x_test, y_train, y_test, epoch)</span><br><span class=\"line\">        acc.append(accuracy_score(y_test, pred_test))</span><br><span class=\"line\">        pre.append(precision_score(y_test, pred_test))</span><br><span class=\"line\">        rec.append(recall_score(y_test, pred_test))</span><br><span class=\"line\">        f1.append(f1_score(y_test, pred_test))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 计算测试集的精度，查准率，查全率，F1</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;My Adaboost outcome in test set with &#123;&#125; epoch:&quot;</span>.<span class=\"built_in\">format</span>(epoch))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;ACC:&quot;</span>, <span class=\"built_in\">sum</span>(acc) / <span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;PRE: &quot;</span>, <span class=\"built_in\">sum</span>(pre) / <span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;REC: &quot;</span>, <span class=\"built_in\">sum</span>(rec) / <span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;F1: &quot;</span>, <span class=\"built_in\">sum</span>(f1) / <span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n\n<p>整体的代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 自己实现的adaboost</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">my_adaboost</span>(<span class=\"params\">Weak_clf, X_train, X_test, Y_train, Y_test, Epoch</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;    :param Weak_clf:    :param X_train:    :param X_test:    :param Y_train:    :param Y_test:    :param Epoch:    :return:    &quot;&quot;&quot;</span></span><br><span class=\"line\">    n_train, n_test = <span class=\"built_in\">len</span>(X_train), <span class=\"built_in\">len</span>(X_test)</span><br><span class=\"line\">    W = np.ones(n_train) / n_train  <span class=\"comment\"># 样本权重初始化</span></span><br><span class=\"line\">    <span class=\"comment\"># W = np.ones(n) / n</span></span><br><span class=\"line\">    pred_train, pred_test = [np.zeros(n_train), np.zeros(n_test)]</span><br><span class=\"line\">    <span class=\"comment\"># pred = [np.zeros(n)]</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Epoch):</span><br><span class=\"line\">        <span class=\"comment\"># 用特定权重训练分类器</span></span><br><span class=\"line\">        Weak_clf.fit(X_train, Y_train, sample_weight=W)</span><br><span class=\"line\">        pred_train_i = weak_clf.predict(X_train)</span><br><span class=\"line\">        pred_test_i = weak_clf.predict(X_test)</span><br><span class=\"line\">        <span class=\"comment\"># pred_i = cross_val_predict(Weak_clf, X, Y, cv=10)</span></span><br><span class=\"line\">        <span class=\"comment\"># 预测不正确的样本数，计算精度</span></span><br><span class=\"line\">        miss = [<span class=\"built_in\">int</span>(x) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> (pred_train_i != Y_train)]</span><br><span class=\"line\">        <span class=\"comment\"># 在当前权重下计算错误率</span></span><br><span class=\"line\">        miss_w = np.dot(W, miss)</span><br><span class=\"line\">        <span class=\"comment\"># 计算alpha</span></span><br><span class=\"line\">        alpha = <span class=\"number\">0.5</span> * np.log(<span class=\"built_in\">float</span>(<span class=\"number\">1</span> - miss_w) / <span class=\"built_in\">float</span>(miss_w + <span class=\"number\">0.01</span>))</span><br><span class=\"line\">        <span class=\"comment\"># 权重的系数</span></span><br><span class=\"line\">        factor = [x <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> miss]</span><br><span class=\"line\">        <span class=\"comment\"># 更新样本权重</span></span><br><span class=\"line\">        W = np.multiply(W, np.exp([<span class=\"built_in\">float</span>(x) * alpha <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> factor]))</span><br><span class=\"line\">        W = W / <span class=\"built_in\">sum</span>(W)  <span class=\"comment\"># normalization</span></span><br><span class=\"line\">        <span class=\"comment\"># predict</span></span><br><span class=\"line\">        pred_train_i = [<span class=\"number\">1</span> <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> pred_train_i]</span><br><span class=\"line\">        <span class=\"comment\"># pred_i = [1 if x == 1 else -1 for x in pred_i]</span></span><br><span class=\"line\">        pred_test_i = [<span class=\"number\">1</span> <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> pred_test_i]</span><br><span class=\"line\">        pred_test = pred_test + np.multiply(alpha, pred_test_i)</span><br><span class=\"line\">        pred_train = pred_train + np.multiply(alpha, pred_train_i)</span><br><span class=\"line\">        <span class=\"comment\"># pred = pred + np.multiply(alpha, pred_i)</span></span><br><span class=\"line\">    pred_train = (pred_train &gt; <span class=\"number\">0</span>) * <span class=\"number\">1</span></span><br><span class=\"line\">    pred_test = (pred_test &gt; <span class=\"number\">0</span>) * <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># pred = (pred &gt; 0) * 1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> pred_test</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"关于Adaboost\"><a href=\"#关于Adaboost\" class=\"headerlink\" title=\"关于Adaboost\"></a>关于Adaboost</h2><p>Adaboost算法是针对二分类问题提出的集成学习算法，是boosting类算法最著名的代表。当一个学习器的学习的正确率仅比随机猜测的正确率略高，那么就称它是弱学习器，当一个学习期的学习的正确率很高，那么就称它是强学习器。而且发现弱学习器算法要容易得多，这样就需要将弱学习器提升为强学习器。Adaboost的做法是首先选择一个弱学习器，然后进行多轮的训练，但是每一轮训练过后，都要根据当前的错误率去调整训练样本的权重，让预测正确的样本权重降低，预测错误的样本权重增加，从而达到每次训练都是针对上一次预测结果较差的部分进行的，从而训练出一个较强的学习器。</p>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>首先展示理论算法描述。</p>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/IMG_ED06002BA2CA-1.jpeg\">\n\n<p>按照上述步骤开始用代码实现。</p>\n<p>首先是初始化样本权重，初始他们的权重都是相等的，都是样本数分之一。代码如下所示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n_train, n_test = <span class=\"built_in\">len</span>(X_train), <span class=\"built_in\">len</span>(X_test)</span><br><span class=\"line\">W = np.ones(n_train) / n_train  <span class=\"comment\"># 样本权重初始化</span></span><br></pre></td></tr></table></figure>\n<p>然后在规定的训练轮数下，在相应的样本权重下对弱分类器进行训练。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Weak_clf.fit(X_train, Y_train, sample_weight=W)</span><br></pre></td></tr></table></figure>\n<p>然后在测试集进行预测，并且计算出不正确的样本数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"comment\"># 预测不正确的样本数，计算精度</span></span><br><span class=\"line\">miss = [<span class=\"built_in\">int</span>(x) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> (pred_train_i != Y_train)]</span><br><span class=\"line\"><span class=\"comment\"># 在当前权重下计算错误率</span></span><br><span class=\"line\">miss_w = np.dot(W, miss)</span><br></pre></td></tr></table></figure>\n<p>下面就是根据预测的结果，对预测正确的样本权重进行削弱，对预测错误的样本权重进行加强，从而对样本对权重进行更新，用于下一次学习器的训练，公式代码如下所示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算alpha</span></span><br><span class=\"line\">alpha = <span class=\"number\">0.5</span> * np.log(<span class=\"built_in\">float</span>(<span class=\"number\">1</span> - miss_w) / <span class=\"built_in\">float</span>(miss_w + <span class=\"number\">0.01</span>))</span><br><span class=\"line\"><span class=\"comment\"># 权重的系数</span></span><br><span class=\"line\">factor = [x <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> miss]</span><br><span class=\"line\"><span class=\"comment\"># 更新样本权重</span></span><br><span class=\"line\">W = np.multiply(W, np.exp([<span class=\"built_in\">float</span>(x) * alpha <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> factor]))</span><br><span class=\"line\">W = W / <span class=\"built_in\">sum</span>(W)  <span class=\"comment\"># normalization</span></span><br></pre></td></tr></table></figure>\n<p>最终输出的H(x)要对每个也测结果乘alhpa然后加入到结果的列表中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># predict</span></span><br><span class=\"line\">pred_train_i = [<span class=\"number\">1</span> <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> pred_train_i]</span><br><span class=\"line\">pred_test_i = [<span class=\"number\">1</span> <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> pred_test_i]</span><br><span class=\"line\">pred_test = pred_test + np.multiply(alpha, pred_test_i)</span><br></pre></td></tr></table></figure>\n<p>最后对于列表中大于0 的值认为它预测为标签值1，小于0的值认为它预测为比标签值0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pred_test = (pred_test &gt; <span class=\"number\">0</span>) * <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"comment\"># pred = (pred &gt; 0) * 1</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> pred_test</span><br></pre></td></tr></table></figure>\n<p>从而完成了Adaboost的一次训练过程。<br>下面阐述一下我自己实现的十折交叉验证，这里使用的是sklearn的KFold，来对数据集进行十次划分，让每次划分的十个部分轮流做测试集，在同一弱学习器下训练十次，最终预测结果指标是这十次的求和取平均，代码如下所示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weak_clf = DecisionTreeClassifier(criterion=<span class=\"string\">&#x27;entropy&#x27;</span>, max_depth=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"comment\"># 十折交叉验证</span></span><br><span class=\"line\">    acc = []</span><br><span class=\"line\">    pre = []</span><br><span class=\"line\">    rec = []</span><br><span class=\"line\">    f1 = []</span><br><span class=\"line\">    Data = data.copy()</span><br><span class=\"line\">    kf = KFold(n_splits=<span class=\"number\">10</span>, shuffle=<span class=\"literal\">True</span>, random_state=<span class=\"number\">0</span>)  <span class=\"comment\"># 10折</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> train_index, test_index <span class=\"keyword\">in</span> tqdm(kf.split(Data)):  <span class=\"comment\"># 将数据划分为10折</span></span><br><span class=\"line\">        train_data = Data[train_index]  <span class=\"comment\"># 选取的训练集数据下标</span></span><br><span class=\"line\">        test_data = Data[test_index]  <span class=\"comment\"># 选取的测试集数据下标</span></span><br><span class=\"line\">        x_train = train_data[:, :<span class=\"number\">8</span>]</span><br><span class=\"line\">        y_train = train_data[:, <span class=\"number\">8</span>]</span><br><span class=\"line\">        x_test = test_data[:, :<span class=\"number\">8</span>]</span><br><span class=\"line\">        y_test = test_data[:, <span class=\"number\">8</span>]</span><br><span class=\"line\">        scaler = StandardScaler()  <span class=\"comment\"># 标准化转换</span></span><br><span class=\"line\">        scaler.fit(x_train)  <span class=\"comment\"># 训练标准化对象</span></span><br><span class=\"line\">        x_train = scaler.transform(x_train)</span><br><span class=\"line\">        scaler.fit(x_test)  <span class=\"comment\"># 训练标准化对象</span></span><br><span class=\"line\">        x_test = scaler.transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">        pred_test = my_adaboost(weak_clf, x_train, x_test, y_train, y_test, epoch)</span><br><span class=\"line\">        acc.append(accuracy_score(y_test, pred_test))</span><br><span class=\"line\">        pre.append(precision_score(y_test, pred_test))</span><br><span class=\"line\">        rec.append(recall_score(y_test, pred_test))</span><br><span class=\"line\">        f1.append(f1_score(y_test, pred_test))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 计算测试集的精度，查准率，查全率，F1</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;My Adaboost outcome in test set with &#123;&#125; epoch:&quot;</span>.<span class=\"built_in\">format</span>(epoch))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;ACC:&quot;</span>, <span class=\"built_in\">sum</span>(acc) / <span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;PRE: &quot;</span>, <span class=\"built_in\">sum</span>(pre) / <span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;REC: &quot;</span>, <span class=\"built_in\">sum</span>(rec) / <span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;F1: &quot;</span>, <span class=\"built_in\">sum</span>(f1) / <span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n\n<p>整体的代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 自己实现的adaboost</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">my_adaboost</span>(<span class=\"params\">Weak_clf, X_train, X_test, Y_train, Y_test, Epoch</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;    :param Weak_clf:    :param X_train:    :param X_test:    :param Y_train:    :param Y_test:    :param Epoch:    :return:    &quot;&quot;&quot;</span></span><br><span class=\"line\">    n_train, n_test = <span class=\"built_in\">len</span>(X_train), <span class=\"built_in\">len</span>(X_test)</span><br><span class=\"line\">    W = np.ones(n_train) / n_train  <span class=\"comment\"># 样本权重初始化</span></span><br><span class=\"line\">    <span class=\"comment\"># W = np.ones(n) / n</span></span><br><span class=\"line\">    pred_train, pred_test = [np.zeros(n_train), np.zeros(n_test)]</span><br><span class=\"line\">    <span class=\"comment\"># pred = [np.zeros(n)]</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Epoch):</span><br><span class=\"line\">        <span class=\"comment\"># 用特定权重训练分类器</span></span><br><span class=\"line\">        Weak_clf.fit(X_train, Y_train, sample_weight=W)</span><br><span class=\"line\">        pred_train_i = weak_clf.predict(X_train)</span><br><span class=\"line\">        pred_test_i = weak_clf.predict(X_test)</span><br><span class=\"line\">        <span class=\"comment\"># pred_i = cross_val_predict(Weak_clf, X, Y, cv=10)</span></span><br><span class=\"line\">        <span class=\"comment\"># 预测不正确的样本数，计算精度</span></span><br><span class=\"line\">        miss = [<span class=\"built_in\">int</span>(x) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> (pred_train_i != Y_train)]</span><br><span class=\"line\">        <span class=\"comment\"># 在当前权重下计算错误率</span></span><br><span class=\"line\">        miss_w = np.dot(W, miss)</span><br><span class=\"line\">        <span class=\"comment\"># 计算alpha</span></span><br><span class=\"line\">        alpha = <span class=\"number\">0.5</span> * np.log(<span class=\"built_in\">float</span>(<span class=\"number\">1</span> - miss_w) / <span class=\"built_in\">float</span>(miss_w + <span class=\"number\">0.01</span>))</span><br><span class=\"line\">        <span class=\"comment\"># 权重的系数</span></span><br><span class=\"line\">        factor = [x <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> miss]</span><br><span class=\"line\">        <span class=\"comment\"># 更新样本权重</span></span><br><span class=\"line\">        W = np.multiply(W, np.exp([<span class=\"built_in\">float</span>(x) * alpha <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> factor]))</span><br><span class=\"line\">        W = W / <span class=\"built_in\">sum</span>(W)  <span class=\"comment\"># normalization</span></span><br><span class=\"line\">        <span class=\"comment\"># predict</span></span><br><span class=\"line\">        pred_train_i = [<span class=\"number\">1</span> <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> pred_train_i]</span><br><span class=\"line\">        <span class=\"comment\"># pred_i = [1 if x == 1 else -1 for x in pred_i]</span></span><br><span class=\"line\">        pred_test_i = [<span class=\"number\">1</span> <span class=\"keyword\">if</span> x == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> pred_test_i]</span><br><span class=\"line\">        pred_test = pred_test + np.multiply(alpha, pred_test_i)</span><br><span class=\"line\">        pred_train = pred_train + np.multiply(alpha, pred_train_i)</span><br><span class=\"line\">        <span class=\"comment\"># pred = pred + np.multiply(alpha, pred_i)</span></span><br><span class=\"line\">    pred_train = (pred_train &gt; <span class=\"number\">0</span>) * <span class=\"number\">1</span></span><br><span class=\"line\">    pred_test = (pred_test &gt; <span class=\"number\">0</span>) * <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># pred = (pred &gt; 0) * 1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> pred_test</span><br></pre></td></tr></table></figure>"},{"title":"Mandelbrot Set","date":"2022-01-13T07:39:18.000Z","_content":"\n## Mandelbrot Set with HPC\n\n### **本课程设计任务**\n\n使用MPI与OpenMP混合并行编程模型，计算一定范围复平面内属于Mandelbrot Set的元素。\n\n将该范围复平面内的元素C是否属于Mandelbrot Set的信息，映射到图片中进行可视化。\n\n调整并行进程数以及每个进程内的并行线程数，探究计算效率的变化情况。\n\n调整数据规模，探究计算效率的变化情况。\n\n### **计算策略**\n\n$$ z_{n+1} = z^2_{n}+c $$\n\n根据该公式，可以对某一个复平面的元素C进行迭代，从而得到z的数列，当该数列收敛，则该C属于Mandelbrot Set。\n但是计算机无法处理无限迭代的问题，于是我们给出一个Limit，当迭代Limit次数列还没有发散时，就将对应的C近似列入Mandelbrot Set。\n\n有文献证明，当|z| > 2时，z会迅速发散，所以这里我们近似地认为当出现模大于2的z时，该C下z的迭代数列发散。\n\n![](https://img-blog.csdnimg.cn/img_convert/6346d892433f5b2f6aa1d5ddf3d32475.png)\n\n将复平面内所有的C分为两类：\n\n    （1）迭代次数达到Limit还数列没有发散的。\n    （2）迭代次数达到n（<Limit）数列就已经发散的。\n\n用矩阵记录下它们的类型，（1）记录为0，（2）记录为发散前最大迭代次数。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code2.png\" width = \"500\" height = \"170\" div align=center /></center>\n\n### **可视化策略**\n\n根据示例，我们的输出图片设置为长宽比4:3。\n\n要将复平面的点映射到图片的坐标系中，映射的策略如下。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1297.jpeg\" width = \"600\" height = \"300\" div align=center /></center>\n\n![](https://img-blog.csdnimg.cn/img_convert/2812d4a4f13b074d39f2c64d15395b99.png)\n\n这里除的目的是将c的虚部限制在(-1,1)，实部限制在(-2,1)。\n\n### **并行优化策略**\n\n*优化策略（1）：*\n\n使用多进程并行，每个进程内使用多线程并行。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1304.jpeg\" width = \"500\" height = \"300\" div align=center /></center>\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code4.png\" width = \"600\" height = \"240\" div align=center /></center>\n\n*优化策略（2）：*\n    \n利用集合的对称性质，进行对称优化。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code5.jpeg\" width = \"500\" height = \"200\" div align=center /></center>\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/mandelbrot.png\" width = \"400\" height = \"300\" div align=center /></center>\n\n### **计算效率探究实验**\n\n  实验分为三部分：\n\n（1）无对称优化和有对称优化下分别探究并行进程数、线程数的变化对计算效率的影响。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1305.jpeg\" width = \"600\" height = \"450\" div align=center /></center>\n\n结论如下：\n\n    （1）在一定的并行进程数下，每个进程内的并行线程数不能一昧增加，受硬件资源CPU核数的限制，会存在一个能取得最好优化效果的最大并行线程数，这个值与当前的并行进程数相关，说明并行进程和并行线程二者占用的硬件资源是相互影响的，超过硬件资源限制的进程数或线程数请求都会起到反作用。\n\n    （2）在硬件资源限制内调用时，随着每个进程内并行线程数的增加，并行进程数的增加对计算时间消耗的优化效果越来越弱。\n\n    （3）与（2）对应的，随着并行进程数的增加，并行线程数的增加对计算时间消耗的影响越来越弱。\n\n    （4）通过实验数据来看，可以分析出每台机器在该数据规模下最多支持32线程并行达到效率最优，再增加调用的并行线程会起到反作用。\n\n    （5）线程更集中的情况下效率更优\n\n\n（2）不同并行进程数、线程数下对称优化对计算效率的影响\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1306.jpeg\" width = \"400\" height = \"300\" div align=center /></center>\n\n结论如下：\n\n    当硬件资源占用较少时数据规模对时间消耗的优化较大，但是当硬件资源占用比较紧张（没有超过资源限制）的情况下，数据规模对时间消耗的优化甚微，甚至有时还会起到副作用。这个原因我想是因为在硬件资源紧张时数据规模的减小会使得资源调度时间占并行计算时间的比例增加，从而导致优化效果不好。\n\n（3）在并行优化最好的资源调用下，数据规模对计算效率的影响。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1307.jpeg\" width = \"400\" height = \"300\" div align=center /></center>\n\n结论如下：\n\n    数据规模越大，计算的时间消耗增长越快。原因是当数据规模过大，会导致硬件资源的利用率达到最大，从而无法更多的发挥并行计算的优势。\n\n### **实验输出结果**\n\n这里抛开性能不谈，观察随数据规模的增加，Mandelbrot Set图片分辨率的变化情况。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1308.jpeg\" width = \"600\" height = \"300\" div align=center /></center>\n\n### **存在的问题**\n\n（1）硬件资源调度时间消耗与并行计算带来的效率提高二者间的平衡情况与数据规模的关系没有体现。\n\n（2）在数据规模对计算效率的影响探究中，数据规模的选择有些不科学。\n","source":"_posts/HPC-mandelbrot-md.md","raw":"\n---\ntitle: Mandelbrot Set\ndate: 2022-01-13 15:39:18\ntags: 高性能计算\n---\n\n## Mandelbrot Set with HPC\n\n### **本课程设计任务**\n\n使用MPI与OpenMP混合并行编程模型，计算一定范围复平面内属于Mandelbrot Set的元素。\n\n将该范围复平面内的元素C是否属于Mandelbrot Set的信息，映射到图片中进行可视化。\n\n调整并行进程数以及每个进程内的并行线程数，探究计算效率的变化情况。\n\n调整数据规模，探究计算效率的变化情况。\n\n### **计算策略**\n\n$$ z_{n+1} = z^2_{n}+c $$\n\n根据该公式，可以对某一个复平面的元素C进行迭代，从而得到z的数列，当该数列收敛，则该C属于Mandelbrot Set。\n但是计算机无法处理无限迭代的问题，于是我们给出一个Limit，当迭代Limit次数列还没有发散时，就将对应的C近似列入Mandelbrot Set。\n\n有文献证明，当|z| > 2时，z会迅速发散，所以这里我们近似地认为当出现模大于2的z时，该C下z的迭代数列发散。\n\n![](https://img-blog.csdnimg.cn/img_convert/6346d892433f5b2f6aa1d5ddf3d32475.png)\n\n将复平面内所有的C分为两类：\n\n    （1）迭代次数达到Limit还数列没有发散的。\n    （2）迭代次数达到n（<Limit）数列就已经发散的。\n\n用矩阵记录下它们的类型，（1）记录为0，（2）记录为发散前最大迭代次数。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code2.png\" width = \"500\" height = \"170\" div align=center /></center>\n\n### **可视化策略**\n\n根据示例，我们的输出图片设置为长宽比4:3。\n\n要将复平面的点映射到图片的坐标系中，映射的策略如下。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1297.jpeg\" width = \"600\" height = \"300\" div align=center /></center>\n\n![](https://img-blog.csdnimg.cn/img_convert/2812d4a4f13b074d39f2c64d15395b99.png)\n\n这里除的目的是将c的虚部限制在(-1,1)，实部限制在(-2,1)。\n\n### **并行优化策略**\n\n*优化策略（1）：*\n\n使用多进程并行，每个进程内使用多线程并行。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1304.jpeg\" width = \"500\" height = \"300\" div align=center /></center>\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code4.png\" width = \"600\" height = \"240\" div align=center /></center>\n\n*优化策略（2）：*\n    \n利用集合的对称性质，进行对称优化。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code5.jpeg\" width = \"500\" height = \"200\" div align=center /></center>\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/mandelbrot.png\" width = \"400\" height = \"300\" div align=center /></center>\n\n### **计算效率探究实验**\n\n  实验分为三部分：\n\n（1）无对称优化和有对称优化下分别探究并行进程数、线程数的变化对计算效率的影响。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1305.jpeg\" width = \"600\" height = \"450\" div align=center /></center>\n\n结论如下：\n\n    （1）在一定的并行进程数下，每个进程内的并行线程数不能一昧增加，受硬件资源CPU核数的限制，会存在一个能取得最好优化效果的最大并行线程数，这个值与当前的并行进程数相关，说明并行进程和并行线程二者占用的硬件资源是相互影响的，超过硬件资源限制的进程数或线程数请求都会起到反作用。\n\n    （2）在硬件资源限制内调用时，随着每个进程内并行线程数的增加，并行进程数的增加对计算时间消耗的优化效果越来越弱。\n\n    （3）与（2）对应的，随着并行进程数的增加，并行线程数的增加对计算时间消耗的影响越来越弱。\n\n    （4）通过实验数据来看，可以分析出每台机器在该数据规模下最多支持32线程并行达到效率最优，再增加调用的并行线程会起到反作用。\n\n    （5）线程更集中的情况下效率更优\n\n\n（2）不同并行进程数、线程数下对称优化对计算效率的影响\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1306.jpeg\" width = \"400\" height = \"300\" div align=center /></center>\n\n结论如下：\n\n    当硬件资源占用较少时数据规模对时间消耗的优化较大，但是当硬件资源占用比较紧张（没有超过资源限制）的情况下，数据规模对时间消耗的优化甚微，甚至有时还会起到副作用。这个原因我想是因为在硬件资源紧张时数据规模的减小会使得资源调度时间占并行计算时间的比例增加，从而导致优化效果不好。\n\n（3）在并行优化最好的资源调用下，数据规模对计算效率的影响。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1307.jpeg\" width = \"400\" height = \"300\" div align=center /></center>\n\n结论如下：\n\n    数据规模越大，计算的时间消耗增长越快。原因是当数据规模过大，会导致硬件资源的利用率达到最大，从而无法更多的发挥并行计算的优势。\n\n### **实验输出结果**\n\n这里抛开性能不谈，观察随数据规模的增加，Mandelbrot Set图片分辨率的变化情况。\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1308.jpeg\" width = \"600\" height = \"300\" div align=center /></center>\n\n### **存在的问题**\n\n（1）硬件资源调度时间消耗与并行计算带来的效率提高二者间的平衡情况与数据规模的关系没有体现。\n\n（2）在数据规模对计算效率的影响探究中，数据规模的选择有些不科学。\n","slug":"HPC-mandelbrot-md","published":1,"updated":"2022-01-13T10:17:38.719Z","_id":"ckyct2g0u0000n2oe3qsb4ise","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Mandelbrot-Set-with-HPC\"><a href=\"#Mandelbrot-Set-with-HPC\" class=\"headerlink\" title=\"Mandelbrot Set with HPC\"></a>Mandelbrot Set with HPC</h2><h3 id=\"本课程设计任务\"><a href=\"#本课程设计任务\" class=\"headerlink\" title=\"本课程设计任务\"></a><strong>本课程设计任务</strong></h3><p>使用MPI与OpenMP混合并行编程模型，计算一定范围复平面内属于Mandelbrot Set的元素。</p>\n<p>将该范围复平面内的元素C是否属于Mandelbrot Set的信息，映射到图片中进行可视化。</p>\n<p>调整并行进程数以及每个进程内的并行线程数，探究计算效率的变化情况。</p>\n<p>调整数据规模，探究计算效率的变化情况。</p>\n<h3 id=\"计算策略\"><a href=\"#计算策略\" class=\"headerlink\" title=\"计算策略\"></a><strong>计算策略</strong></h3><p>$$ z_{n+1} = z^2_{n}+c $$</p>\n<p>根据该公式，可以对某一个复平面的元素C进行迭代，从而得到z的数列，当该数列收敛，则该C属于Mandelbrot Set。<br>但是计算机无法处理无限迭代的问题，于是我们给出一个Limit，当迭代Limit次数列还没有发散时，就将对应的C近似列入Mandelbrot Set。</p>\n<p>有文献证明，当|z| &gt; 2时，z会迅速发散，所以这里我们近似地认为当出现模大于2的z时，该C下z的迭代数列发散。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/img_convert/6346d892433f5b2f6aa1d5ddf3d32475.png\"></p>\n<p>将复平面内所有的C分为两类：</p>\n<pre><code>（1）迭代次数达到Limit还数列没有发散的。\n（2）迭代次数达到n（&lt;Limit）数列就已经发散的。\n</code></pre>\n<p>用矩阵记录下它们的类型，（1）记录为0，（2）记录为发散前最大迭代次数。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code2.png\" width = \"500\" height = \"170\" div align=center /></center>\n\n<h3 id=\"可视化策略\"><a href=\"#可视化策略\" class=\"headerlink\" title=\"可视化策略\"></a><strong>可视化策略</strong></h3><p>根据示例，我们的输出图片设置为长宽比4:3。</p>\n<p>要将复平面的点映射到图片的坐标系中，映射的策略如下。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1297.jpeg\" width = \"600\" height = \"300\" div align=center /></center>\n\n<p><img src=\"https://img-blog.csdnimg.cn/img_convert/2812d4a4f13b074d39f2c64d15395b99.png\"></p>\n<p>这里除的目的是将c的虚部限制在(-1,1)，实部限制在(-2,1)。</p>\n<h3 id=\"并行优化策略\"><a href=\"#并行优化策略\" class=\"headerlink\" title=\"并行优化策略\"></a><strong>并行优化策略</strong></h3><p><em>优化策略（1）：</em></p>\n<p>使用多进程并行，每个进程内使用多线程并行。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1304.jpeg\" width = \"500\" height = \"300\" div align=center /></center>\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code4.png\" width = \"600\" height = \"240\" div align=center /></center>\n\n<p><em>优化策略（2）：</em></p>\n<p>利用集合的对称性质，进行对称优化。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code5.jpeg\" width = \"500\" height = \"200\" div align=center /></center>\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/mandelbrot.png\" width = \"400\" height = \"300\" div align=center /></center>\n\n<h3 id=\"计算效率探究实验\"><a href=\"#计算效率探究实验\" class=\"headerlink\" title=\"计算效率探究实验\"></a><strong>计算效率探究实验</strong></h3><p>  实验分为三部分：</p>\n<p>（1）无对称优化和有对称优化下分别探究并行进程数、线程数的变化对计算效率的影响。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1305.jpeg\" width = \"600\" height = \"450\" div align=center /></center>\n\n<p>结论如下：</p>\n<pre><code>（1）在一定的并行进程数下，每个进程内的并行线程数不能一昧增加，受硬件资源CPU核数的限制，会存在一个能取得最好优化效果的最大并行线程数，这个值与当前的并行进程数相关，说明并行进程和并行线程二者占用的硬件资源是相互影响的，超过硬件资源限制的进程数或线程数请求都会起到反作用。\n\n（2）在硬件资源限制内调用时，随着每个进程内并行线程数的增加，并行进程数的增加对计算时间消耗的优化效果越来越弱。\n\n（3）与（2）对应的，随着并行进程数的增加，并行线程数的增加对计算时间消耗的影响越来越弱。\n\n（4）通过实验数据来看，可以分析出每台机器在该数据规模下最多支持32线程并行达到效率最优，再增加调用的并行线程会起到反作用。\n\n（5）线程更集中的情况下效率更优\n</code></pre>\n<p>（2）不同并行进程数、线程数下对称优化对计算效率的影响</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1306.jpeg\" width = \"400\" height = \"300\" div align=center /></center>\n\n<p>结论如下：</p>\n<pre><code>当硬件资源占用较少时数据规模对时间消耗的优化较大，但是当硬件资源占用比较紧张（没有超过资源限制）的情况下，数据规模对时间消耗的优化甚微，甚至有时还会起到副作用。这个原因我想是因为在硬件资源紧张时数据规模的减小会使得资源调度时间占并行计算时间的比例增加，从而导致优化效果不好。\n</code></pre>\n<p>（3）在并行优化最好的资源调用下，数据规模对计算效率的影响。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1307.jpeg\" width = \"400\" height = \"300\" div align=center /></center>\n\n<p>结论如下：</p>\n<pre><code>数据规模越大，计算的时间消耗增长越快。原因是当数据规模过大，会导致硬件资源的利用率达到最大，从而无法更多的发挥并行计算的优势。\n</code></pre>\n<h3 id=\"实验输出结果\"><a href=\"#实验输出结果\" class=\"headerlink\" title=\"实验输出结果\"></a><strong>实验输出结果</strong></h3><p>这里抛开性能不谈，观察随数据规模的增加，Mandelbrot Set图片分辨率的变化情况。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1308.jpeg\" width = \"600\" height = \"300\" div align=center /></center>\n\n<h3 id=\"存在的问题\"><a href=\"#存在的问题\" class=\"headerlink\" title=\"存在的问题\"></a><strong>存在的问题</strong></h3><p>（1）硬件资源调度时间消耗与并行计算带来的效率提高二者间的平衡情况与数据规模的关系没有体现。</p>\n<p>（2）在数据规模对计算效率的影响探究中，数据规模的选择有些不科学。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Mandelbrot-Set-with-HPC\"><a href=\"#Mandelbrot-Set-with-HPC\" class=\"headerlink\" title=\"Mandelbrot Set with HPC\"></a>Mandelbrot Set with HPC</h2><h3 id=\"本课程设计任务\"><a href=\"#本课程设计任务\" class=\"headerlink\" title=\"本课程设计任务\"></a><strong>本课程设计任务</strong></h3><p>使用MPI与OpenMP混合并行编程模型，计算一定范围复平面内属于Mandelbrot Set的元素。</p>\n<p>将该范围复平面内的元素C是否属于Mandelbrot Set的信息，映射到图片中进行可视化。</p>\n<p>调整并行进程数以及每个进程内的并行线程数，探究计算效率的变化情况。</p>\n<p>调整数据规模，探究计算效率的变化情况。</p>\n<h3 id=\"计算策略\"><a href=\"#计算策略\" class=\"headerlink\" title=\"计算策略\"></a><strong>计算策略</strong></h3><p>$$ z_{n+1} = z^2_{n}+c $$</p>\n<p>根据该公式，可以对某一个复平面的元素C进行迭代，从而得到z的数列，当该数列收敛，则该C属于Mandelbrot Set。<br>但是计算机无法处理无限迭代的问题，于是我们给出一个Limit，当迭代Limit次数列还没有发散时，就将对应的C近似列入Mandelbrot Set。</p>\n<p>有文献证明，当|z| &gt; 2时，z会迅速发散，所以这里我们近似地认为当出现模大于2的z时，该C下z的迭代数列发散。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/img_convert/6346d892433f5b2f6aa1d5ddf3d32475.png\"></p>\n<p>将复平面内所有的C分为两类：</p>\n<pre><code>（1）迭代次数达到Limit还数列没有发散的。\n（2）迭代次数达到n（&lt;Limit）数列就已经发散的。\n</code></pre>\n<p>用矩阵记录下它们的类型，（1）记录为0，（2）记录为发散前最大迭代次数。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code2.png\" width = \"500\" height = \"170\" div align=center /></center>\n\n<h3 id=\"可视化策略\"><a href=\"#可视化策略\" class=\"headerlink\" title=\"可视化策略\"></a><strong>可视化策略</strong></h3><p>根据示例，我们的输出图片设置为长宽比4:3。</p>\n<p>要将复平面的点映射到图片的坐标系中，映射的策略如下。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1297.jpeg\" width = \"600\" height = \"300\" div align=center /></center>\n\n<p><img src=\"https://img-blog.csdnimg.cn/img_convert/2812d4a4f13b074d39f2c64d15395b99.png\"></p>\n<p>这里除的目的是将c的虚部限制在(-1,1)，实部限制在(-2,1)。</p>\n<h3 id=\"并行优化策略\"><a href=\"#并行优化策略\" class=\"headerlink\" title=\"并行优化策略\"></a><strong>并行优化策略</strong></h3><p><em>优化策略（1）：</em></p>\n<p>使用多进程并行，每个进程内使用多线程并行。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1304.jpeg\" width = \"500\" height = \"300\" div align=center /></center>\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code4.png\" width = \"600\" height = \"240\" div align=center /></center>\n\n<p><em>优化策略（2）：</em></p>\n<p>利用集合的对称性质，进行对称优化。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/code5.jpeg\" width = \"500\" height = \"200\" div align=center /></center>\n\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/mandelbrot.png\" width = \"400\" height = \"300\" div align=center /></center>\n\n<h3 id=\"计算效率探究实验\"><a href=\"#计算效率探究实验\" class=\"headerlink\" title=\"计算效率探究实验\"></a><strong>计算效率探究实验</strong></h3><p>  实验分为三部分：</p>\n<p>（1）无对称优化和有对称优化下分别探究并行进程数、线程数的变化对计算效率的影响。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1305.jpeg\" width = \"600\" height = \"450\" div align=center /></center>\n\n<p>结论如下：</p>\n<pre><code>（1）在一定的并行进程数下，每个进程内的并行线程数不能一昧增加，受硬件资源CPU核数的限制，会存在一个能取得最好优化效果的最大并行线程数，这个值与当前的并行进程数相关，说明并行进程和并行线程二者占用的硬件资源是相互影响的，超过硬件资源限制的进程数或线程数请求都会起到反作用。\n\n（2）在硬件资源限制内调用时，随着每个进程内并行线程数的增加，并行进程数的增加对计算时间消耗的优化效果越来越弱。\n\n（3）与（2）对应的，随着并行进程数的增加，并行线程数的增加对计算时间消耗的影响越来越弱。\n\n（4）通过实验数据来看，可以分析出每台机器在该数据规模下最多支持32线程并行达到效率最优，再增加调用的并行线程会起到反作用。\n\n（5）线程更集中的情况下效率更优\n</code></pre>\n<p>（2）不同并行进程数、线程数下对称优化对计算效率的影响</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1306.jpeg\" width = \"400\" height = \"300\" div align=center /></center>\n\n<p>结论如下：</p>\n<pre><code>当硬件资源占用较少时数据规模对时间消耗的优化较大，但是当硬件资源占用比较紧张（没有超过资源限制）的情况下，数据规模对时间消耗的优化甚微，甚至有时还会起到副作用。这个原因我想是因为在硬件资源紧张时数据规模的减小会使得资源调度时间占并行计算时间的比例增加，从而导致优化效果不好。\n</code></pre>\n<p>（3）在并行优化最好的资源调用下，数据规模对计算效率的影响。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1307.jpeg\" width = \"400\" height = \"300\" div align=center /></center>\n\n<p>结论如下：</p>\n<pre><code>数据规模越大，计算的时间消耗增长越快。原因是当数据规模过大，会导致硬件资源的利用率达到最大，从而无法更多的发挥并行计算的优势。\n</code></pre>\n<h3 id=\"实验输出结果\"><a href=\"#实验输出结果\" class=\"headerlink\" title=\"实验输出结果\"></a><strong>实验输出结果</strong></h3><p>这里抛开性能不谈，观察随数据规模的增加，Mandelbrot Set图片分辨率的变化情况。</p>\n<center><img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1308.jpeg\" width = \"600\" height = \"300\" div align=center /></center>\n\n<h3 id=\"存在的问题\"><a href=\"#存在的问题\" class=\"headerlink\" title=\"存在的问题\"></a><strong>存在的问题</strong></h3><p>（1）硬件资源调度时间消耗与并行计算带来的效率提高二者间的平衡情况与数据规模的关系没有体现。</p>\n<p>（2）在数据规模对计算效率的影响探究中，数据规模的选择有些不科学。</p>\n"},{"title":"OpenCL并行编程框架【高性能计算导论课程作业】","date":"2022-01-19T08:58:31.000Z","_content":"\n## OpenCL环境安装配置\n\n由于我使用的机器是MacBook Pro 2020，mac系统里面已经集成了OpenCL的SDK，所以不需要去另外下载，只需要在Xcode工程中将其加入进来就可以进行OpenCL的开发了。下面展示的是工程文件中配置OpenCL环境的过程。\n\n（1）首先在Xcode工程文件的Build Phases下导入OpenCL的库文件。\n\n（2）导入后，就可以看到在旁边的文件栏有OpenCL的.framework文件。\n\n（3）然后就是在相应的cpp文件中写入对应的头文件即可。\n\n这样OpenCL在Xcode工程文件内的编译环境就配置好了。\n\n## 本机设备参数分析\n\n这里方法是使用OpenCL的代码进行设备参数信息的输出。由于代码过长而且与并行计算相关性不大就不予展示。\n\n所以下面就是对OpenCL代码的输出进行分析。\n\n首先是本机OpenCL的版本，是OpenCL1.2。\n\n然后下面可以看到本机的两个Device，一个是CPU，一个是GPU。\n\n这里重点描述GPU的硬件设备参数，主要信息如下：\n\nGPU型号：\t\t\t\t\t\tIntel(R) Iris(TM) Plus Graphics\n\n最大计算单元个数(work group)：\t64\n\n最大work item维度:\t\t\t\t3\n\nwork item每个维度的容量：\t\t256\n\nwork group容量：\t\t\t\t\t256\n\n该Device的id：\t\t\t\t\t0x7fff0000\n\nglobal的内存大小：\t\t\t\t1610612736\n\ndevice的缓存大小：\t\t\t\t65536\n\ndevice的local内存大小：\t\t\t65536\n\n## 向量运算代码分析与实验\n\n代码如下\n\n```c++\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <OpenCL/opencl.h>\n#include <iostream>\n \nusing namespace std;\n// OpenCL kernel. Each work item takes care of one element of c\n#define KERNEL(...) #__VA_ARGS__\n\nconst char *kernelSource = KERNEL(\n_Pragma (\"OPENCL EXTENSION cl_khr_fp64:enable\")\n__kernel void vecAdd(  __global float *a,\n                       __global float *b,\n                       __global float *c,\n                       const unsigned int n)\n{\n    //Get our global thread ID\n    int id = get_global_id(0);\n                                                              \n    //Make sure we do not go out of bounds\n    if (id < n)\n    {\n        c[id] = a[id] + b[id];\n    }\n}\n);\nint main( int argc, char* argv[] )\n{\n    int i=0;\n    size_t globalSize, localSize;\n    cl_int err;\n    cl_event event;\n    float sum = 0.;\n\n    // Length of vectors\n    // unsigned int n = 100000;\n    int n = 10000000;\n    /*\n    // Host input vectors\n    double *h_a;\n    double *h_b;\n    // Host output vector\n    double *h_c;\n     */\n    \n    // Host input vectors\n    float *h_a;\n    float *h_b;\n    // Host output vector\n    float *h_c;\n \n    // Device input buffers\n    cl_mem d_a;\n    cl_mem d_b;\n    // Device output buffer\n    cl_mem d_c;\n \n    cl_platform_id platform;        // OpenCL platform\n    cl_device_id device_id;           // device ID\n    cl_context context;               // context\n    cl_command_queue queue;           // command queue\n    cl_program program;               // program\n    cl_kernel kernel;                 // kernel\n \n    // Size, in bytes, of each vector\n    size_t bytes = n * sizeof(float);\n \n    // Allocate memory for each vector on host\n    h_a = (float*)malloc(bytes);\n    h_b = (float*)malloc(bytes);\n    h_c = (float*)malloc(bytes);\n    // Initialize vectors on host\n   \n    for(  i = 0; i < n; i++ ){\n        h_a[i] = sinf(i)*sinf(i);\n        h_b[i] = cosf(i)*cosf(i);\n    }\n    // size_t globalSize, localSize;\n    \n    //cl_int err;\n \n    // Number of work items in each local work group\n    localSize = 64;\n \n    // Number of total work items - localSize must be devisor\n    globalSize =(size_t)ceil(n/(float)localSize)*localSize;\n \n    // Bind to platform\n    err = clGetPlatformIDs(1, &platform, NULL);\n \n    // Get ID for the device\n    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &device_id, NULL);\n \n    // Create a context\n    context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);\n \n    // Create a command queue\n    //queue = clCreateCommandQueue(context, device_id, 0, &err);\n    queue = clCreateCommandQueue(context, device_id, CL_QUEUE_PROFILING_ENABLE, &err);\n \n    // Create the compute program from the source buffer\n    program = clCreateProgramWithSource(context, 1,\n                            (const char **) & kernelSource, NULL, &err);\n \n    // Build the program executable\n    clBuildProgram(program, 0, NULL, NULL, NULL, NULL);\n \n    // Create the compute kernel in the program we wish to run\n    kernel = clCreateKernel(program, \"vecAdd\", &err);\n \n    // Create the input and output arrays in device memory for our calculation\n    d_a = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, NULL);\n    d_b = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, NULL);\n    d_c = clCreateBuffer(context, CL_MEM_WRITE_ONLY, bytes, NULL, NULL);\n \n    // Write our data set into the input array in device memory\n    err = clEnqueueWriteBuffer(queue, d_a, CL_TRUE, 0,\n                                   bytes, h_a, 0, NULL, NULL);\n    err |= clEnqueueWriteBuffer(queue, d_b, CL_TRUE, 0,\n                                   bytes, h_b, 0, NULL, NULL);\n \n    // Set the arguments to our compute kernel\n    err  = clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_a);\n    err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_b);\n    err |= clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_c);\n    err |= clSetKernelArg(kernel, 3, sizeof(unsigned int), &n);\n \n    // Execute the kernel over the entire range of the data set\n    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize,\n                                                              0, NULL, &event);\n \n    // Wait for the command queue to get serviced before reading back results\n    clFinish(queue);\n    cl_ulong time_start;\n    cl_ulong time_end;\n\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n\n    double times = time_end - time_start;\n    printf(\"OpenCL Kernel Execution time is: %0.4f\\n\", times / 1000000.0);\n    \n    // Read the results from the device\n    clEnqueueReadBuffer(queue, d_c, CL_TRUE, 0, bytes, h_c, 0, NULL, NULL );\n \n    //Sum up vector c and print result divided by n, this should equal 1 within error\n    //double sum = 0;\n    for(i=0; i<n; i++)\n    {\n        sum += h_c[i];\n    }\n    printf(\"final result: %lf\\n\", sum/n);\n \n    // release OpenCL resources\n    clReleaseMemObject(d_a);\n    clReleaseMemObject(d_b);\n    clReleaseMemObject(d_c);\n    clReleaseProgram(program);\n    clReleaseKernel(kernel);\n    clReleaseCommandQueue(queue);\n    clReleaseContext(context);\n \n    //release host memory\n    free(h_a);\n    free(h_b);\n    free(h_c);\n \n    return 0;\n}\n```\n\n这里进行代码的详细分析，后面的PI计算相似的内容就不再赘述了。\n\n首先是一个可变长的宏，用来以参数的形式定义kernel函数。\n```cpp\n#define KERNEL(...) #__VA_ARGS__\n```\n接下来用字符串的形式去定义kernel函数，这样它不会与host的代码一起编译，在调用时才会被编译。同时，为了OpenCL对64位浮点运算的支持，需要加一个如下的声明。\n```cpp\n_Pragma (\"OPENCL EXTENSION cl_khr_fp64:enable\")\n```\n但是这里我们只用到了float，float类型的参数是不需要这样的声明的。\n\n然后是对kernel内容的解释，首先要获取当前所在work item的global id，因为由上面的GPU信息可以知道，id可以有三个维度，但是我们做的是向量加法，所以只需要用到它的第一个维度，所以里面的参数为0。每个work item负责结果向量中一个位置的计算，对应如下代码。\n```cpp\n//Get our global thread ID\n    int id = get_global_id(0);\n                                                              \n    //Make sure we do not go out of bounds\n    if (id < n)\n    {\n        c[id] = a[id] + b[id];\n    }\n```\n这样就定义好了kernel函数，也就是并行的部分。\n\n下面开始为OpenCL在本机的运行做准备。\n\n因为我们的向量加法，既要在host输入，也要在device中并行计算。所以要在host和device上分别开辟相应的空间。然后在host的变量内存空间对两个向量进行赋值，一个是cos^2(x)一个是sin^2(x)，这样保证每个元素相加得到的结果为1。\n```cpp\n    // Host input vectors\n    float *h_a;\n    float *h_b;\n    // Host output vector\n    float *h_c;\n \n    // Device input buffers\n    cl_mem d_a;\n    cl_mem d_b;\n    // Device output buffer\n    cl_mem d_c;\n```\n下面开始设定并行计算的global size和local size，其中local size是指每个work group内的work item数目，其中global size是指所有work group内的work item数目之和，也就是所有参与并行计算的work item数目。\n\n这里需要让每个work item负责输出向量一个位置的运算，所以global size不仅需要是local size的整数倍，而且还要大于等于向量的长度。\n所以代码如下。\n```cpp\n    // Number of work items in each local work group\n    localSize = 64;\n \n    // Number of total work items - localSize must be devisor\n    globalSize =(size_t)ceil(n/(float)localSize)*localSize;\n```\n然后获取要使用的platform id，它可以看作是一个handel，用来获取相应的device id，也就是上面设备信息给出的0x7fff0000，然后再根据它来获取设备当前的运行状态或运行环境，从而创建命令队列。\n```cpp\n// Bind to platform\n    err = clGetPlatformIDs(1, &platform, NULL);\n \n    // Get ID for the device\n    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &device_id, NULL);\n \n    // Create a context\n    context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);\n \n    // Create a command queue\n    //queue = clCreateCommandQueue(context, device_id, 0, &err);\n    queue = clCreateCommandQueue(context, device_id, CL_QUEUE_PROFILING_ENABLE, &err);\n```\n这里注意到在创建命令队列时与源代码不同的是我加入了参数CL_QUEUE_PROFILING_ENABLE，用来获取kernel的运行时间。方便后面的实验。\n\n然后就是将以变量形式声明的kernel进行实例化，代码运行至这里才会进行这个kernel函数的编译。\n```cpp\n// Create the compute program from the source buffer\n    program = clCreateProgramWithSource(context, 1,\n                            (const char **) & kernelSource, NULL, &err);\n \n    // Build the program executable\n    clBuildProgram(program, 0, NULL, NULL, NULL, NULL);\n \n    // Create the compute kernel in the program we wish to run\n    kernel = clCreateKernel(program, \"vecAdd\", &err);\n```\n下面是在Device中为上面需要的内存开辟地址空间，并且从host的内存中写入Device的内存中。\n```cpp\n// Create the input and output arrays in device memory for our calculation\n    d_a = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, NULL);\n    d_b = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, NULL);\n    d_c = clCreateBuffer(context, CL_MEM_WRITE_ONLY, bytes, NULL, NULL);\n \n    // Write our data set into the input array in device memory\n    err = clEnqueueWriteBuffer(queue, d_a, CL_TRUE, 0,\n                                   bytes, h_a, 0, NULL, NULL);\n    err |= clEnqueueWriteBuffer(queue, d_b, CL_TRUE, 0,\n                                   bytes, h_b, 0, NULL, NULL);\n```\n然后对kernel函数进行参数的传递，然后开始并行的运算。\n```cpp\n    // Set the arguments to our compute kernel\n    err  = clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_a);\n    err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_b);\n    err |= clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_c);\n    err |= clSetKernelArg(kernel, 3, sizeof(unsigned int), &n);\n \n    // Execute the kernel over the entire range of the data set\n    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize,\n                                                              0, NULL, &event);\n```\n最后等待命令队列的相应，从而完成并行计算的部分，最后要对设备上的一些内存空间进行释放。\n\n这里为了方便实验，进行了kernel代码运行时间的计算，使用的是OpenCL自带的函数，操作如下所示。\n```cpp\n    // Wait for the command queue to get serviced before reading back results\n    clFinish(queue);\n    cl_ulong time_start;\n    cl_ulong time_end;\n\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n\n    double times = time_end - time_start;\n    printf(\"OpenCL Kernel Execution time is: %0.4f\\n\", times / 1000000.0);\n```\n***实验***\n\n这里我们探究随着local size，也就是每个work group内的work item数目的变化，运行效率（计算时间）怎么变化。\n\n为了对比明显，这里的数据规模令n = 10000000，然后在local size分别为1，2，4，8，16，32，64的情况下进行实验。\n\n|     | 1  | 2 | 4 | 8 | 16 | 32 | 64 |\n|  ----  | ----  | ---- | ---- | ---- | ---- | ---- | ---- |\n| t/ms  | 53.5526 | 25.3358 | 13.0841 | 7.8111 | 3.5446 | 2.9774 | 2.8876 |\n\n可以发现随着Local Size数的增加，计算时间逐渐减小，计算效率逐渐增加，但是增加的幅度越来越小，越来越趋于平稳。\n\n原因是，当Local Size比较小时，64个work group无法为所有的元素计算提供资源，只能串行地进行多次并行的计算，来达到完成计算任务的目的。所以也就增加了计算时间消耗。\n\n下面探究在local size为64的情况下，随着数据规模的增加，计算效率的变化情况。这里的数据规模我们取10000、100000、1000000、10000000、100000000。\n\n|     | 10000  | 100000 | 1000000 | 10000000 | 100000000 |\n|  ----  | ----  | ---- | ---- | ---- | ---- |\n| t/ms  | 0.0264 | 0.0674 | 0.4833 | 2.8852 | 28.6238 |\n\n可以发现随着数据规模的增加，一开始的时间消耗增长较慢，后来就几乎与数据规模的增长规模相同了。\n\n原因就是当数据规模没那么大时，所有work group的所有work item资源没有被全部利用，当增加数据规模，可以使用更多的并行资源，从而与计算时间的增加进行平衡。但是当数据规模很大时，总共需要的work item数目超过了本机可以提供的数目（256 * 64），再增加数据规模，增加的时间就不是并行后的结果了，而是串行地等待前面的并行计算完，之后的再进行并行计算，所以时间的增量几乎就是数据规模的增量。\n\n\n## PI值运算代码分析与实验\n\n代码如下\n```cpp\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <OpenCL/opencl.h>\n\n\n// OpenCL kernel. many workGroups compute n iterations\n#define KERNEL(...) #__VA_ARGS__\nconst char * kernelSource =  KERNEL(\n__kernel void Pi(__global float *workGroupBuffer, // 0..NumWorkGroups-1\n__local float *insideWorkGroup,  // 0..workGroupSize-1\nconst uint n,        // Total iterations\nconst uint chunk)        // Chunk size\n{\nconst uint lid = get_local_id(0);\nconst uint gid = get_global_id(0);\n\nconst float step = (1.0/(float)n);\nfloat partial_sum = 0.0;\n\n// Each work-item computes chunk iterations\nfor(uint i=gid*chunk; i<(gid*chunk)+chunk; i++) {\n    float x = step * ((float) i - 0.5);\n    partial_sum += 4.0 / (1.0 + x * x);\n}\n\n// Each work-item stores its partial sum in the workgroup array\ninsideWorkGroup[lid] = partial_sum;\n\n// Synchronize all threads within the workgroup\nbarrier(CLK_LOCAL_MEM_FENCE);\n\nfloat local_pi = 0;\n\n// Only work-item 0 of each workgroup perform the reduction\n// of that workgroup\nif(lid == 0) {\n    const uint length = lid + get_local_size(0);\n    for (uint i = lid; i<length; i++) {\n        local_pi += insideWorkGroup[i];\n    }\n// It store the workgroup sum\n// Final reduction, between block, is done out by CPU\n    workGroupBuffer[get_group_id(0)] = local_pi;\n}\n}\n\n);\n\nint main( int argc, char* argv[] )\n{\n    int i=0;\n    float pi;\n    float *pi_partial;\n    size_t maxWorkGroupSize;\n    cl_int err;\n    cl_mem memObjects;\n    int niter, chunks, workGroups;\n    size_t globalWorkSize;\n    size_t localWorkSize;\n    cl_event event;\n\n    cl_platform_id platform;        // OpenCL platform\n    cl_device_id device_id;           // device ID\n    cl_context context;               // context\n    cl_command_queue queue;           // command queue\n    cl_program program;               // program\n    cl_kernel kernel;                 // kernel\n\n    niter = 262144;\n    chunks=64;\n\n    err = clGetPlatformIDs(1, &platform, NULL);\n\n    // Get ID for the device\n    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &device_id, NULL);\n    clGetDeviceInfo(device_id, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(size_t),\n                    &maxWorkGroupSize, NULL);\n    workGroups = ceil((float)(niter/maxWorkGroupSize/chunks));\n\n    pi_partial = (float*)malloc(sizeof(float)*workGroups);\n\n    // Create a context\n    context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);\n\n    // Create a command queue\n    queue = clCreateCommandQueue(context, device_id, CL_QUEUE_PROFILING_ENABLE, &err);\n\n    // Create the compute program from the source buffer\n\n    program = clCreateProgramWithSource(context, 1,\n                                        &kernelSource, NULL, &err);\n    // Build the program executable\n    err = clBuildProgram(program, 0, NULL, NULL, NULL, NULL);\n    localWorkSize =  maxWorkGroupSize;\n    globalWorkSize = niter / chunks;\n\n    // Create the compute kernel in the program we wish to run\n    kernel = clCreateKernel(program, \"Pi\", &err);\n\n    // Create the input and output arrays in device memory for our calculation\n    memObjects = clCreateBuffer(context, CL_MEM_READ_WRITE,\n                                sizeof(float)*workGroups, NULL, &err);\n\n    err |= clSetKernelArg(kernel, 0, sizeof(cl_mem), &memObjects);\n    err  = clSetKernelArg(kernel, 1, sizeof(float)*maxWorkGroupSize, NULL);\n    err |= clSetKernelArg(kernel, 2, sizeof(unsigned int), &niter);\n    err |= clSetKernelArg(kernel, 3, sizeof(unsigned int), &chunks);\n\n    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalWorkSize, &localWorkSize,\n0, NULL, &event);\n    clFinish(queue);\n    cl_ulong time_start;\n    cl_ulong time_end;\n\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n\n    double times = time_end - time_start;\n    printf(\"OpenCL Kernel Execution time is: %0.4f\\n\", times / 1000000.0);\n    err = clEnqueueReadBuffer(queue, memObjects, CL_TRUE, 0,\n                              sizeof(float)*workGroups, pi_partial, 0, NULL, NULL);\n    pi=0;\n\n    for(i=0; i<workGroups; i++) {\n        pi += pi_partial[i];\n    }\n    pi *= (1.0/(float)niter);\n    printf(\"final result: %f\\n\", pi);\n\n    // release OpenCL resources\n    clReleaseMemObject(memObjects);\n    clReleaseProgram(program);\n    clReleaseKernel(kernel);\n    clReleaseCommandQueue(queue);\n    clReleaseContext(context);\n\n    //release host memory\n    free(pi_partial);\n    return 0;\n}\n```\n这里进行部分代码的分析，首先阐述chunk的概念，这里用如下的图进行表示。\n\n<center>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1329.png\" height=\"260\" width=\"450\">\n</center>\n\n所以这里的chunk指的是多个work group组成的组合。因此work group的数量是由所规定的chunk大小以及数据规模决定的。\n```cpp\n    workGroups = ceil((float)(niter/maxWorkGroupSize/chunks));\n\n    pi_partial = (float*)malloc(sizeof(float)*workGroups);\n```\n再向下理解，这里的local work size还是每个work group内的work item数量，这里取的最大，也就是本机器的256。但是这里的global work size与global size不同，他是将一个chunk的所有work group作为一个global work group，那么global work size就是chunks的数目。\n```cpp\n    localWorkSize =  maxWorkGroupSize;\n    globalWorkSize = niter / chunks;\n```\n这样就可以去理解kernel函数的内容了，首先它是将数据niter分成了chunk份，这就是为什么niter必须是chunk的整数倍，然后每个chunk内work item的数目就是要并行计算的数目，也就是每个chunk内做并行。\n```cpp\nfor(uint i=gid*chunk; i<(gid*chunk)+chunk; i++) {\n    float x = step * ((float) i - 0.5);\n    partial_sum += 4.0 / (1.0 + x * x);\n}\n```\n然后对每个chunk的计算结果进行保存，并在lid为0的work item中进行结果的归约，然后将归约的结果作为这个work group（其实是这个chunk）的运算结果，然后这些chunk的结果要在kernel函数外，也就是CPU当中进行归约，从而计算出最终的结果。对应代码如下。\n```cpp\n// Each work-item stores its partial sum in the workgroup array\ninsideWorkGroup[lid] = partial_sum;\n\n// Synchronize all threads within the workgroup\nbarrier(CLK_LOCAL_MEM_FENCE);\n\nfloat local_pi = 0;\n\n// Only work-item 0 of each workgroup perform the reduction\n// of that workgroup\nif(lid == 0) {\n    const uint length = lid + get_local_size(0);\n    for (uint i = lid; i<length; i++) {\n        local_pi += insideWorkGroup[i];\n    }\n// It store the workgroup sum\n// Final reduction, between block, is done out by CPU\n    workGroupBuffer[get_group_id(0)] = local_pi;\n}\n```\n这样就完成了PI计算的并行过程。\n\n***实验***\n\n由于这里的local size规定死了为最大的256，所以不宜做改变，所以这里我们更改chunks的数量。也就是修改在一个for内并行计算的部分，分别计算chunk数为1、2、4、8、16、32、64的时间消耗。这里的数据规模取26214400。\n输出结果如下。\n\n|     | 1  | 2 | 4 | 8 | 16 | 32 | 64 |\n|  ----  | ----  | ---- | ---- | ---- | ---- | ---- | ---- |\n| t/ms  | 8.3137 | 5.2593 | 2.5912 | 1.6443 | 1.2119 | 1.0183 | 0.9311 |\n\n可以看到，随着chunk数目的增加，计算的效率越来越高，计算的时间消耗越来越少，并且计算时间的优化效果越来越弱。这说明在同一个chunk内的并行数越多，计算效率越高。\n\n下面选择在chunk为64的条件下进行计算效率随数据规模的变化情况，这里的数据规模我们选择131072、262144、2621440、26214400、262144000。\n\n|     | 131072  | 262144 | 2621440 | 26214400 | 262144000 |\n|  ----  | ----  | ---- | ---- | ---- | ---- |\n| t/ms  | 0.0324 | 0.0342 | 0.1155 | 0.9268 | 10.7321 |\n\n可以看到，随着数据规模的增加，计算时间一开始增加缓慢，到后面也是以类似10倍的速率增长，原因与向量加法中的阐述相同。再增加数据规模，增加的时间就不是并行后的结果了，而是串行地等待前面的并行计算完，之后的再进行并行计算，所以时间的增量几乎就是数据规模的增量。\n\n## 环境\n\nmacOS Big Sur Version 11.1","source":"_posts/HPC-OpenCL-md.md","raw":"---\ntitle: OpenCL并行编程框架【高性能计算导论课程作业】\ndate: 2022-01-19 16:58:31\ntags: HPC\n---\n\n## OpenCL环境安装配置\n\n由于我使用的机器是MacBook Pro 2020，mac系统里面已经集成了OpenCL的SDK，所以不需要去另外下载，只需要在Xcode工程中将其加入进来就可以进行OpenCL的开发了。下面展示的是工程文件中配置OpenCL环境的过程。\n\n（1）首先在Xcode工程文件的Build Phases下导入OpenCL的库文件。\n\n（2）导入后，就可以看到在旁边的文件栏有OpenCL的.framework文件。\n\n（3）然后就是在相应的cpp文件中写入对应的头文件即可。\n\n这样OpenCL在Xcode工程文件内的编译环境就配置好了。\n\n## 本机设备参数分析\n\n这里方法是使用OpenCL的代码进行设备参数信息的输出。由于代码过长而且与并行计算相关性不大就不予展示。\n\n所以下面就是对OpenCL代码的输出进行分析。\n\n首先是本机OpenCL的版本，是OpenCL1.2。\n\n然后下面可以看到本机的两个Device，一个是CPU，一个是GPU。\n\n这里重点描述GPU的硬件设备参数，主要信息如下：\n\nGPU型号：\t\t\t\t\t\tIntel(R) Iris(TM) Plus Graphics\n\n最大计算单元个数(work group)：\t64\n\n最大work item维度:\t\t\t\t3\n\nwork item每个维度的容量：\t\t256\n\nwork group容量：\t\t\t\t\t256\n\n该Device的id：\t\t\t\t\t0x7fff0000\n\nglobal的内存大小：\t\t\t\t1610612736\n\ndevice的缓存大小：\t\t\t\t65536\n\ndevice的local内存大小：\t\t\t65536\n\n## 向量运算代码分析与实验\n\n代码如下\n\n```c++\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <OpenCL/opencl.h>\n#include <iostream>\n \nusing namespace std;\n// OpenCL kernel. Each work item takes care of one element of c\n#define KERNEL(...) #__VA_ARGS__\n\nconst char *kernelSource = KERNEL(\n_Pragma (\"OPENCL EXTENSION cl_khr_fp64:enable\")\n__kernel void vecAdd(  __global float *a,\n                       __global float *b,\n                       __global float *c,\n                       const unsigned int n)\n{\n    //Get our global thread ID\n    int id = get_global_id(0);\n                                                              \n    //Make sure we do not go out of bounds\n    if (id < n)\n    {\n        c[id] = a[id] + b[id];\n    }\n}\n);\nint main( int argc, char* argv[] )\n{\n    int i=0;\n    size_t globalSize, localSize;\n    cl_int err;\n    cl_event event;\n    float sum = 0.;\n\n    // Length of vectors\n    // unsigned int n = 100000;\n    int n = 10000000;\n    /*\n    // Host input vectors\n    double *h_a;\n    double *h_b;\n    // Host output vector\n    double *h_c;\n     */\n    \n    // Host input vectors\n    float *h_a;\n    float *h_b;\n    // Host output vector\n    float *h_c;\n \n    // Device input buffers\n    cl_mem d_a;\n    cl_mem d_b;\n    // Device output buffer\n    cl_mem d_c;\n \n    cl_platform_id platform;        // OpenCL platform\n    cl_device_id device_id;           // device ID\n    cl_context context;               // context\n    cl_command_queue queue;           // command queue\n    cl_program program;               // program\n    cl_kernel kernel;                 // kernel\n \n    // Size, in bytes, of each vector\n    size_t bytes = n * sizeof(float);\n \n    // Allocate memory for each vector on host\n    h_a = (float*)malloc(bytes);\n    h_b = (float*)malloc(bytes);\n    h_c = (float*)malloc(bytes);\n    // Initialize vectors on host\n   \n    for(  i = 0; i < n; i++ ){\n        h_a[i] = sinf(i)*sinf(i);\n        h_b[i] = cosf(i)*cosf(i);\n    }\n    // size_t globalSize, localSize;\n    \n    //cl_int err;\n \n    // Number of work items in each local work group\n    localSize = 64;\n \n    // Number of total work items - localSize must be devisor\n    globalSize =(size_t)ceil(n/(float)localSize)*localSize;\n \n    // Bind to platform\n    err = clGetPlatformIDs(1, &platform, NULL);\n \n    // Get ID for the device\n    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &device_id, NULL);\n \n    // Create a context\n    context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);\n \n    // Create a command queue\n    //queue = clCreateCommandQueue(context, device_id, 0, &err);\n    queue = clCreateCommandQueue(context, device_id, CL_QUEUE_PROFILING_ENABLE, &err);\n \n    // Create the compute program from the source buffer\n    program = clCreateProgramWithSource(context, 1,\n                            (const char **) & kernelSource, NULL, &err);\n \n    // Build the program executable\n    clBuildProgram(program, 0, NULL, NULL, NULL, NULL);\n \n    // Create the compute kernel in the program we wish to run\n    kernel = clCreateKernel(program, \"vecAdd\", &err);\n \n    // Create the input and output arrays in device memory for our calculation\n    d_a = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, NULL);\n    d_b = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, NULL);\n    d_c = clCreateBuffer(context, CL_MEM_WRITE_ONLY, bytes, NULL, NULL);\n \n    // Write our data set into the input array in device memory\n    err = clEnqueueWriteBuffer(queue, d_a, CL_TRUE, 0,\n                                   bytes, h_a, 0, NULL, NULL);\n    err |= clEnqueueWriteBuffer(queue, d_b, CL_TRUE, 0,\n                                   bytes, h_b, 0, NULL, NULL);\n \n    // Set the arguments to our compute kernel\n    err  = clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_a);\n    err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_b);\n    err |= clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_c);\n    err |= clSetKernelArg(kernel, 3, sizeof(unsigned int), &n);\n \n    // Execute the kernel over the entire range of the data set\n    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize,\n                                                              0, NULL, &event);\n \n    // Wait for the command queue to get serviced before reading back results\n    clFinish(queue);\n    cl_ulong time_start;\n    cl_ulong time_end;\n\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n\n    double times = time_end - time_start;\n    printf(\"OpenCL Kernel Execution time is: %0.4f\\n\", times / 1000000.0);\n    \n    // Read the results from the device\n    clEnqueueReadBuffer(queue, d_c, CL_TRUE, 0, bytes, h_c, 0, NULL, NULL );\n \n    //Sum up vector c and print result divided by n, this should equal 1 within error\n    //double sum = 0;\n    for(i=0; i<n; i++)\n    {\n        sum += h_c[i];\n    }\n    printf(\"final result: %lf\\n\", sum/n);\n \n    // release OpenCL resources\n    clReleaseMemObject(d_a);\n    clReleaseMemObject(d_b);\n    clReleaseMemObject(d_c);\n    clReleaseProgram(program);\n    clReleaseKernel(kernel);\n    clReleaseCommandQueue(queue);\n    clReleaseContext(context);\n \n    //release host memory\n    free(h_a);\n    free(h_b);\n    free(h_c);\n \n    return 0;\n}\n```\n\n这里进行代码的详细分析，后面的PI计算相似的内容就不再赘述了。\n\n首先是一个可变长的宏，用来以参数的形式定义kernel函数。\n```cpp\n#define KERNEL(...) #__VA_ARGS__\n```\n接下来用字符串的形式去定义kernel函数，这样它不会与host的代码一起编译，在调用时才会被编译。同时，为了OpenCL对64位浮点运算的支持，需要加一个如下的声明。\n```cpp\n_Pragma (\"OPENCL EXTENSION cl_khr_fp64:enable\")\n```\n但是这里我们只用到了float，float类型的参数是不需要这样的声明的。\n\n然后是对kernel内容的解释，首先要获取当前所在work item的global id，因为由上面的GPU信息可以知道，id可以有三个维度，但是我们做的是向量加法，所以只需要用到它的第一个维度，所以里面的参数为0。每个work item负责结果向量中一个位置的计算，对应如下代码。\n```cpp\n//Get our global thread ID\n    int id = get_global_id(0);\n                                                              \n    //Make sure we do not go out of bounds\n    if (id < n)\n    {\n        c[id] = a[id] + b[id];\n    }\n```\n这样就定义好了kernel函数，也就是并行的部分。\n\n下面开始为OpenCL在本机的运行做准备。\n\n因为我们的向量加法，既要在host输入，也要在device中并行计算。所以要在host和device上分别开辟相应的空间。然后在host的变量内存空间对两个向量进行赋值，一个是cos^2(x)一个是sin^2(x)，这样保证每个元素相加得到的结果为1。\n```cpp\n    // Host input vectors\n    float *h_a;\n    float *h_b;\n    // Host output vector\n    float *h_c;\n \n    // Device input buffers\n    cl_mem d_a;\n    cl_mem d_b;\n    // Device output buffer\n    cl_mem d_c;\n```\n下面开始设定并行计算的global size和local size，其中local size是指每个work group内的work item数目，其中global size是指所有work group内的work item数目之和，也就是所有参与并行计算的work item数目。\n\n这里需要让每个work item负责输出向量一个位置的运算，所以global size不仅需要是local size的整数倍，而且还要大于等于向量的长度。\n所以代码如下。\n```cpp\n    // Number of work items in each local work group\n    localSize = 64;\n \n    // Number of total work items - localSize must be devisor\n    globalSize =(size_t)ceil(n/(float)localSize)*localSize;\n```\n然后获取要使用的platform id，它可以看作是一个handel，用来获取相应的device id，也就是上面设备信息给出的0x7fff0000，然后再根据它来获取设备当前的运行状态或运行环境，从而创建命令队列。\n```cpp\n// Bind to platform\n    err = clGetPlatformIDs(1, &platform, NULL);\n \n    // Get ID for the device\n    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &device_id, NULL);\n \n    // Create a context\n    context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);\n \n    // Create a command queue\n    //queue = clCreateCommandQueue(context, device_id, 0, &err);\n    queue = clCreateCommandQueue(context, device_id, CL_QUEUE_PROFILING_ENABLE, &err);\n```\n这里注意到在创建命令队列时与源代码不同的是我加入了参数CL_QUEUE_PROFILING_ENABLE，用来获取kernel的运行时间。方便后面的实验。\n\n然后就是将以变量形式声明的kernel进行实例化，代码运行至这里才会进行这个kernel函数的编译。\n```cpp\n// Create the compute program from the source buffer\n    program = clCreateProgramWithSource(context, 1,\n                            (const char **) & kernelSource, NULL, &err);\n \n    // Build the program executable\n    clBuildProgram(program, 0, NULL, NULL, NULL, NULL);\n \n    // Create the compute kernel in the program we wish to run\n    kernel = clCreateKernel(program, \"vecAdd\", &err);\n```\n下面是在Device中为上面需要的内存开辟地址空间，并且从host的内存中写入Device的内存中。\n```cpp\n// Create the input and output arrays in device memory for our calculation\n    d_a = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, NULL);\n    d_b = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, NULL);\n    d_c = clCreateBuffer(context, CL_MEM_WRITE_ONLY, bytes, NULL, NULL);\n \n    // Write our data set into the input array in device memory\n    err = clEnqueueWriteBuffer(queue, d_a, CL_TRUE, 0,\n                                   bytes, h_a, 0, NULL, NULL);\n    err |= clEnqueueWriteBuffer(queue, d_b, CL_TRUE, 0,\n                                   bytes, h_b, 0, NULL, NULL);\n```\n然后对kernel函数进行参数的传递，然后开始并行的运算。\n```cpp\n    // Set the arguments to our compute kernel\n    err  = clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_a);\n    err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_b);\n    err |= clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_c);\n    err |= clSetKernelArg(kernel, 3, sizeof(unsigned int), &n);\n \n    // Execute the kernel over the entire range of the data set\n    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize,\n                                                              0, NULL, &event);\n```\n最后等待命令队列的相应，从而完成并行计算的部分，最后要对设备上的一些内存空间进行释放。\n\n这里为了方便实验，进行了kernel代码运行时间的计算，使用的是OpenCL自带的函数，操作如下所示。\n```cpp\n    // Wait for the command queue to get serviced before reading back results\n    clFinish(queue);\n    cl_ulong time_start;\n    cl_ulong time_end;\n\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n\n    double times = time_end - time_start;\n    printf(\"OpenCL Kernel Execution time is: %0.4f\\n\", times / 1000000.0);\n```\n***实验***\n\n这里我们探究随着local size，也就是每个work group内的work item数目的变化，运行效率（计算时间）怎么变化。\n\n为了对比明显，这里的数据规模令n = 10000000，然后在local size分别为1，2，4，8，16，32，64的情况下进行实验。\n\n|     | 1  | 2 | 4 | 8 | 16 | 32 | 64 |\n|  ----  | ----  | ---- | ---- | ---- | ---- | ---- | ---- |\n| t/ms  | 53.5526 | 25.3358 | 13.0841 | 7.8111 | 3.5446 | 2.9774 | 2.8876 |\n\n可以发现随着Local Size数的增加，计算时间逐渐减小，计算效率逐渐增加，但是增加的幅度越来越小，越来越趋于平稳。\n\n原因是，当Local Size比较小时，64个work group无法为所有的元素计算提供资源，只能串行地进行多次并行的计算，来达到完成计算任务的目的。所以也就增加了计算时间消耗。\n\n下面探究在local size为64的情况下，随着数据规模的增加，计算效率的变化情况。这里的数据规模我们取10000、100000、1000000、10000000、100000000。\n\n|     | 10000  | 100000 | 1000000 | 10000000 | 100000000 |\n|  ----  | ----  | ---- | ---- | ---- | ---- |\n| t/ms  | 0.0264 | 0.0674 | 0.4833 | 2.8852 | 28.6238 |\n\n可以发现随着数据规模的增加，一开始的时间消耗增长较慢，后来就几乎与数据规模的增长规模相同了。\n\n原因就是当数据规模没那么大时，所有work group的所有work item资源没有被全部利用，当增加数据规模，可以使用更多的并行资源，从而与计算时间的增加进行平衡。但是当数据规模很大时，总共需要的work item数目超过了本机可以提供的数目（256 * 64），再增加数据规模，增加的时间就不是并行后的结果了，而是串行地等待前面的并行计算完，之后的再进行并行计算，所以时间的增量几乎就是数据规模的增量。\n\n\n## PI值运算代码分析与实验\n\n代码如下\n```cpp\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <OpenCL/opencl.h>\n\n\n// OpenCL kernel. many workGroups compute n iterations\n#define KERNEL(...) #__VA_ARGS__\nconst char * kernelSource =  KERNEL(\n__kernel void Pi(__global float *workGroupBuffer, // 0..NumWorkGroups-1\n__local float *insideWorkGroup,  // 0..workGroupSize-1\nconst uint n,        // Total iterations\nconst uint chunk)        // Chunk size\n{\nconst uint lid = get_local_id(0);\nconst uint gid = get_global_id(0);\n\nconst float step = (1.0/(float)n);\nfloat partial_sum = 0.0;\n\n// Each work-item computes chunk iterations\nfor(uint i=gid*chunk; i<(gid*chunk)+chunk; i++) {\n    float x = step * ((float) i - 0.5);\n    partial_sum += 4.0 / (1.0 + x * x);\n}\n\n// Each work-item stores its partial sum in the workgroup array\ninsideWorkGroup[lid] = partial_sum;\n\n// Synchronize all threads within the workgroup\nbarrier(CLK_LOCAL_MEM_FENCE);\n\nfloat local_pi = 0;\n\n// Only work-item 0 of each workgroup perform the reduction\n// of that workgroup\nif(lid == 0) {\n    const uint length = lid + get_local_size(0);\n    for (uint i = lid; i<length; i++) {\n        local_pi += insideWorkGroup[i];\n    }\n// It store the workgroup sum\n// Final reduction, between block, is done out by CPU\n    workGroupBuffer[get_group_id(0)] = local_pi;\n}\n}\n\n);\n\nint main( int argc, char* argv[] )\n{\n    int i=0;\n    float pi;\n    float *pi_partial;\n    size_t maxWorkGroupSize;\n    cl_int err;\n    cl_mem memObjects;\n    int niter, chunks, workGroups;\n    size_t globalWorkSize;\n    size_t localWorkSize;\n    cl_event event;\n\n    cl_platform_id platform;        // OpenCL platform\n    cl_device_id device_id;           // device ID\n    cl_context context;               // context\n    cl_command_queue queue;           // command queue\n    cl_program program;               // program\n    cl_kernel kernel;                 // kernel\n\n    niter = 262144;\n    chunks=64;\n\n    err = clGetPlatformIDs(1, &platform, NULL);\n\n    // Get ID for the device\n    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &device_id, NULL);\n    clGetDeviceInfo(device_id, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(size_t),\n                    &maxWorkGroupSize, NULL);\n    workGroups = ceil((float)(niter/maxWorkGroupSize/chunks));\n\n    pi_partial = (float*)malloc(sizeof(float)*workGroups);\n\n    // Create a context\n    context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);\n\n    // Create a command queue\n    queue = clCreateCommandQueue(context, device_id, CL_QUEUE_PROFILING_ENABLE, &err);\n\n    // Create the compute program from the source buffer\n\n    program = clCreateProgramWithSource(context, 1,\n                                        &kernelSource, NULL, &err);\n    // Build the program executable\n    err = clBuildProgram(program, 0, NULL, NULL, NULL, NULL);\n    localWorkSize =  maxWorkGroupSize;\n    globalWorkSize = niter / chunks;\n\n    // Create the compute kernel in the program we wish to run\n    kernel = clCreateKernel(program, \"Pi\", &err);\n\n    // Create the input and output arrays in device memory for our calculation\n    memObjects = clCreateBuffer(context, CL_MEM_READ_WRITE,\n                                sizeof(float)*workGroups, NULL, &err);\n\n    err |= clSetKernelArg(kernel, 0, sizeof(cl_mem), &memObjects);\n    err  = clSetKernelArg(kernel, 1, sizeof(float)*maxWorkGroupSize, NULL);\n    err |= clSetKernelArg(kernel, 2, sizeof(unsigned int), &niter);\n    err |= clSetKernelArg(kernel, 3, sizeof(unsigned int), &chunks);\n\n    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalWorkSize, &localWorkSize,\n0, NULL, &event);\n    clFinish(queue);\n    cl_ulong time_start;\n    cl_ulong time_end;\n\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n    clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n\n    double times = time_end - time_start;\n    printf(\"OpenCL Kernel Execution time is: %0.4f\\n\", times / 1000000.0);\n    err = clEnqueueReadBuffer(queue, memObjects, CL_TRUE, 0,\n                              sizeof(float)*workGroups, pi_partial, 0, NULL, NULL);\n    pi=0;\n\n    for(i=0; i<workGroups; i++) {\n        pi += pi_partial[i];\n    }\n    pi *= (1.0/(float)niter);\n    printf(\"final result: %f\\n\", pi);\n\n    // release OpenCL resources\n    clReleaseMemObject(memObjects);\n    clReleaseProgram(program);\n    clReleaseKernel(kernel);\n    clReleaseCommandQueue(queue);\n    clReleaseContext(context);\n\n    //release host memory\n    free(pi_partial);\n    return 0;\n}\n```\n这里进行部分代码的分析，首先阐述chunk的概念，这里用如下的图进行表示。\n\n<center>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1329.png\" height=\"260\" width=\"450\">\n</center>\n\n所以这里的chunk指的是多个work group组成的组合。因此work group的数量是由所规定的chunk大小以及数据规模决定的。\n```cpp\n    workGroups = ceil((float)(niter/maxWorkGroupSize/chunks));\n\n    pi_partial = (float*)malloc(sizeof(float)*workGroups);\n```\n再向下理解，这里的local work size还是每个work group内的work item数量，这里取的最大，也就是本机器的256。但是这里的global work size与global size不同，他是将一个chunk的所有work group作为一个global work group，那么global work size就是chunks的数目。\n```cpp\n    localWorkSize =  maxWorkGroupSize;\n    globalWorkSize = niter / chunks;\n```\n这样就可以去理解kernel函数的内容了，首先它是将数据niter分成了chunk份，这就是为什么niter必须是chunk的整数倍，然后每个chunk内work item的数目就是要并行计算的数目，也就是每个chunk内做并行。\n```cpp\nfor(uint i=gid*chunk; i<(gid*chunk)+chunk; i++) {\n    float x = step * ((float) i - 0.5);\n    partial_sum += 4.0 / (1.0 + x * x);\n}\n```\n然后对每个chunk的计算结果进行保存，并在lid为0的work item中进行结果的归约，然后将归约的结果作为这个work group（其实是这个chunk）的运算结果，然后这些chunk的结果要在kernel函数外，也就是CPU当中进行归约，从而计算出最终的结果。对应代码如下。\n```cpp\n// Each work-item stores its partial sum in the workgroup array\ninsideWorkGroup[lid] = partial_sum;\n\n// Synchronize all threads within the workgroup\nbarrier(CLK_LOCAL_MEM_FENCE);\n\nfloat local_pi = 0;\n\n// Only work-item 0 of each workgroup perform the reduction\n// of that workgroup\nif(lid == 0) {\n    const uint length = lid + get_local_size(0);\n    for (uint i = lid; i<length; i++) {\n        local_pi += insideWorkGroup[i];\n    }\n// It store the workgroup sum\n// Final reduction, between block, is done out by CPU\n    workGroupBuffer[get_group_id(0)] = local_pi;\n}\n```\n这样就完成了PI计算的并行过程。\n\n***实验***\n\n由于这里的local size规定死了为最大的256，所以不宜做改变，所以这里我们更改chunks的数量。也就是修改在一个for内并行计算的部分，分别计算chunk数为1、2、4、8、16、32、64的时间消耗。这里的数据规模取26214400。\n输出结果如下。\n\n|     | 1  | 2 | 4 | 8 | 16 | 32 | 64 |\n|  ----  | ----  | ---- | ---- | ---- | ---- | ---- | ---- |\n| t/ms  | 8.3137 | 5.2593 | 2.5912 | 1.6443 | 1.2119 | 1.0183 | 0.9311 |\n\n可以看到，随着chunk数目的增加，计算的效率越来越高，计算的时间消耗越来越少，并且计算时间的优化效果越来越弱。这说明在同一个chunk内的并行数越多，计算效率越高。\n\n下面选择在chunk为64的条件下进行计算效率随数据规模的变化情况，这里的数据规模我们选择131072、262144、2621440、26214400、262144000。\n\n|     | 131072  | 262144 | 2621440 | 26214400 | 262144000 |\n|  ----  | ----  | ---- | ---- | ---- | ---- |\n| t/ms  | 0.0324 | 0.0342 | 0.1155 | 0.9268 | 10.7321 |\n\n可以看到，随着数据规模的增加，计算时间一开始增加缓慢，到后面也是以类似10倍的速率增长，原因与向量加法中的阐述相同。再增加数据规模，增加的时间就不是并行后的结果了，而是串行地等待前面的并行计算完，之后的再进行并行计算，所以时间的增量几乎就是数据规模的增量。\n\n## 环境\n\nmacOS Big Sur Version 11.1","slug":"HPC-OpenCL-md","published":1,"updated":"2022-01-26T10:29:48.774Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckzk2pxlu0000i3oeas710l53","content":"<h2 id=\"OpenCL环境安装配置\"><a href=\"#OpenCL环境安装配置\" class=\"headerlink\" title=\"OpenCL环境安装配置\"></a>OpenCL环境安装配置</h2><p>由于我使用的机器是MacBook Pro 2020，mac系统里面已经集成了OpenCL的SDK，所以不需要去另外下载，只需要在Xcode工程中将其加入进来就可以进行OpenCL的开发了。下面展示的是工程文件中配置OpenCL环境的过程。</p>\n<p>（1）首先在Xcode工程文件的Build Phases下导入OpenCL的库文件。</p>\n<p>（2）导入后，就可以看到在旁边的文件栏有OpenCL的.framework文件。</p>\n<p>（3）然后就是在相应的cpp文件中写入对应的头文件即可。</p>\n<p>这样OpenCL在Xcode工程文件内的编译环境就配置好了。</p>\n<h2 id=\"本机设备参数分析\"><a href=\"#本机设备参数分析\" class=\"headerlink\" title=\"本机设备参数分析\"></a>本机设备参数分析</h2><p>这里方法是使用OpenCL的代码进行设备参数信息的输出。由于代码过长而且与并行计算相关性不大就不予展示。</p>\n<p>所以下面就是对OpenCL代码的输出进行分析。</p>\n<p>首先是本机OpenCL的版本，是OpenCL1.2。</p>\n<p>然后下面可以看到本机的两个Device，一个是CPU，一个是GPU。</p>\n<p>这里重点描述GPU的硬件设备参数，主要信息如下：</p>\n<p>GPU型号：                        Intel(R) Iris(TM) Plus Graphics</p>\n<p>最大计算单元个数(work group)：    64</p>\n<p>最大work item维度:                3</p>\n<p>work item每个维度的容量：        256</p>\n<p>work group容量：                    256</p>\n<p>该Device的id：                    0x7fff0000</p>\n<p>global的内存大小：                1610612736</p>\n<p>device的缓存大小：                65536</p>\n<p>device的local内存大小：            65536</p>\n<h2 id=\"向量运算代码分析与实验\"><a href=\"#向量运算代码分析与实验\" class=\"headerlink\" title=\"向量运算代码分析与实验\"></a>向量运算代码分析与实验</h2><p>代码如下</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;math.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;OpenCL/opencl.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"><span class=\"comment\">// OpenCL kernel. Each work item takes care of one element of c</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> KERNEL(...) #__VA_ARGS__</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *kernelSource = <span class=\"built_in\">KERNEL</span>(</span><br><span class=\"line\">_Pragma (<span class=\"string\">&quot;OPENCL EXTENSION cl_khr_fp64:enable&quot;</span>)</span><br><span class=\"line\">__kernel <span class=\"keyword\">void</span> <span class=\"built_in\">vecAdd</span>(  __global <span class=\"keyword\">float</span> *a,</span><br><span class=\"line\">                       __global <span class=\"keyword\">float</span> *b,</span><br><span class=\"line\">                       __global <span class=\"keyword\">float</span> *c,</span><br><span class=\"line\">                       <span class=\"keyword\">const</span> <span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span> n)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">//Get our global thread ID</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> id = <span class=\"built_in\">get_global_id</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\">                                                              </span><br><span class=\"line\">    <span class=\"comment\">//Make sure we do not go out of bounds</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (id &lt; n)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c[id] = a[id] + b[id];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">);</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">( <span class=\"keyword\">int</span> argc, <span class=\"keyword\">char</span>* argv[] )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">size_t</span> globalSize, localSize;</span><br><span class=\"line\">    cl_int err;</span><br><span class=\"line\">    cl_event event;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> sum = <span class=\"number\">0.</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Length of vectors</span></span><br><span class=\"line\">    <span class=\"comment\">// unsigned int n = 100000;</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> n = <span class=\"number\">10000000</span>;</span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    // Host input vectors</span></span><br><span class=\"line\"><span class=\"comment\">    double *h_a;</span></span><br><span class=\"line\"><span class=\"comment\">    double *h_b;</span></span><br><span class=\"line\"><span class=\"comment\">    // Host output vector</span></span><br><span class=\"line\"><span class=\"comment\">    double *h_c;</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// Host input vectors</span></span><br><span class=\"line\">    <span class=\"keyword\">float</span> *h_a;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> *h_b;</span><br><span class=\"line\">    <span class=\"comment\">// Host output vector</span></span><br><span class=\"line\">    <span class=\"keyword\">float</span> *h_c;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Device input buffers</span></span><br><span class=\"line\">    cl_mem d_a;</span><br><span class=\"line\">    cl_mem d_b;</span><br><span class=\"line\">    <span class=\"comment\">// Device output buffer</span></span><br><span class=\"line\">    cl_mem d_c;</span><br><span class=\"line\"> </span><br><span class=\"line\">    cl_platform_id platform;        <span class=\"comment\">// OpenCL platform</span></span><br><span class=\"line\">    cl_device_id device_id;           <span class=\"comment\">// device ID</span></span><br><span class=\"line\">    cl_context context;               <span class=\"comment\">// context</span></span><br><span class=\"line\">    cl_command_queue queue;           <span class=\"comment\">// command queue</span></span><br><span class=\"line\">    cl_program program;               <span class=\"comment\">// program</span></span><br><span class=\"line\">    cl_kernel kernel;                 <span class=\"comment\">// kernel</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Size, in bytes, of each vector</span></span><br><span class=\"line\">    <span class=\"keyword\">size_t</span> bytes = n * <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Allocate memory for each vector on host</span></span><br><span class=\"line\">    h_a = (<span class=\"keyword\">float</span>*)<span class=\"built_in\">malloc</span>(bytes);</span><br><span class=\"line\">    h_b = (<span class=\"keyword\">float</span>*)<span class=\"built_in\">malloc</span>(bytes);</span><br><span class=\"line\">    h_c = (<span class=\"keyword\">float</span>*)<span class=\"built_in\">malloc</span>(bytes);</span><br><span class=\"line\">    <span class=\"comment\">// Initialize vectors on host</span></span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"keyword\">for</span>(  i = <span class=\"number\">0</span>; i &lt; n; i++ )&#123;</span><br><span class=\"line\">        h_a[i] = <span class=\"built_in\">sinf</span>(i)*<span class=\"built_in\">sinf</span>(i);</span><br><span class=\"line\">        h_b[i] = <span class=\"built_in\">cosf</span>(i)*<span class=\"built_in\">cosf</span>(i);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// size_t globalSize, localSize;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//cl_int err;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Number of work items in each local work group</span></span><br><span class=\"line\">    localSize = <span class=\"number\">64</span>;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Number of total work items - localSize must be devisor</span></span><br><span class=\"line\">    globalSize =(<span class=\"keyword\">size_t</span>)<span class=\"built_in\">ceil</span>(n/(<span class=\"keyword\">float</span>)localSize)*localSize;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Bind to platform</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetPlatformIDs</span>(<span class=\"number\">1</span>, &amp;platform, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Get ID for the device</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetDeviceIDs</span>(platform, CL_DEVICE_TYPE_GPU, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create a context</span></span><br><span class=\"line\">    context = <span class=\"built_in\">clCreateContext</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create a command queue</span></span><br><span class=\"line\">    <span class=\"comment\">//queue = clCreateCommandQueue(context, device_id, 0, &amp;err);</span></span><br><span class=\"line\">    queue = <span class=\"built_in\">clCreateCommandQueue</span>(context, device_id, CL_QUEUE_PROFILING_ENABLE, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create the compute program from the source buffer</span></span><br><span class=\"line\">    program = <span class=\"built_in\">clCreateProgramWithSource</span>(context, <span class=\"number\">1</span>,</span><br><span class=\"line\">                            (<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> **) &amp; kernelSource, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Build the program executable</span></span><br><span class=\"line\">    <span class=\"built_in\">clBuildProgram</span>(program, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create the compute kernel in the program we wish to run</span></span><br><span class=\"line\">    kernel = <span class=\"built_in\">clCreateKernel</span>(program, <span class=\"string\">&quot;vecAdd&quot;</span>, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create the input and output arrays in device memory for our calculation</span></span><br><span class=\"line\">    d_a = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_READ_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    d_b = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_READ_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    d_c = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_WRITE_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Write our data set into the input array in device memory</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clEnqueueWriteBuffer</span>(queue, d_a, CL_TRUE, <span class=\"number\">0</span>,</span><br><span class=\"line\">                                   bytes, h_a, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clEnqueueWriteBuffer</span>(queue, d_b, CL_TRUE, <span class=\"number\">0</span>,</span><br><span class=\"line\">                                   bytes, h_b, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Set the arguments to our compute kernel</span></span><br><span class=\"line\">    err  = <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">0</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_a);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">1</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_b);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">2</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_c);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">3</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span>), &amp;n);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Execute the kernel over the entire range of the data set</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clEnqueueNDRangeKernel</span>(queue, kernel, <span class=\"number\">1</span>, <span class=\"literal\">NULL</span>, &amp;globalSize, &amp;localSize,</span><br><span class=\"line\">                                                              <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, &amp;event);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Wait for the command queue to get serviced before reading back results</span></span><br><span class=\"line\">    <span class=\"built_in\">clFinish</span>(queue);</span><br><span class=\"line\">    cl_ulong time_start;</span><br><span class=\"line\">    cl_ulong time_end;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_START, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_start), &amp;time_start, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_END, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_end), &amp;time_end, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">double</span> times = time_end - time_start;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;OpenCL Kernel Execution time is: %0.4f\\n&quot;</span>, times / <span class=\"number\">1000000.0</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// Read the results from the device</span></span><br><span class=\"line\">    <span class=\"built_in\">clEnqueueReadBuffer</span>(queue, d_c, CL_TRUE, <span class=\"number\">0</span>, bytes, h_c, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span> );</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">//Sum up vector c and print result divided by n, this should equal 1 within error</span></span><br><span class=\"line\">    <span class=\"comment\">//double sum = 0;</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">0</span>; i&lt;n; i++)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        sum += h_c[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;final result: %lf\\n&quot;</span>, sum/n);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// release OpenCL resources</span></span><br><span class=\"line\">    <span class=\"built_in\">clReleaseMemObject</span>(d_a);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseMemObject</span>(d_b);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseMemObject</span>(d_c);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseProgram</span>(program);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseKernel</span>(kernel);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseCommandQueue</span>(queue);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseContext</span>(context);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">//release host memory</span></span><br><span class=\"line\">    <span class=\"built_in\">free</span>(h_a);</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(h_b);</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(h_c);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>这里进行代码的详细分析，后面的PI计算相似的内容就不再赘述了。</p>\n<p>首先是一个可变长的宏，用来以参数的形式定义kernel函数。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> KERNEL(...) #__VA_ARGS__</span></span><br></pre></td></tr></table></figure>\n<p>接下来用字符串的形式去定义kernel函数，这样它不会与host的代码一起编译，在调用时才会被编译。同时，为了OpenCL对64位浮点运算的支持，需要加一个如下的声明。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_Pragma (<span class=\"string\">&quot;OPENCL EXTENSION cl_khr_fp64:enable&quot;</span>)</span><br></pre></td></tr></table></figure>\n<p>但是这里我们只用到了float，float类型的参数是不需要这样的声明的。</p>\n<p>然后是对kernel内容的解释，首先要获取当前所在work item的global id，因为由上面的GPU信息可以知道，id可以有三个维度，但是我们做的是向量加法，所以只需要用到它的第一个维度，所以里面的参数为0。每个work item负责结果向量中一个位置的计算，对应如下代码。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//Get our global thread ID</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> id = <span class=\"built_in\">get_global_id</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\">                                                              </span><br><span class=\"line\">    <span class=\"comment\">//Make sure we do not go out of bounds</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (id &lt; n)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c[id] = a[id] + b[id];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>这样就定义好了kernel函数，也就是并行的部分。</p>\n<p>下面开始为OpenCL在本机的运行做准备。</p>\n<p>因为我们的向量加法，既要在host输入，也要在device中并行计算。所以要在host和device上分别开辟相应的空间。然后在host的变量内存空间对两个向量进行赋值，一个是cos^2(x)一个是sin^2(x)，这样保证每个元素相加得到的结果为1。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Host input vectors</span></span><br><span class=\"line\"><span class=\"keyword\">float</span> *h_a;</span><br><span class=\"line\"><span class=\"keyword\">float</span> *h_b;</span><br><span class=\"line\"><span class=\"comment\">// Host output vector</span></span><br><span class=\"line\"><span class=\"keyword\">float</span> *h_c;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// Device input buffers</span></span><br><span class=\"line\">cl_mem d_a;</span><br><span class=\"line\">cl_mem d_b;</span><br><span class=\"line\"><span class=\"comment\">// Device output buffer</span></span><br><span class=\"line\">cl_mem d_c;</span><br></pre></td></tr></table></figure>\n<p>下面开始设定并行计算的global size和local size，其中local size是指每个work group内的work item数目，其中global size是指所有work group内的work item数目之和，也就是所有参与并行计算的work item数目。</p>\n<p>这里需要让每个work item负责输出向量一个位置的运算，所以global size不仅需要是local size的整数倍，而且还要大于等于向量的长度。<br>所以代码如下。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Number of work items in each local work group</span></span><br><span class=\"line\">localSize = <span class=\"number\">64</span>;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// Number of total work items - localSize must be devisor</span></span><br><span class=\"line\">globalSize =(<span class=\"keyword\">size_t</span>)<span class=\"built_in\">ceil</span>(n/(<span class=\"keyword\">float</span>)localSize)*localSize;</span><br></pre></td></tr></table></figure>\n<p>然后获取要使用的platform id，它可以看作是一个handel，用来获取相应的device id，也就是上面设备信息给出的0x7fff0000，然后再根据它来获取设备当前的运行状态或运行环境，从而创建命令队列。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Bind to platform</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetPlatformIDs</span>(<span class=\"number\">1</span>, &amp;platform, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Get ID for the device</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetDeviceIDs</span>(platform, CL_DEVICE_TYPE_GPU, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create a context</span></span><br><span class=\"line\">    context = <span class=\"built_in\">clCreateContext</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create a command queue</span></span><br><span class=\"line\">    <span class=\"comment\">//queue = clCreateCommandQueue(context, device_id, 0, &amp;err);</span></span><br><span class=\"line\">    queue = <span class=\"built_in\">clCreateCommandQueue</span>(context, device_id, CL_QUEUE_PROFILING_ENABLE, &amp;err);</span><br></pre></td></tr></table></figure>\n<p>这里注意到在创建命令队列时与源代码不同的是我加入了参数CL_QUEUE_PROFILING_ENABLE，用来获取kernel的运行时间。方便后面的实验。</p>\n<p>然后就是将以变量形式声明的kernel进行实例化，代码运行至这里才会进行这个kernel函数的编译。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Create the compute program from the source buffer</span></span><br><span class=\"line\">    program = <span class=\"built_in\">clCreateProgramWithSource</span>(context, <span class=\"number\">1</span>,</span><br><span class=\"line\">                            (<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> **) &amp; kernelSource, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Build the program executable</span></span><br><span class=\"line\">    <span class=\"built_in\">clBuildProgram</span>(program, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create the compute kernel in the program we wish to run</span></span><br><span class=\"line\">    kernel = <span class=\"built_in\">clCreateKernel</span>(program, <span class=\"string\">&quot;vecAdd&quot;</span>, &amp;err);</span><br></pre></td></tr></table></figure>\n<p>下面是在Device中为上面需要的内存开辟地址空间，并且从host的内存中写入Device的内存中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Create the input and output arrays in device memory for our calculation</span></span><br><span class=\"line\">    d_a = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_READ_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    d_b = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_READ_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    d_c = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_WRITE_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Write our data set into the input array in device memory</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clEnqueueWriteBuffer</span>(queue, d_a, CL_TRUE, <span class=\"number\">0</span>,</span><br><span class=\"line\">                                   bytes, h_a, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clEnqueueWriteBuffer</span>(queue, d_b, CL_TRUE, <span class=\"number\">0</span>,</span><br><span class=\"line\">                                   bytes, h_b, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br></pre></td></tr></table></figure>\n<p>然后对kernel函数进行参数的传递，然后开始并行的运算。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Set the arguments to our compute kernel</span></span><br><span class=\"line\">err  = <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">0</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_a);</span><br><span class=\"line\">err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">1</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_b);</span><br><span class=\"line\">err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">2</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_c);</span><br><span class=\"line\">err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">3</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span>), &amp;n);</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// Execute the kernel over the entire range of the data set</span></span><br><span class=\"line\">err = <span class=\"built_in\">clEnqueueNDRangeKernel</span>(queue, kernel, <span class=\"number\">1</span>, <span class=\"literal\">NULL</span>, &amp;globalSize, &amp;localSize,</span><br><span class=\"line\">                                                          <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, &amp;event);</span><br></pre></td></tr></table></figure>\n<p>最后等待命令队列的相应，从而完成并行计算的部分，最后要对设备上的一些内存空间进行释放。</p>\n<p>这里为了方便实验，进行了kernel代码运行时间的计算，使用的是OpenCL自带的函数，操作如下所示。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Wait for the command queue to get serviced before reading back results</span></span><br><span class=\"line\"><span class=\"built_in\">clFinish</span>(queue);</span><br><span class=\"line\">cl_ulong time_start;</span><br><span class=\"line\">cl_ulong time_end;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_START, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_start), &amp;time_start, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"><span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_END, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_end), &amp;time_end, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">double</span> times = time_end - time_start;</span><br><span class=\"line\"><span class=\"built_in\">printf</span>(<span class=\"string\">&quot;OpenCL Kernel Execution time is: %0.4f\\n&quot;</span>, times / <span class=\"number\">1000000.0</span>);</span><br></pre></td></tr></table></figure>\n<p><em><strong>实验</strong></em></p>\n<p>这里我们探究随着local size，也就是每个work group内的work item数目的变化，运行效率（计算时间）怎么变化。</p>\n<p>为了对比明显，这里的数据规模令n = 10000000，然后在local size分别为1，2，4，8，16，32，64的情况下进行实验。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>1</th>\n<th>2</th>\n<th>4</th>\n<th>8</th>\n<th>16</th>\n<th>32</th>\n<th>64</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>t/ms</td>\n<td>53.5526</td>\n<td>25.3358</td>\n<td>13.0841</td>\n<td>7.8111</td>\n<td>3.5446</td>\n<td>2.9774</td>\n<td>2.8876</td>\n</tr>\n</tbody></table>\n<p>可以发现随着Local Size数的增加，计算时间逐渐减小，计算效率逐渐增加，但是增加的幅度越来越小，越来越趋于平稳。</p>\n<p>原因是，当Local Size比较小时，64个work group无法为所有的元素计算提供资源，只能串行地进行多次并行的计算，来达到完成计算任务的目的。所以也就增加了计算时间消耗。</p>\n<p>下面探究在local size为64的情况下，随着数据规模的增加，计算效率的变化情况。这里的数据规模我们取10000、100000、1000000、10000000、100000000。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>10000</th>\n<th>100000</th>\n<th>1000000</th>\n<th>10000000</th>\n<th>100000000</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>t/ms</td>\n<td>0.0264</td>\n<td>0.0674</td>\n<td>0.4833</td>\n<td>2.8852</td>\n<td>28.6238</td>\n</tr>\n</tbody></table>\n<p>可以发现随着数据规模的增加，一开始的时间消耗增长较慢，后来就几乎与数据规模的增长规模相同了。</p>\n<p>原因就是当数据规模没那么大时，所有work group的所有work item资源没有被全部利用，当增加数据规模，可以使用更多的并行资源，从而与计算时间的增加进行平衡。但是当数据规模很大时，总共需要的work item数目超过了本机可以提供的数目（256 * 64），再增加数据规模，增加的时间就不是并行后的结果了，而是串行地等待前面的并行计算完，之后的再进行并行计算，所以时间的增量几乎就是数据规模的增量。</p>\n<h2 id=\"PI值运算代码分析与实验\"><a href=\"#PI值运算代码分析与实验\" class=\"headerlink\" title=\"PI值运算代码分析与实验\"></a>PI值运算代码分析与实验</h2><p>代码如下</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;string.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;math.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;OpenCL/opencl.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// OpenCL kernel. many workGroups compute n iterations</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> KERNEL(...) #__VA_ARGS__</span></span><br><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">char</span> * kernelSource =  <span class=\"built_in\">KERNEL</span>(</span><br><span class=\"line\">__kernel <span class=\"keyword\">void</span> <span class=\"built_in\">Pi</span>(__global <span class=\"keyword\">float</span> *workGroupBuffer, <span class=\"comment\">// 0..NumWorkGroups-1</span></span><br><span class=\"line\">__local <span class=\"keyword\">float</span> *insideWorkGroup,  <span class=\"comment\">// 0..workGroupSize-1</span></span><br><span class=\"line\"><span class=\"keyword\">const</span> uint n,        <span class=\"comment\">// Total iterations</span></span><br><span class=\"line\"><span class=\"keyword\">const</span> uint chunk)        <span class=\"comment\">// Chunk size</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">const</span> uint lid = <span class=\"built_in\">get_local_id</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\"><span class=\"keyword\">const</span> uint gid = <span class=\"built_in\">get_global_id</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">float</span> step = (<span class=\"number\">1.0</span>/(<span class=\"keyword\">float</span>)n);</span><br><span class=\"line\"><span class=\"keyword\">float</span> partial_sum = <span class=\"number\">0.0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Each work-item computes chunk iterations</span></span><br><span class=\"line\"><span class=\"keyword\">for</span>(uint i=gid*chunk; i&lt;(gid*chunk)+chunk; i++) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> x = step * ((<span class=\"keyword\">float</span>) i - <span class=\"number\">0.5</span>);</span><br><span class=\"line\">    partial_sum += <span class=\"number\">4.0</span> / (<span class=\"number\">1.0</span> + x * x);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Each work-item stores its partial sum in the workgroup array</span></span><br><span class=\"line\">insideWorkGroup[lid] = partial_sum;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Synchronize all threads within the workgroup</span></span><br><span class=\"line\"><span class=\"built_in\">barrier</span>(CLK_LOCAL_MEM_FENCE);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">float</span> local_pi = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Only work-item 0 of each workgroup perform the reduction</span></span><br><span class=\"line\"><span class=\"comment\">// of that workgroup</span></span><br><span class=\"line\"><span class=\"keyword\">if</span>(lid == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> uint length = lid + <span class=\"built_in\">get_local_size</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (uint i = lid; i&lt;length; i++) &#123;</span><br><span class=\"line\">        local_pi += insideWorkGroup[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">// It store the workgroup sum</span></span><br><span class=\"line\"><span class=\"comment\">// Final reduction, between block, is done out by CPU</span></span><br><span class=\"line\">    workGroupBuffer[<span class=\"built_in\">get_group_id</span>(<span class=\"number\">0</span>)] = local_pi;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">( <span class=\"keyword\">int</span> argc, <span class=\"keyword\">char</span>* argv[] )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> pi;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> *pi_partial;</span><br><span class=\"line\">    <span class=\"keyword\">size_t</span> maxWorkGroupSize;</span><br><span class=\"line\">    cl_int err;</span><br><span class=\"line\">    cl_mem memObjects;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> niter, chunks, workGroups;</span><br><span class=\"line\">    <span class=\"keyword\">size_t</span> globalWorkSize;</span><br><span class=\"line\">    <span class=\"keyword\">size_t</span> localWorkSize;</span><br><span class=\"line\">    cl_event event;</span><br><span class=\"line\"></span><br><span class=\"line\">    cl_platform_id platform;        <span class=\"comment\">// OpenCL platform</span></span><br><span class=\"line\">    cl_device_id device_id;           <span class=\"comment\">// device ID</span></span><br><span class=\"line\">    cl_context context;               <span class=\"comment\">// context</span></span><br><span class=\"line\">    cl_command_queue queue;           <span class=\"comment\">// command queue</span></span><br><span class=\"line\">    cl_program program;               <span class=\"comment\">// program</span></span><br><span class=\"line\">    cl_kernel kernel;                 <span class=\"comment\">// kernel</span></span><br><span class=\"line\"></span><br><span class=\"line\">    niter = <span class=\"number\">262144</span>;</span><br><span class=\"line\">    chunks=<span class=\"number\">64</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetPlatformIDs</span>(<span class=\"number\">1</span>, &amp;platform, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Get ID for the device</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetDeviceIDs</span>(platform, CL_DEVICE_TYPE_GPU, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"built_in\">clGetDeviceInfo</span>(device_id, CL_DEVICE_MAX_WORK_GROUP_SIZE, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">size_t</span>),</span><br><span class=\"line\">                    &amp;maxWorkGroupSize, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    workGroups = <span class=\"built_in\">ceil</span>((<span class=\"keyword\">float</span>)(niter/maxWorkGroupSize/chunks));</span><br><span class=\"line\"></span><br><span class=\"line\">    pi_partial = (<span class=\"keyword\">float</span>*)<span class=\"built_in\">malloc</span>(<span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>)*workGroups);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Create a context</span></span><br><span class=\"line\">    context = <span class=\"built_in\">clCreateContext</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Create a command queue</span></span><br><span class=\"line\">    queue = <span class=\"built_in\">clCreateCommandQueue</span>(context, device_id, CL_QUEUE_PROFILING_ENABLE, &amp;err);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Create the compute program from the source buffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">    program = <span class=\"built_in\">clCreateProgramWithSource</span>(context, <span class=\"number\">1</span>,</span><br><span class=\"line\">                                        &amp;kernelSource, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\">    <span class=\"comment\">// Build the program executable</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clBuildProgram</span>(program, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    localWorkSize =  maxWorkGroupSize;</span><br><span class=\"line\">    globalWorkSize = niter / chunks;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Create the compute kernel in the program we wish to run</span></span><br><span class=\"line\">    kernel = <span class=\"built_in\">clCreateKernel</span>(program, <span class=\"string\">&quot;Pi&quot;</span>, &amp;err);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Create the input and output arrays in device memory for our calculation</span></span><br><span class=\"line\">    memObjects = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_READ_WRITE,</span><br><span class=\"line\">                                <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>)*workGroups, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"></span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">0</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;memObjects);</span><br><span class=\"line\">    err  = <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">1</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>)*maxWorkGroupSize, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">2</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span>), &amp;niter);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">3</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span>), &amp;chunks);</span><br><span class=\"line\"></span><br><span class=\"line\">    err = <span class=\"built_in\">clEnqueueNDRangeKernel</span>(queue, kernel, <span class=\"number\">1</span>, <span class=\"literal\">NULL</span>, &amp;globalWorkSize, &amp;localWorkSize,</span><br><span class=\"line\"><span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, &amp;event);</span><br><span class=\"line\">    <span class=\"built_in\">clFinish</span>(queue);</span><br><span class=\"line\">    cl_ulong time_start;</span><br><span class=\"line\">    cl_ulong time_end;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_START, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_start), &amp;time_start, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_END, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_end), &amp;time_end, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">double</span> times = time_end - time_start;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;OpenCL Kernel Execution time is: %0.4f\\n&quot;</span>, times / <span class=\"number\">1000000.0</span>);</span><br><span class=\"line\">    err = <span class=\"built_in\">clEnqueueReadBuffer</span>(queue, memObjects, CL_TRUE, <span class=\"number\">0</span>,</span><br><span class=\"line\">                              <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>)*workGroups, pi_partial, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    pi=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">0</span>; i&lt;workGroups; i++) &#123;</span><br><span class=\"line\">        pi += pi_partial[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    pi *= (<span class=\"number\">1.0</span>/(<span class=\"keyword\">float</span>)niter);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;final result: %f\\n&quot;</span>, pi);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// release OpenCL resources</span></span><br><span class=\"line\">    <span class=\"built_in\">clReleaseMemObject</span>(memObjects);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseProgram</span>(program);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseKernel</span>(kernel);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseCommandQueue</span>(queue);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseContext</span>(context);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//release host memory</span></span><br><span class=\"line\">    <span class=\"built_in\">free</span>(pi_partial);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里进行部分代码的分析，首先阐述chunk的概念，这里用如下的图进行表示。</p>\n<center>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1329.png\" height=\"260\" width=\"450\">\n</center>\n\n<p>所以这里的chunk指的是多个work group组成的组合。因此work group的数量是由所规定的chunk大小以及数据规模决定的。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">workGroups = <span class=\"built_in\">ceil</span>((<span class=\"keyword\">float</span>)(niter/maxWorkGroupSize/chunks));</span><br><span class=\"line\"></span><br><span class=\"line\">pi_partial = (<span class=\"keyword\">float</span>*)<span class=\"built_in\">malloc</span>(<span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>)*workGroups);</span><br></pre></td></tr></table></figure>\n<p>再向下理解，这里的local work size还是每个work group内的work item数量，这里取的最大，也就是本机器的256。但是这里的global work size与global size不同，他是将一个chunk的所有work group作为一个global work group，那么global work size就是chunks的数目。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">localWorkSize =  maxWorkGroupSize;</span><br><span class=\"line\">globalWorkSize = niter / chunks;</span><br></pre></td></tr></table></figure>\n<p>这样就可以去理解kernel函数的内容了，首先它是将数据niter分成了chunk份，这就是为什么niter必须是chunk的整数倍，然后每个chunk内work item的数目就是要并行计算的数目，也就是每个chunk内做并行。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(uint i=gid*chunk; i&lt;(gid*chunk)+chunk; i++) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> x = step * ((<span class=\"keyword\">float</span>) i - <span class=\"number\">0.5</span>);</span><br><span class=\"line\">    partial_sum += <span class=\"number\">4.0</span> / (<span class=\"number\">1.0</span> + x * x);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后对每个chunk的计算结果进行保存，并在lid为0的work item中进行结果的归约，然后将归约的结果作为这个work group（其实是这个chunk）的运算结果，然后这些chunk的结果要在kernel函数外，也就是CPU当中进行归约，从而计算出最终的结果。对应代码如下。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Each work-item stores its partial sum in the workgroup array</span></span><br><span class=\"line\">insideWorkGroup[lid] = partial_sum;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Synchronize all threads within the workgroup</span></span><br><span class=\"line\"><span class=\"built_in\">barrier</span>(CLK_LOCAL_MEM_FENCE);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">float</span> local_pi = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Only work-item 0 of each workgroup perform the reduction</span></span><br><span class=\"line\"><span class=\"comment\">// of that workgroup</span></span><br><span class=\"line\"><span class=\"keyword\">if</span>(lid == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> uint length = lid + <span class=\"built_in\">get_local_size</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (uint i = lid; i&lt;length; i++) &#123;</span><br><span class=\"line\">        local_pi += insideWorkGroup[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">// It store the workgroup sum</span></span><br><span class=\"line\"><span class=\"comment\">// Final reduction, between block, is done out by CPU</span></span><br><span class=\"line\">    workGroupBuffer[<span class=\"built_in\">get_group_id</span>(<span class=\"number\">0</span>)] = local_pi;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这样就完成了PI计算的并行过程。</p>\n<p><em><strong>实验</strong></em></p>\n<p>由于这里的local size规定死了为最大的256，所以不宜做改变，所以这里我们更改chunks的数量。也就是修改在一个for内并行计算的部分，分别计算chunk数为1、2、4、8、16、32、64的时间消耗。这里的数据规模取26214400。<br>输出结果如下。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>1</th>\n<th>2</th>\n<th>4</th>\n<th>8</th>\n<th>16</th>\n<th>32</th>\n<th>64</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>t/ms</td>\n<td>8.3137</td>\n<td>5.2593</td>\n<td>2.5912</td>\n<td>1.6443</td>\n<td>1.2119</td>\n<td>1.0183</td>\n<td>0.9311</td>\n</tr>\n</tbody></table>\n<p>可以看到，随着chunk数目的增加，计算的效率越来越高，计算的时间消耗越来越少，并且计算时间的优化效果越来越弱。这说明在同一个chunk内的并行数越多，计算效率越高。</p>\n<p>下面选择在chunk为64的条件下进行计算效率随数据规模的变化情况，这里的数据规模我们选择131072、262144、2621440、26214400、262144000。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>131072</th>\n<th>262144</th>\n<th>2621440</th>\n<th>26214400</th>\n<th>262144000</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>t/ms</td>\n<td>0.0324</td>\n<td>0.0342</td>\n<td>0.1155</td>\n<td>0.9268</td>\n<td>10.7321</td>\n</tr>\n</tbody></table>\n<p>可以看到，随着数据规模的增加，计算时间一开始增加缓慢，到后面也是以类似10倍的速率增长，原因与向量加法中的阐述相同。再增加数据规模，增加的时间就不是并行后的结果了，而是串行地等待前面的并行计算完，之后的再进行并行计算，所以时间的增量几乎就是数据规模的增量。</p>\n<h2 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h2><p>macOS Big Sur Version 11.1</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"OpenCL环境安装配置\"><a href=\"#OpenCL环境安装配置\" class=\"headerlink\" title=\"OpenCL环境安装配置\"></a>OpenCL环境安装配置</h2><p>由于我使用的机器是MacBook Pro 2020，mac系统里面已经集成了OpenCL的SDK，所以不需要去另外下载，只需要在Xcode工程中将其加入进来就可以进行OpenCL的开发了。下面展示的是工程文件中配置OpenCL环境的过程。</p>\n<p>（1）首先在Xcode工程文件的Build Phases下导入OpenCL的库文件。</p>\n<p>（2）导入后，就可以看到在旁边的文件栏有OpenCL的.framework文件。</p>\n<p>（3）然后就是在相应的cpp文件中写入对应的头文件即可。</p>\n<p>这样OpenCL在Xcode工程文件内的编译环境就配置好了。</p>\n<h2 id=\"本机设备参数分析\"><a href=\"#本机设备参数分析\" class=\"headerlink\" title=\"本机设备参数分析\"></a>本机设备参数分析</h2><p>这里方法是使用OpenCL的代码进行设备参数信息的输出。由于代码过长而且与并行计算相关性不大就不予展示。</p>\n<p>所以下面就是对OpenCL代码的输出进行分析。</p>\n<p>首先是本机OpenCL的版本，是OpenCL1.2。</p>\n<p>然后下面可以看到本机的两个Device，一个是CPU，一个是GPU。</p>\n<p>这里重点描述GPU的硬件设备参数，主要信息如下：</p>\n<p>GPU型号：                        Intel(R) Iris(TM) Plus Graphics</p>\n<p>最大计算单元个数(work group)：    64</p>\n<p>最大work item维度:                3</p>\n<p>work item每个维度的容量：        256</p>\n<p>work group容量：                    256</p>\n<p>该Device的id：                    0x7fff0000</p>\n<p>global的内存大小：                1610612736</p>\n<p>device的缓存大小：                65536</p>\n<p>device的local内存大小：            65536</p>\n<h2 id=\"向量运算代码分析与实验\"><a href=\"#向量运算代码分析与实验\" class=\"headerlink\" title=\"向量运算代码分析与实验\"></a>向量运算代码分析与实验</h2><p>代码如下</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;math.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;OpenCL/opencl.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"><span class=\"comment\">// OpenCL kernel. Each work item takes care of one element of c</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> KERNEL(...) #__VA_ARGS__</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *kernelSource = <span class=\"built_in\">KERNEL</span>(</span><br><span class=\"line\">_Pragma (<span class=\"string\">&quot;OPENCL EXTENSION cl_khr_fp64:enable&quot;</span>)</span><br><span class=\"line\">__kernel <span class=\"keyword\">void</span> <span class=\"built_in\">vecAdd</span>(  __global <span class=\"keyword\">float</span> *a,</span><br><span class=\"line\">                       __global <span class=\"keyword\">float</span> *b,</span><br><span class=\"line\">                       __global <span class=\"keyword\">float</span> *c,</span><br><span class=\"line\">                       <span class=\"keyword\">const</span> <span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span> n)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">//Get our global thread ID</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> id = <span class=\"built_in\">get_global_id</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\">                                                              </span><br><span class=\"line\">    <span class=\"comment\">//Make sure we do not go out of bounds</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (id &lt; n)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c[id] = a[id] + b[id];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">);</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">( <span class=\"keyword\">int</span> argc, <span class=\"keyword\">char</span>* argv[] )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">size_t</span> globalSize, localSize;</span><br><span class=\"line\">    cl_int err;</span><br><span class=\"line\">    cl_event event;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> sum = <span class=\"number\">0.</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Length of vectors</span></span><br><span class=\"line\">    <span class=\"comment\">// unsigned int n = 100000;</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> n = <span class=\"number\">10000000</span>;</span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    // Host input vectors</span></span><br><span class=\"line\"><span class=\"comment\">    double *h_a;</span></span><br><span class=\"line\"><span class=\"comment\">    double *h_b;</span></span><br><span class=\"line\"><span class=\"comment\">    // Host output vector</span></span><br><span class=\"line\"><span class=\"comment\">    double *h_c;</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// Host input vectors</span></span><br><span class=\"line\">    <span class=\"keyword\">float</span> *h_a;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> *h_b;</span><br><span class=\"line\">    <span class=\"comment\">// Host output vector</span></span><br><span class=\"line\">    <span class=\"keyword\">float</span> *h_c;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Device input buffers</span></span><br><span class=\"line\">    cl_mem d_a;</span><br><span class=\"line\">    cl_mem d_b;</span><br><span class=\"line\">    <span class=\"comment\">// Device output buffer</span></span><br><span class=\"line\">    cl_mem d_c;</span><br><span class=\"line\"> </span><br><span class=\"line\">    cl_platform_id platform;        <span class=\"comment\">// OpenCL platform</span></span><br><span class=\"line\">    cl_device_id device_id;           <span class=\"comment\">// device ID</span></span><br><span class=\"line\">    cl_context context;               <span class=\"comment\">// context</span></span><br><span class=\"line\">    cl_command_queue queue;           <span class=\"comment\">// command queue</span></span><br><span class=\"line\">    cl_program program;               <span class=\"comment\">// program</span></span><br><span class=\"line\">    cl_kernel kernel;                 <span class=\"comment\">// kernel</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Size, in bytes, of each vector</span></span><br><span class=\"line\">    <span class=\"keyword\">size_t</span> bytes = n * <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Allocate memory for each vector on host</span></span><br><span class=\"line\">    h_a = (<span class=\"keyword\">float</span>*)<span class=\"built_in\">malloc</span>(bytes);</span><br><span class=\"line\">    h_b = (<span class=\"keyword\">float</span>*)<span class=\"built_in\">malloc</span>(bytes);</span><br><span class=\"line\">    h_c = (<span class=\"keyword\">float</span>*)<span class=\"built_in\">malloc</span>(bytes);</span><br><span class=\"line\">    <span class=\"comment\">// Initialize vectors on host</span></span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"keyword\">for</span>(  i = <span class=\"number\">0</span>; i &lt; n; i++ )&#123;</span><br><span class=\"line\">        h_a[i] = <span class=\"built_in\">sinf</span>(i)*<span class=\"built_in\">sinf</span>(i);</span><br><span class=\"line\">        h_b[i] = <span class=\"built_in\">cosf</span>(i)*<span class=\"built_in\">cosf</span>(i);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// size_t globalSize, localSize;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//cl_int err;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Number of work items in each local work group</span></span><br><span class=\"line\">    localSize = <span class=\"number\">64</span>;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Number of total work items - localSize must be devisor</span></span><br><span class=\"line\">    globalSize =(<span class=\"keyword\">size_t</span>)<span class=\"built_in\">ceil</span>(n/(<span class=\"keyword\">float</span>)localSize)*localSize;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Bind to platform</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetPlatformIDs</span>(<span class=\"number\">1</span>, &amp;platform, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Get ID for the device</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetDeviceIDs</span>(platform, CL_DEVICE_TYPE_GPU, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create a context</span></span><br><span class=\"line\">    context = <span class=\"built_in\">clCreateContext</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create a command queue</span></span><br><span class=\"line\">    <span class=\"comment\">//queue = clCreateCommandQueue(context, device_id, 0, &amp;err);</span></span><br><span class=\"line\">    queue = <span class=\"built_in\">clCreateCommandQueue</span>(context, device_id, CL_QUEUE_PROFILING_ENABLE, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create the compute program from the source buffer</span></span><br><span class=\"line\">    program = <span class=\"built_in\">clCreateProgramWithSource</span>(context, <span class=\"number\">1</span>,</span><br><span class=\"line\">                            (<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> **) &amp; kernelSource, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Build the program executable</span></span><br><span class=\"line\">    <span class=\"built_in\">clBuildProgram</span>(program, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create the compute kernel in the program we wish to run</span></span><br><span class=\"line\">    kernel = <span class=\"built_in\">clCreateKernel</span>(program, <span class=\"string\">&quot;vecAdd&quot;</span>, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create the input and output arrays in device memory for our calculation</span></span><br><span class=\"line\">    d_a = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_READ_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    d_b = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_READ_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    d_c = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_WRITE_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Write our data set into the input array in device memory</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clEnqueueWriteBuffer</span>(queue, d_a, CL_TRUE, <span class=\"number\">0</span>,</span><br><span class=\"line\">                                   bytes, h_a, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clEnqueueWriteBuffer</span>(queue, d_b, CL_TRUE, <span class=\"number\">0</span>,</span><br><span class=\"line\">                                   bytes, h_b, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Set the arguments to our compute kernel</span></span><br><span class=\"line\">    err  = <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">0</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_a);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">1</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_b);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">2</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_c);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">3</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span>), &amp;n);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Execute the kernel over the entire range of the data set</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clEnqueueNDRangeKernel</span>(queue, kernel, <span class=\"number\">1</span>, <span class=\"literal\">NULL</span>, &amp;globalSize, &amp;localSize,</span><br><span class=\"line\">                                                              <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, &amp;event);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Wait for the command queue to get serviced before reading back results</span></span><br><span class=\"line\">    <span class=\"built_in\">clFinish</span>(queue);</span><br><span class=\"line\">    cl_ulong time_start;</span><br><span class=\"line\">    cl_ulong time_end;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_START, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_start), &amp;time_start, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_END, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_end), &amp;time_end, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">double</span> times = time_end - time_start;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;OpenCL Kernel Execution time is: %0.4f\\n&quot;</span>, times / <span class=\"number\">1000000.0</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// Read the results from the device</span></span><br><span class=\"line\">    <span class=\"built_in\">clEnqueueReadBuffer</span>(queue, d_c, CL_TRUE, <span class=\"number\">0</span>, bytes, h_c, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span> );</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">//Sum up vector c and print result divided by n, this should equal 1 within error</span></span><br><span class=\"line\">    <span class=\"comment\">//double sum = 0;</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">0</span>; i&lt;n; i++)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        sum += h_c[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;final result: %lf\\n&quot;</span>, sum/n);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// release OpenCL resources</span></span><br><span class=\"line\">    <span class=\"built_in\">clReleaseMemObject</span>(d_a);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseMemObject</span>(d_b);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseMemObject</span>(d_c);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseProgram</span>(program);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseKernel</span>(kernel);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseCommandQueue</span>(queue);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseContext</span>(context);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">//release host memory</span></span><br><span class=\"line\">    <span class=\"built_in\">free</span>(h_a);</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(h_b);</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(h_c);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>这里进行代码的详细分析，后面的PI计算相似的内容就不再赘述了。</p>\n<p>首先是一个可变长的宏，用来以参数的形式定义kernel函数。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> KERNEL(...) #__VA_ARGS__</span></span><br></pre></td></tr></table></figure>\n<p>接下来用字符串的形式去定义kernel函数，这样它不会与host的代码一起编译，在调用时才会被编译。同时，为了OpenCL对64位浮点运算的支持，需要加一个如下的声明。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_Pragma (<span class=\"string\">&quot;OPENCL EXTENSION cl_khr_fp64:enable&quot;</span>)</span><br></pre></td></tr></table></figure>\n<p>但是这里我们只用到了float，float类型的参数是不需要这样的声明的。</p>\n<p>然后是对kernel内容的解释，首先要获取当前所在work item的global id，因为由上面的GPU信息可以知道，id可以有三个维度，但是我们做的是向量加法，所以只需要用到它的第一个维度，所以里面的参数为0。每个work item负责结果向量中一个位置的计算，对应如下代码。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//Get our global thread ID</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> id = <span class=\"built_in\">get_global_id</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\">                                                              </span><br><span class=\"line\">    <span class=\"comment\">//Make sure we do not go out of bounds</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (id &lt; n)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c[id] = a[id] + b[id];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>这样就定义好了kernel函数，也就是并行的部分。</p>\n<p>下面开始为OpenCL在本机的运行做准备。</p>\n<p>因为我们的向量加法，既要在host输入，也要在device中并行计算。所以要在host和device上分别开辟相应的空间。然后在host的变量内存空间对两个向量进行赋值，一个是cos^2(x)一个是sin^2(x)，这样保证每个元素相加得到的结果为1。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Host input vectors</span></span><br><span class=\"line\"><span class=\"keyword\">float</span> *h_a;</span><br><span class=\"line\"><span class=\"keyword\">float</span> *h_b;</span><br><span class=\"line\"><span class=\"comment\">// Host output vector</span></span><br><span class=\"line\"><span class=\"keyword\">float</span> *h_c;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// Device input buffers</span></span><br><span class=\"line\">cl_mem d_a;</span><br><span class=\"line\">cl_mem d_b;</span><br><span class=\"line\"><span class=\"comment\">// Device output buffer</span></span><br><span class=\"line\">cl_mem d_c;</span><br></pre></td></tr></table></figure>\n<p>下面开始设定并行计算的global size和local size，其中local size是指每个work group内的work item数目，其中global size是指所有work group内的work item数目之和，也就是所有参与并行计算的work item数目。</p>\n<p>这里需要让每个work item负责输出向量一个位置的运算，所以global size不仅需要是local size的整数倍，而且还要大于等于向量的长度。<br>所以代码如下。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Number of work items in each local work group</span></span><br><span class=\"line\">localSize = <span class=\"number\">64</span>;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// Number of total work items - localSize must be devisor</span></span><br><span class=\"line\">globalSize =(<span class=\"keyword\">size_t</span>)<span class=\"built_in\">ceil</span>(n/(<span class=\"keyword\">float</span>)localSize)*localSize;</span><br></pre></td></tr></table></figure>\n<p>然后获取要使用的platform id，它可以看作是一个handel，用来获取相应的device id，也就是上面设备信息给出的0x7fff0000，然后再根据它来获取设备当前的运行状态或运行环境，从而创建命令队列。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Bind to platform</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetPlatformIDs</span>(<span class=\"number\">1</span>, &amp;platform, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Get ID for the device</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetDeviceIDs</span>(platform, CL_DEVICE_TYPE_GPU, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create a context</span></span><br><span class=\"line\">    context = <span class=\"built_in\">clCreateContext</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create a command queue</span></span><br><span class=\"line\">    <span class=\"comment\">//queue = clCreateCommandQueue(context, device_id, 0, &amp;err);</span></span><br><span class=\"line\">    queue = <span class=\"built_in\">clCreateCommandQueue</span>(context, device_id, CL_QUEUE_PROFILING_ENABLE, &amp;err);</span><br></pre></td></tr></table></figure>\n<p>这里注意到在创建命令队列时与源代码不同的是我加入了参数CL_QUEUE_PROFILING_ENABLE，用来获取kernel的运行时间。方便后面的实验。</p>\n<p>然后就是将以变量形式声明的kernel进行实例化，代码运行至这里才会进行这个kernel函数的编译。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Create the compute program from the source buffer</span></span><br><span class=\"line\">    program = <span class=\"built_in\">clCreateProgramWithSource</span>(context, <span class=\"number\">1</span>,</span><br><span class=\"line\">                            (<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> **) &amp; kernelSource, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Build the program executable</span></span><br><span class=\"line\">    <span class=\"built_in\">clBuildProgram</span>(program, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Create the compute kernel in the program we wish to run</span></span><br><span class=\"line\">    kernel = <span class=\"built_in\">clCreateKernel</span>(program, <span class=\"string\">&quot;vecAdd&quot;</span>, &amp;err);</span><br></pre></td></tr></table></figure>\n<p>下面是在Device中为上面需要的内存开辟地址空间，并且从host的内存中写入Device的内存中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Create the input and output arrays in device memory for our calculation</span></span><br><span class=\"line\">    d_a = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_READ_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    d_b = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_READ_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    d_c = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_WRITE_ONLY, bytes, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">// Write our data set into the input array in device memory</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clEnqueueWriteBuffer</span>(queue, d_a, CL_TRUE, <span class=\"number\">0</span>,</span><br><span class=\"line\">                                   bytes, h_a, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clEnqueueWriteBuffer</span>(queue, d_b, CL_TRUE, <span class=\"number\">0</span>,</span><br><span class=\"line\">                                   bytes, h_b, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br></pre></td></tr></table></figure>\n<p>然后对kernel函数进行参数的传递，然后开始并行的运算。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Set the arguments to our compute kernel</span></span><br><span class=\"line\">err  = <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">0</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_a);</span><br><span class=\"line\">err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">1</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_b);</span><br><span class=\"line\">err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">2</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;d_c);</span><br><span class=\"line\">err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">3</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span>), &amp;n);</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// Execute the kernel over the entire range of the data set</span></span><br><span class=\"line\">err = <span class=\"built_in\">clEnqueueNDRangeKernel</span>(queue, kernel, <span class=\"number\">1</span>, <span class=\"literal\">NULL</span>, &amp;globalSize, &amp;localSize,</span><br><span class=\"line\">                                                          <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, &amp;event);</span><br></pre></td></tr></table></figure>\n<p>最后等待命令队列的相应，从而完成并行计算的部分，最后要对设备上的一些内存空间进行释放。</p>\n<p>这里为了方便实验，进行了kernel代码运行时间的计算，使用的是OpenCL自带的函数，操作如下所示。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Wait for the command queue to get serviced before reading back results</span></span><br><span class=\"line\"><span class=\"built_in\">clFinish</span>(queue);</span><br><span class=\"line\">cl_ulong time_start;</span><br><span class=\"line\">cl_ulong time_end;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_START, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_start), &amp;time_start, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"><span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_END, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_end), &amp;time_end, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">double</span> times = time_end - time_start;</span><br><span class=\"line\"><span class=\"built_in\">printf</span>(<span class=\"string\">&quot;OpenCL Kernel Execution time is: %0.4f\\n&quot;</span>, times / <span class=\"number\">1000000.0</span>);</span><br></pre></td></tr></table></figure>\n<p><em><strong>实验</strong></em></p>\n<p>这里我们探究随着local size，也就是每个work group内的work item数目的变化，运行效率（计算时间）怎么变化。</p>\n<p>为了对比明显，这里的数据规模令n = 10000000，然后在local size分别为1，2，4，8，16，32，64的情况下进行实验。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>1</th>\n<th>2</th>\n<th>4</th>\n<th>8</th>\n<th>16</th>\n<th>32</th>\n<th>64</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>t/ms</td>\n<td>53.5526</td>\n<td>25.3358</td>\n<td>13.0841</td>\n<td>7.8111</td>\n<td>3.5446</td>\n<td>2.9774</td>\n<td>2.8876</td>\n</tr>\n</tbody></table>\n<p>可以发现随着Local Size数的增加，计算时间逐渐减小，计算效率逐渐增加，但是增加的幅度越来越小，越来越趋于平稳。</p>\n<p>原因是，当Local Size比较小时，64个work group无法为所有的元素计算提供资源，只能串行地进行多次并行的计算，来达到完成计算任务的目的。所以也就增加了计算时间消耗。</p>\n<p>下面探究在local size为64的情况下，随着数据规模的增加，计算效率的变化情况。这里的数据规模我们取10000、100000、1000000、10000000、100000000。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>10000</th>\n<th>100000</th>\n<th>1000000</th>\n<th>10000000</th>\n<th>100000000</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>t/ms</td>\n<td>0.0264</td>\n<td>0.0674</td>\n<td>0.4833</td>\n<td>2.8852</td>\n<td>28.6238</td>\n</tr>\n</tbody></table>\n<p>可以发现随着数据规模的增加，一开始的时间消耗增长较慢，后来就几乎与数据规模的增长规模相同了。</p>\n<p>原因就是当数据规模没那么大时，所有work group的所有work item资源没有被全部利用，当增加数据规模，可以使用更多的并行资源，从而与计算时间的增加进行平衡。但是当数据规模很大时，总共需要的work item数目超过了本机可以提供的数目（256 * 64），再增加数据规模，增加的时间就不是并行后的结果了，而是串行地等待前面的并行计算完，之后的再进行并行计算，所以时间的增量几乎就是数据规模的增量。</p>\n<h2 id=\"PI值运算代码分析与实验\"><a href=\"#PI值运算代码分析与实验\" class=\"headerlink\" title=\"PI值运算代码分析与实验\"></a>PI值运算代码分析与实验</h2><p>代码如下</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;string.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;math.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;OpenCL/opencl.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// OpenCL kernel. many workGroups compute n iterations</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> KERNEL(...) #__VA_ARGS__</span></span><br><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">char</span> * kernelSource =  <span class=\"built_in\">KERNEL</span>(</span><br><span class=\"line\">__kernel <span class=\"keyword\">void</span> <span class=\"built_in\">Pi</span>(__global <span class=\"keyword\">float</span> *workGroupBuffer, <span class=\"comment\">// 0..NumWorkGroups-1</span></span><br><span class=\"line\">__local <span class=\"keyword\">float</span> *insideWorkGroup,  <span class=\"comment\">// 0..workGroupSize-1</span></span><br><span class=\"line\"><span class=\"keyword\">const</span> uint n,        <span class=\"comment\">// Total iterations</span></span><br><span class=\"line\"><span class=\"keyword\">const</span> uint chunk)        <span class=\"comment\">// Chunk size</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">const</span> uint lid = <span class=\"built_in\">get_local_id</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\"><span class=\"keyword\">const</span> uint gid = <span class=\"built_in\">get_global_id</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">float</span> step = (<span class=\"number\">1.0</span>/(<span class=\"keyword\">float</span>)n);</span><br><span class=\"line\"><span class=\"keyword\">float</span> partial_sum = <span class=\"number\">0.0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Each work-item computes chunk iterations</span></span><br><span class=\"line\"><span class=\"keyword\">for</span>(uint i=gid*chunk; i&lt;(gid*chunk)+chunk; i++) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> x = step * ((<span class=\"keyword\">float</span>) i - <span class=\"number\">0.5</span>);</span><br><span class=\"line\">    partial_sum += <span class=\"number\">4.0</span> / (<span class=\"number\">1.0</span> + x * x);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Each work-item stores its partial sum in the workgroup array</span></span><br><span class=\"line\">insideWorkGroup[lid] = partial_sum;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Synchronize all threads within the workgroup</span></span><br><span class=\"line\"><span class=\"built_in\">barrier</span>(CLK_LOCAL_MEM_FENCE);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">float</span> local_pi = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Only work-item 0 of each workgroup perform the reduction</span></span><br><span class=\"line\"><span class=\"comment\">// of that workgroup</span></span><br><span class=\"line\"><span class=\"keyword\">if</span>(lid == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> uint length = lid + <span class=\"built_in\">get_local_size</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (uint i = lid; i&lt;length; i++) &#123;</span><br><span class=\"line\">        local_pi += insideWorkGroup[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">// It store the workgroup sum</span></span><br><span class=\"line\"><span class=\"comment\">// Final reduction, between block, is done out by CPU</span></span><br><span class=\"line\">    workGroupBuffer[<span class=\"built_in\">get_group_id</span>(<span class=\"number\">0</span>)] = local_pi;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">( <span class=\"keyword\">int</span> argc, <span class=\"keyword\">char</span>* argv[] )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> pi;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> *pi_partial;</span><br><span class=\"line\">    <span class=\"keyword\">size_t</span> maxWorkGroupSize;</span><br><span class=\"line\">    cl_int err;</span><br><span class=\"line\">    cl_mem memObjects;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> niter, chunks, workGroups;</span><br><span class=\"line\">    <span class=\"keyword\">size_t</span> globalWorkSize;</span><br><span class=\"line\">    <span class=\"keyword\">size_t</span> localWorkSize;</span><br><span class=\"line\">    cl_event event;</span><br><span class=\"line\"></span><br><span class=\"line\">    cl_platform_id platform;        <span class=\"comment\">// OpenCL platform</span></span><br><span class=\"line\">    cl_device_id device_id;           <span class=\"comment\">// device ID</span></span><br><span class=\"line\">    cl_context context;               <span class=\"comment\">// context</span></span><br><span class=\"line\">    cl_command_queue queue;           <span class=\"comment\">// command queue</span></span><br><span class=\"line\">    cl_program program;               <span class=\"comment\">// program</span></span><br><span class=\"line\">    cl_kernel kernel;                 <span class=\"comment\">// kernel</span></span><br><span class=\"line\"></span><br><span class=\"line\">    niter = <span class=\"number\">262144</span>;</span><br><span class=\"line\">    chunks=<span class=\"number\">64</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetPlatformIDs</span>(<span class=\"number\">1</span>, &amp;platform, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Get ID for the device</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clGetDeviceIDs</span>(platform, CL_DEVICE_TYPE_GPU, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"built_in\">clGetDeviceInfo</span>(device_id, CL_DEVICE_MAX_WORK_GROUP_SIZE, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">size_t</span>),</span><br><span class=\"line\">                    &amp;maxWorkGroupSize, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    workGroups = <span class=\"built_in\">ceil</span>((<span class=\"keyword\">float</span>)(niter/maxWorkGroupSize/chunks));</span><br><span class=\"line\"></span><br><span class=\"line\">    pi_partial = (<span class=\"keyword\">float</span>*)<span class=\"built_in\">malloc</span>(<span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>)*workGroups);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Create a context</span></span><br><span class=\"line\">    context = <span class=\"built_in\">clCreateContext</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, &amp;device_id, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Create a command queue</span></span><br><span class=\"line\">    queue = <span class=\"built_in\">clCreateCommandQueue</span>(context, device_id, CL_QUEUE_PROFILING_ENABLE, &amp;err);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Create the compute program from the source buffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">    program = <span class=\"built_in\">clCreateProgramWithSource</span>(context, <span class=\"number\">1</span>,</span><br><span class=\"line\">                                        &amp;kernelSource, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\">    <span class=\"comment\">// Build the program executable</span></span><br><span class=\"line\">    err = <span class=\"built_in\">clBuildProgram</span>(program, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    localWorkSize =  maxWorkGroupSize;</span><br><span class=\"line\">    globalWorkSize = niter / chunks;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Create the compute kernel in the program we wish to run</span></span><br><span class=\"line\">    kernel = <span class=\"built_in\">clCreateKernel</span>(program, <span class=\"string\">&quot;Pi&quot;</span>, &amp;err);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Create the input and output arrays in device memory for our calculation</span></span><br><span class=\"line\">    memObjects = <span class=\"built_in\">clCreateBuffer</span>(context, CL_MEM_READ_WRITE,</span><br><span class=\"line\">                                <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>)*workGroups, <span class=\"literal\">NULL</span>, &amp;err);</span><br><span class=\"line\"></span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">0</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(cl_mem), &amp;memObjects);</span><br><span class=\"line\">    err  = <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">1</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>)*maxWorkGroupSize, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">2</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span>), &amp;niter);</span><br><span class=\"line\">    err |= <span class=\"built_in\">clSetKernelArg</span>(kernel, <span class=\"number\">3</span>, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span>), &amp;chunks);</span><br><span class=\"line\"></span><br><span class=\"line\">    err = <span class=\"built_in\">clEnqueueNDRangeKernel</span>(queue, kernel, <span class=\"number\">1</span>, <span class=\"literal\">NULL</span>, &amp;globalWorkSize, &amp;localWorkSize,</span><br><span class=\"line\"><span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, &amp;event);</span><br><span class=\"line\">    <span class=\"built_in\">clFinish</span>(queue);</span><br><span class=\"line\">    cl_ulong time_start;</span><br><span class=\"line\">    cl_ulong time_end;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_START, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_start), &amp;time_start, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"built_in\">clGetEventProfilingInfo</span>(event, CL_PROFILING_COMMAND_END, <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(time_end), &amp;time_end, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">double</span> times = time_end - time_start;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;OpenCL Kernel Execution time is: %0.4f\\n&quot;</span>, times / <span class=\"number\">1000000.0</span>);</span><br><span class=\"line\">    err = <span class=\"built_in\">clEnqueueReadBuffer</span>(queue, memObjects, CL_TRUE, <span class=\"number\">0</span>,</span><br><span class=\"line\">                              <span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>)*workGroups, pi_partial, <span class=\"number\">0</span>, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    pi=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">0</span>; i&lt;workGroups; i++) &#123;</span><br><span class=\"line\">        pi += pi_partial[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    pi *= (<span class=\"number\">1.0</span>/(<span class=\"keyword\">float</span>)niter);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;final result: %f\\n&quot;</span>, pi);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// release OpenCL resources</span></span><br><span class=\"line\">    <span class=\"built_in\">clReleaseMemObject</span>(memObjects);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseProgram</span>(program);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseKernel</span>(kernel);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseCommandQueue</span>(queue);</span><br><span class=\"line\">    <span class=\"built_in\">clReleaseContext</span>(context);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//release host memory</span></span><br><span class=\"line\">    <span class=\"built_in\">free</span>(pi_partial);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里进行部分代码的分析，首先阐述chunk的概念，这里用如下的图进行表示。</p>\n<center>\n<img src=\"https://raw.githubusercontent.com/hhy-huang/Image/main/WechatIMG1329.png\" height=\"260\" width=\"450\">\n</center>\n\n<p>所以这里的chunk指的是多个work group组成的组合。因此work group的数量是由所规定的chunk大小以及数据规模决定的。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">workGroups = <span class=\"built_in\">ceil</span>((<span class=\"keyword\">float</span>)(niter/maxWorkGroupSize/chunks));</span><br><span class=\"line\"></span><br><span class=\"line\">pi_partial = (<span class=\"keyword\">float</span>*)<span class=\"built_in\">malloc</span>(<span class=\"built_in\"><span class=\"keyword\">sizeof</span></span>(<span class=\"keyword\">float</span>)*workGroups);</span><br></pre></td></tr></table></figure>\n<p>再向下理解，这里的local work size还是每个work group内的work item数量，这里取的最大，也就是本机器的256。但是这里的global work size与global size不同，他是将一个chunk的所有work group作为一个global work group，那么global work size就是chunks的数目。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">localWorkSize =  maxWorkGroupSize;</span><br><span class=\"line\">globalWorkSize = niter / chunks;</span><br></pre></td></tr></table></figure>\n<p>这样就可以去理解kernel函数的内容了，首先它是将数据niter分成了chunk份，这就是为什么niter必须是chunk的整数倍，然后每个chunk内work item的数目就是要并行计算的数目，也就是每个chunk内做并行。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(uint i=gid*chunk; i&lt;(gid*chunk)+chunk; i++) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> x = step * ((<span class=\"keyword\">float</span>) i - <span class=\"number\">0.5</span>);</span><br><span class=\"line\">    partial_sum += <span class=\"number\">4.0</span> / (<span class=\"number\">1.0</span> + x * x);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后对每个chunk的计算结果进行保存，并在lid为0的work item中进行结果的归约，然后将归约的结果作为这个work group（其实是这个chunk）的运算结果，然后这些chunk的结果要在kernel函数外，也就是CPU当中进行归约，从而计算出最终的结果。对应代码如下。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Each work-item stores its partial sum in the workgroup array</span></span><br><span class=\"line\">insideWorkGroup[lid] = partial_sum;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Synchronize all threads within the workgroup</span></span><br><span class=\"line\"><span class=\"built_in\">barrier</span>(CLK_LOCAL_MEM_FENCE);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">float</span> local_pi = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Only work-item 0 of each workgroup perform the reduction</span></span><br><span class=\"line\"><span class=\"comment\">// of that workgroup</span></span><br><span class=\"line\"><span class=\"keyword\">if</span>(lid == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> uint length = lid + <span class=\"built_in\">get_local_size</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (uint i = lid; i&lt;length; i++) &#123;</span><br><span class=\"line\">        local_pi += insideWorkGroup[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">// It store the workgroup sum</span></span><br><span class=\"line\"><span class=\"comment\">// Final reduction, between block, is done out by CPU</span></span><br><span class=\"line\">    workGroupBuffer[<span class=\"built_in\">get_group_id</span>(<span class=\"number\">0</span>)] = local_pi;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这样就完成了PI计算的并行过程。</p>\n<p><em><strong>实验</strong></em></p>\n<p>由于这里的local size规定死了为最大的256，所以不宜做改变，所以这里我们更改chunks的数量。也就是修改在一个for内并行计算的部分，分别计算chunk数为1、2、4、8、16、32、64的时间消耗。这里的数据规模取26214400。<br>输出结果如下。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>1</th>\n<th>2</th>\n<th>4</th>\n<th>8</th>\n<th>16</th>\n<th>32</th>\n<th>64</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>t/ms</td>\n<td>8.3137</td>\n<td>5.2593</td>\n<td>2.5912</td>\n<td>1.6443</td>\n<td>1.2119</td>\n<td>1.0183</td>\n<td>0.9311</td>\n</tr>\n</tbody></table>\n<p>可以看到，随着chunk数目的增加，计算的效率越来越高，计算的时间消耗越来越少，并且计算时间的优化效果越来越弱。这说明在同一个chunk内的并行数越多，计算效率越高。</p>\n<p>下面选择在chunk为64的条件下进行计算效率随数据规模的变化情况，这里的数据规模我们选择131072、262144、2621440、26214400、262144000。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>131072</th>\n<th>262144</th>\n<th>2621440</th>\n<th>26214400</th>\n<th>262144000</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>t/ms</td>\n<td>0.0324</td>\n<td>0.0342</td>\n<td>0.1155</td>\n<td>0.9268</td>\n<td>10.7321</td>\n</tr>\n</tbody></table>\n<p>可以看到，随着数据规模的增加，计算时间一开始增加缓慢，到后面也是以类似10倍的速率增长，原因与向量加法中的阐述相同。再增加数据规模，增加的时间就不是并行后的结果了，而是串行地等待前面的并行计算完，之后的再进行并行计算，所以时间的增量几乎就是数据规模的增量。</p>\n<h2 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h2><p>macOS Big Sur Version 11.1</p>\n"},{"title":"Meta Path Based Random Walk","date":"2022-02-12T16:43:08.000Z","_content":"\n## Meta Path Based Random Walk\n\n课题原因需要复现ARNN模型。即“**An Attentional Recurrent Neural Networkfor Personalized Next Location Recommendation**”这篇论文，早就听说随机游走模型以及PageRank之类的算法，现在算是自己动手复现了，因为其中需要使用随机游走来获得每个POI的neighbors，从而训练attention的权重。\n\n**本文详述该游走模型的复现思路，代码连接会给出，注释充足便不赘述，当然想必也存在不足，如有发现问题，还望及时提出以便修改。**\n\n首先描述一下随机游走（random walk）模型，给定一个含有n个节点都有向图，在有向图上定义随机游走，也就是一阶马尔可夫链，节点可用来表示状态，有向边表示状态之间的转移。\n\n这里有一个假设，从一个节点通过有向边相连的所有节点的转移概率相等，当节点的type相同时，可以将转移关系描述为n阶的转移矩阵$M$。\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?M&space;=&space;[m_{ij}]_{n\\times&space;n}\" title=\"M = [m_{ij}]_{n\\times n}\" />\n</center>\n\n它描述的是列下标对应的节点转移至行下标对应的节点的概率，换句话说，$m_{ij}$是j节点指向i节点的概率。那么它的性质也有了。第一个共识很显然，第二个就是如果存在由j指向i的有向边，那么该位置的值位j节点的出度分之一，理由就是他们是等概率的。\n\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?\\sum_{i&space;=&space;1}^{n}m_{ij}&space;=&space;1\" title=\"\\sum_{i = 1}^{n}m_{ij} = 1\" />\n</center>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?if~~j->i~:~m_{ij}&space;=&space;\\frac{1}{d^{&plus;}_{j}}\" title=\"if~~j->i~:~m_{ij} = \\frac{1}{d^{+}_{j}}\" />\n</center>\n\n这个矩阵$M$也叫做随机矩阵（stochastic matrix）。\n\n可以理解，$M$是用来进行表征节点的转移偏好的，但游走过程不仅由转移偏好决定，同时也受转移的起点决定。所以提出一个n维向量$V_t$来表征t时刻转移前，本次转移过程的初始节点的概率分布。\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?V_t&space;\\epsilon&space;\\mathbb{R}^{n&space;\\times&space;1}\" title=\"V_t \\epsilon \\mathbb{R}^{n \\times 1}\" />\n</center>\n\n那么：\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?V_{t&plus;1}&space;=&space;M&space;V_{t}\" title=\"V_{t+1} = M V_{t}\" />\n</center>\n\n这里得到的t+1时间步的V就是时间步t这次转移活动在$V_t$初始分布前提下的转移结果分布，因此，可以以这种方式进行迭代，从而进行多次游走。\n\n那么基于**元路径**的随机游走，大体与之相同，但是也有区别。既然要引入meta path的概念，那么图中节点的种类就不是唯一的，在ARNN要解决的任务中，图中存在三类节点，L:地点，U：访问者，V：地点种类，他们构成图。而基于“LL”、“LVL”、“LUL”这三类元路径的路径都要分别以他们为路径元素type的最小重复单元。\n\n也就是每次转移的随机矩阵是需要不同的，“LL”就与上面讲的一样，而对“LVL”而言，需要两个随机矩阵，分别是(num_v, n)与(n, num_v)，前者与地点分布向量相乘（单个path起点loc为1，其余均为0），从而得到type为v的节点概率分布向量，后者再与地点种类分布向量相乘，又得到地点分布向量，然后继续如此迭代。\n\n“LUL”也是如此。\n\n需要注意的是，这里我虽然将三类节点统一编码，并用三元组构成图谱，但并没有将所有类型的节点放在同一个tensor里，而是meta path在当前需要什么类型，我就单独把起点与终点的类型的节点构成tensor来进行计算，拓扑上讲就是讲图拆分，但是概率依赖关系不受影响。因为我认为所有实体的个数作为tensor的大小用来计算，效率会很低，不如拆分成多组tensor，直观且高效。\n\n思路就是这样，代码实现方面，一开始按照<a href=\"https://github.com/xinbowu2\">@Xinbo Wu</a>复现Personalised Page Rank的方法编写，使用dict来进行矩阵运算，结果显然是差强人意的，面对foursquare的数据运算效率就已经无法接受了，因此使用tensor放到GPU上进行运算，结果明显快了很多，效率勉强让人接受，其实也许可以通过解决矩阵稀疏的问题再加快游走效率，作者目前能力有限，没找到能更加提高效率的办法，希望大家给予思路。\n\n代码链接如下：\n[https://github.com/hhy-huang/Meta-Path-Based-Random-Walk](https://github.com/hhy-huang/Meta-Path-Based-Random-Walk)\n\n【参考《统计学习方法》李航】","source":"_posts/MPbasedRandomWalk-md.md","raw":"---\ntitle: Meta Path Based Random Walk\ndate: 2022-02-13 00:43:08\ntags: NLP的一些收获\n---\n\n## Meta Path Based Random Walk\n\n课题原因需要复现ARNN模型。即“**An Attentional Recurrent Neural Networkfor Personalized Next Location Recommendation**”这篇论文，早就听说随机游走模型以及PageRank之类的算法，现在算是自己动手复现了，因为其中需要使用随机游走来获得每个POI的neighbors，从而训练attention的权重。\n\n**本文详述该游走模型的复现思路，代码连接会给出，注释充足便不赘述，当然想必也存在不足，如有发现问题，还望及时提出以便修改。**\n\n首先描述一下随机游走（random walk）模型，给定一个含有n个节点都有向图，在有向图上定义随机游走，也就是一阶马尔可夫链，节点可用来表示状态，有向边表示状态之间的转移。\n\n这里有一个假设，从一个节点通过有向边相连的所有节点的转移概率相等，当节点的type相同时，可以将转移关系描述为n阶的转移矩阵$M$。\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?M&space;=&space;[m_{ij}]_{n\\times&space;n}\" title=\"M = [m_{ij}]_{n\\times n}\" />\n</center>\n\n它描述的是列下标对应的节点转移至行下标对应的节点的概率，换句话说，$m_{ij}$是j节点指向i节点的概率。那么它的性质也有了。第一个共识很显然，第二个就是如果存在由j指向i的有向边，那么该位置的值位j节点的出度分之一，理由就是他们是等概率的。\n\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?\\sum_{i&space;=&space;1}^{n}m_{ij}&space;=&space;1\" title=\"\\sum_{i = 1}^{n}m_{ij} = 1\" />\n</center>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?if~~j->i~:~m_{ij}&space;=&space;\\frac{1}{d^{&plus;}_{j}}\" title=\"if~~j->i~:~m_{ij} = \\frac{1}{d^{+}_{j}}\" />\n</center>\n\n这个矩阵$M$也叫做随机矩阵（stochastic matrix）。\n\n可以理解，$M$是用来进行表征节点的转移偏好的，但游走过程不仅由转移偏好决定，同时也受转移的起点决定。所以提出一个n维向量$V_t$来表征t时刻转移前，本次转移过程的初始节点的概率分布。\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?V_t&space;\\epsilon&space;\\mathbb{R}^{n&space;\\times&space;1}\" title=\"V_t \\epsilon \\mathbb{R}^{n \\times 1}\" />\n</center>\n\n那么：\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?V_{t&plus;1}&space;=&space;M&space;V_{t}\" title=\"V_{t+1} = M V_{t}\" />\n</center>\n\n这里得到的t+1时间步的V就是时间步t这次转移活动在$V_t$初始分布前提下的转移结果分布，因此，可以以这种方式进行迭代，从而进行多次游走。\n\n那么基于**元路径**的随机游走，大体与之相同，但是也有区别。既然要引入meta path的概念，那么图中节点的种类就不是唯一的，在ARNN要解决的任务中，图中存在三类节点，L:地点，U：访问者，V：地点种类，他们构成图。而基于“LL”、“LVL”、“LUL”这三类元路径的路径都要分别以他们为路径元素type的最小重复单元。\n\n也就是每次转移的随机矩阵是需要不同的，“LL”就与上面讲的一样，而对“LVL”而言，需要两个随机矩阵，分别是(num_v, n)与(n, num_v)，前者与地点分布向量相乘（单个path起点loc为1，其余均为0），从而得到type为v的节点概率分布向量，后者再与地点种类分布向量相乘，又得到地点分布向量，然后继续如此迭代。\n\n“LUL”也是如此。\n\n需要注意的是，这里我虽然将三类节点统一编码，并用三元组构成图谱，但并没有将所有类型的节点放在同一个tensor里，而是meta path在当前需要什么类型，我就单独把起点与终点的类型的节点构成tensor来进行计算，拓扑上讲就是讲图拆分，但是概率依赖关系不受影响。因为我认为所有实体的个数作为tensor的大小用来计算，效率会很低，不如拆分成多组tensor，直观且高效。\n\n思路就是这样，代码实现方面，一开始按照<a href=\"https://github.com/xinbowu2\">@Xinbo Wu</a>复现Personalised Page Rank的方法编写，使用dict来进行矩阵运算，结果显然是差强人意的，面对foursquare的数据运算效率就已经无法接受了，因此使用tensor放到GPU上进行运算，结果明显快了很多，效率勉强让人接受，其实也许可以通过解决矩阵稀疏的问题再加快游走效率，作者目前能力有限，没找到能更加提高效率的办法，希望大家给予思路。\n\n代码链接如下：\n[https://github.com/hhy-huang/Meta-Path-Based-Random-Walk](https://github.com/hhy-huang/Meta-Path-Based-Random-Walk)\n\n【参考《统计学习方法》李航】","slug":"MPbasedRandomWalk-md","published":1,"updated":"2022-02-12T18:42:08.817Z","_id":"ckzk2pxly0001i3oe27tmerkw","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Meta-Path-Based-Random-Walk\"><a href=\"#Meta-Path-Based-Random-Walk\" class=\"headerlink\" title=\"Meta Path Based Random Walk\"></a>Meta Path Based Random Walk</h2><p>课题原因需要复现ARNN模型。即“<strong>An Attentional Recurrent Neural Networkfor Personalized Next Location Recommendation</strong>”这篇论文，早就听说随机游走模型以及PageRank之类的算法，现在算是自己动手复现了，因为其中需要使用随机游走来获得每个POI的neighbors，从而训练attention的权重。</p>\n<p><strong>本文详述该游走模型的复现思路，代码连接会给出，注释充足便不赘述，当然想必也存在不足，如有发现问题，还望及时提出以便修改。</strong></p>\n<p>首先描述一下随机游走（random walk）模型，给定一个含有n个节点都有向图，在有向图上定义随机游走，也就是一阶马尔可夫链，节点可用来表示状态，有向边表示状态之间的转移。</p>\n<p>这里有一个假设，从一个节点通过有向边相连的所有节点的转移概率相等，当节点的type相同时，可以将转移关系描述为n阶的转移矩阵$M$。</p>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?M&space;=&space;[m_{ij}]_{n\\times&space;n}\" title=\"M = [m_{ij}]_{n\\times n}\" />\n</center>\n\n<p>它描述的是列下标对应的节点转移至行下标对应的节点的概率，换句话说，$m_{ij}$是j节点指向i节点的概率。那么它的性质也有了。第一个共识很显然，第二个就是如果存在由j指向i的有向边，那么该位置的值位j节点的出度分之一，理由就是他们是等概率的。</p>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?\\sum_{i&space;=&space;1}^{n}m_{ij}&space;=&space;1\" title=\"\\sum_{i = 1}^{n}m_{ij} = 1\" />\n</center>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?if~~j->i~:~m_{ij}&space;=&space;\\frac{1}{d^{&plus;}_{j}}\" title=\"if~~j->i~:~m_{ij} = \\frac{1}{d^{+}_{j}}\" />\n</center>\n\n<p>这个矩阵$M$也叫做随机矩阵（stochastic matrix）。</p>\n<p>可以理解，$M$是用来进行表征节点的转移偏好的，但游走过程不仅由转移偏好决定，同时也受转移的起点决定。所以提出一个n维向量$V_t$来表征t时刻转移前，本次转移过程的初始节点的概率分布。</p>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?V_t&space;\\epsilon&space;\\mathbb{R}^{n&space;\\times&space;1}\" title=\"V_t \\epsilon \\mathbb{R}^{n \\times 1}\" />\n</center>\n\n<p>那么：</p>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?V_{t&plus;1}&space;=&space;M&space;V_{t}\" title=\"V_{t+1} = M V_{t}\" />\n</center>\n\n<p>这里得到的t+1时间步的V就是时间步t这次转移活动在$V_t$初始分布前提下的转移结果分布，因此，可以以这种方式进行迭代，从而进行多次游走。</p>\n<p>那么基于<strong>元路径</strong>的随机游走，大体与之相同，但是也有区别。既然要引入meta path的概念，那么图中节点的种类就不是唯一的，在ARNN要解决的任务中，图中存在三类节点，L:地点，U：访问者，V：地点种类，他们构成图。而基于“LL”、“LVL”、“LUL”这三类元路径的路径都要分别以他们为路径元素type的最小重复单元。</p>\n<p>也就是每次转移的随机矩阵是需要不同的，“LL”就与上面讲的一样，而对“LVL”而言，需要两个随机矩阵，分别是(num_v, n)与(n, num_v)，前者与地点分布向量相乘（单个path起点loc为1，其余均为0），从而得到type为v的节点概率分布向量，后者再与地点种类分布向量相乘，又得到地点分布向量，然后继续如此迭代。</p>\n<p>“LUL”也是如此。</p>\n<p>需要注意的是，这里我虽然将三类节点统一编码，并用三元组构成图谱，但并没有将所有类型的节点放在同一个tensor里，而是meta path在当前需要什么类型，我就单独把起点与终点的类型的节点构成tensor来进行计算，拓扑上讲就是讲图拆分，但是概率依赖关系不受影响。因为我认为所有实体的个数作为tensor的大小用来计算，效率会很低，不如拆分成多组tensor，直观且高效。</p>\n<p>思路就是这样，代码实现方面，一开始按照<a href=\"https://github.com/xinbowu2\">@Xinbo Wu</a>复现Personalised Page Rank的方法编写，使用dict来进行矩阵运算，结果显然是差强人意的，面对foursquare的数据运算效率就已经无法接受了，因此使用tensor放到GPU上进行运算，结果明显快了很多，效率勉强让人接受，其实也许可以通过解决矩阵稀疏的问题再加快游走效率，作者目前能力有限，没找到能更加提高效率的办法，希望大家给予思路。</p>\n<p>代码链接如下：<br><a href=\"https://github.com/hhy-huang/Meta-Path-Based-Random-Walk\">https://github.com/hhy-huang/Meta-Path-Based-Random-Walk</a></p>\n<p>【参考《统计学习方法》李航】</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Meta-Path-Based-Random-Walk\"><a href=\"#Meta-Path-Based-Random-Walk\" class=\"headerlink\" title=\"Meta Path Based Random Walk\"></a>Meta Path Based Random Walk</h2><p>课题原因需要复现ARNN模型。即“<strong>An Attentional Recurrent Neural Networkfor Personalized Next Location Recommendation</strong>”这篇论文，早就听说随机游走模型以及PageRank之类的算法，现在算是自己动手复现了，因为其中需要使用随机游走来获得每个POI的neighbors，从而训练attention的权重。</p>\n<p><strong>本文详述该游走模型的复现思路，代码连接会给出，注释充足便不赘述，当然想必也存在不足，如有发现问题，还望及时提出以便修改。</strong></p>\n<p>首先描述一下随机游走（random walk）模型，给定一个含有n个节点都有向图，在有向图上定义随机游走，也就是一阶马尔可夫链，节点可用来表示状态，有向边表示状态之间的转移。</p>\n<p>这里有一个假设，从一个节点通过有向边相连的所有节点的转移概率相等，当节点的type相同时，可以将转移关系描述为n阶的转移矩阵$M$。</p>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?M&space;=&space;[m_{ij}]_{n\\times&space;n}\" title=\"M = [m_{ij}]_{n\\times n}\" />\n</center>\n\n<p>它描述的是列下标对应的节点转移至行下标对应的节点的概率，换句话说，$m_{ij}$是j节点指向i节点的概率。那么它的性质也有了。第一个共识很显然，第二个就是如果存在由j指向i的有向边，那么该位置的值位j节点的出度分之一，理由就是他们是等概率的。</p>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?\\sum_{i&space;=&space;1}^{n}m_{ij}&space;=&space;1\" title=\"\\sum_{i = 1}^{n}m_{ij} = 1\" />\n</center>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?if~~j->i~:~m_{ij}&space;=&space;\\frac{1}{d^{&plus;}_{j}}\" title=\"if~~j->i~:~m_{ij} = \\frac{1}{d^{+}_{j}}\" />\n</center>\n\n<p>这个矩阵$M$也叫做随机矩阵（stochastic matrix）。</p>\n<p>可以理解，$M$是用来进行表征节点的转移偏好的，但游走过程不仅由转移偏好决定，同时也受转移的起点决定。所以提出一个n维向量$V_t$来表征t时刻转移前，本次转移过程的初始节点的概率分布。</p>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?V_t&space;\\epsilon&space;\\mathbb{R}^{n&space;\\times&space;1}\" title=\"V_t \\epsilon \\mathbb{R}^{n \\times 1}\" />\n</center>\n\n<p>那么：</p>\n<center>\n<img src=\"https://latex.codecogs.com/svg.image?V_{t&plus;1}&space;=&space;M&space;V_{t}\" title=\"V_{t+1} = M V_{t}\" />\n</center>\n\n<p>这里得到的t+1时间步的V就是时间步t这次转移活动在$V_t$初始分布前提下的转移结果分布，因此，可以以这种方式进行迭代，从而进行多次游走。</p>\n<p>那么基于<strong>元路径</strong>的随机游走，大体与之相同，但是也有区别。既然要引入meta path的概念，那么图中节点的种类就不是唯一的，在ARNN要解决的任务中，图中存在三类节点，L:地点，U：访问者，V：地点种类，他们构成图。而基于“LL”、“LVL”、“LUL”这三类元路径的路径都要分别以他们为路径元素type的最小重复单元。</p>\n<p>也就是每次转移的随机矩阵是需要不同的，“LL”就与上面讲的一样，而对“LVL”而言，需要两个随机矩阵，分别是(num_v, n)与(n, num_v)，前者与地点分布向量相乘（单个path起点loc为1，其余均为0），从而得到type为v的节点概率分布向量，后者再与地点种类分布向量相乘，又得到地点分布向量，然后继续如此迭代。</p>\n<p>“LUL”也是如此。</p>\n<p>需要注意的是，这里我虽然将三类节点统一编码，并用三元组构成图谱，但并没有将所有类型的节点放在同一个tensor里，而是meta path在当前需要什么类型，我就单独把起点与终点的类型的节点构成tensor来进行计算，拓扑上讲就是讲图拆分，但是概率依赖关系不受影响。因为我认为所有实体的个数作为tensor的大小用来计算，效率会很低，不如拆分成多组tensor，直观且高效。</p>\n<p>思路就是这样，代码实现方面，一开始按照<a href=\"https://github.com/xinbowu2\">@Xinbo Wu</a>复现Personalised Page Rank的方法编写，使用dict来进行矩阵运算，结果显然是差强人意的，面对foursquare的数据运算效率就已经无法接受了，因此使用tensor放到GPU上进行运算，结果明显快了很多，效率勉强让人接受，其实也许可以通过解决矩阵稀疏的问题再加快游走效率，作者目前能力有限，没找到能更加提高效率的办法，希望大家给予思路。</p>\n<p>代码链接如下：<br><a href=\"https://github.com/hhy-huang/Meta-Path-Based-Random-Walk\">https://github.com/hhy-huang/Meta-Path-Based-Random-Walk</a></p>\n<p>【参考《统计学习方法》李航】</p>\n"},{"title":"linux_command.md","date":"2022-03-06T14:44:04.000Z","_content":"","source":"_posts/linux-command-md.md","raw":"---\ntitle: linux_command.md\ndate: 2022-03-06 22:44:04\ntags:\n---\n","slug":"linux-command-md","published":1,"updated":"2024-09-14T17:33:45.217Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cm12ffx9r0000kvoe39k3b1xp","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"ARNN复现反思","date":"2022-04-26T14:37:56.000Z","_content":"\n因为找遍了一二三四作，都没有能得到An Attentional Recurrent Neural Networkfor Personalized Next Location Recommendation这篇论文的代码，一作没反应，二三四都让我找一作...麻了，所以硬下头皮准备复现。\n\n其实任务量还好，最幸运的是这篇论文的模型架构与另外一篇DeepMove的模型十分相似，都是先embedding序列后，对序列元素进行attention的思路，不过也是有很多不同的。\n\n## 思路\n这篇论文的思路很清晰，先将check in序列处理好，得到用户的历史轨迹，这个历史轨迹包括loc、time和word的序列，分别把它们用相应的维度embedding后，在第三个维度拼接起来得到tensor：x(batch_size, num, dim)，这样这个轨迹序列的元素就融合了地点、时间、语义上的含义。\n\n之后对于历史轨迹的每一个loc，与该地点的所有neighbors求相似度，然后加权进行一次attention，得到targer，也就是与之最相似的loc向量，结果为ck。\n\n然后将x和ck同样在第三个维度拼接起来得到一个新的tensor，让每一个位置的元素融合入与其它loc的转移关系，然后将它pack后输入LSTM，取出最后一个hidden state，融合入user的embedding，最后用softmax得到next poi的概率分布。\n\n其中的loc的neighbors得到的方法是使用基于meta path的随机游走模型得到的，将历史轨迹序列构成图，我这里的操作其实和pageRank的处理方法类似，搞一个邻接表，然后严格按照原路径的类型进行游走，将访问到的loc纳入path，也就是起点loc的neighbor。具体做法详见上篇blog。\n\n## 遇到的问题\n（1）在随机游走时，太慢了，虽然现在也不算快，但是一开始参照一位githuber的pageRank代码改造，是用dict代替多维list，连每一步的带权乘法都要自己用for循环写，很慢。后来想着可以把它搞成矩阵，然后转化为tensor，既能调用torh里的乘法，还可以放到GPU上运算，所以就这么做了，真的有很大的改进，但确实也不算快。\n\n（2）不会写attention，第一次复现嘛，一开始很傻，从tensor中一条一条数据遍历，然后找到对应的loc_id，再通过loc_id找到对应的neighbors的序列，然后再对neighbors embedding...太繁琐了，导致一个batch就要两三分钟。\n\n所以，我思考了一下，可不可以把neighbors的embedding也做成一个tensor，然后让二者去运算，这样是可以调用torch.matmul，方便加速的。但是问题在于不同的loc邻居的数量也不同，所以我采用的办法是取游走获得的path中出现次数最多的前n个邻居作为loc的neighbors，这样维度就统一了。\n\n假设batch_size为128，n是10，dim是100，那么一开始的loc_neighbor_emb就是(128, 464, 100, 10)，原本的loc_emb是(128, 464, 100)，为了方便相乘，unsqueeze一下为(128, 464, 1, 100)，这样二者的batch就统一了，为128*464，因为torch.matmul规定四维tensor的运算前两维为batch，前者转置一下，相乘后softmax就是相似度的矩阵了，大小为(128, 464, 1, 10)。这个大小一看就很对，对于每个loc都有10个neighbors对应的weight。最后再将其和neighbors的embedding相乘，得到最终的结果。\n\n然后用这样的方法再尝试，果然快了很多，一个batch就20s左右。时间估计都用在attention前loc_neighbor_emb的构建上了。\n\n（3）每个epoch的最后一个batch不足batch_size，一开始我还想着continue过去，但是想想会影响到测试集到验证的，所以就查了查，发现有解决办法，在DataLoader中设置参数drop_last = True，其实也是drop掉了，不过在一开始就去掉了，不会产生影响。\n\n## 总结\n总结一下，最困难的部分也就是核心模块的编写和随机游走的编写了，对于框架代码的编写其实没有涉足过多，毕竟是拿别人的代码改动的，下次有机会还是尝试一下自己从0开始，体会应该更深刻一些吧。另外就是一些torch的函数，了解的还是太少，还是应该多分析分析别人的代码。\n\n## 最后\n代码：[ARNN-master](https://github.com/hhy-huang/ARNN-master)\n\n我学识鄙陋，有问题一定要告诉我！\n","source":"_posts/ARNN-md.md","raw":"---\ntitle: ARNN复现反思\ndate: 2022-04-26 22:37:56\ntags: NLP的一些收获\n---\n\n因为找遍了一二三四作，都没有能得到An Attentional Recurrent Neural Networkfor Personalized Next Location Recommendation这篇论文的代码，一作没反应，二三四都让我找一作...麻了，所以硬下头皮准备复现。\n\n其实任务量还好，最幸运的是这篇论文的模型架构与另外一篇DeepMove的模型十分相似，都是先embedding序列后，对序列元素进行attention的思路，不过也是有很多不同的。\n\n## 思路\n这篇论文的思路很清晰，先将check in序列处理好，得到用户的历史轨迹，这个历史轨迹包括loc、time和word的序列，分别把它们用相应的维度embedding后，在第三个维度拼接起来得到tensor：x(batch_size, num, dim)，这样这个轨迹序列的元素就融合了地点、时间、语义上的含义。\n\n之后对于历史轨迹的每一个loc，与该地点的所有neighbors求相似度，然后加权进行一次attention，得到targer，也就是与之最相似的loc向量，结果为ck。\n\n然后将x和ck同样在第三个维度拼接起来得到一个新的tensor，让每一个位置的元素融合入与其它loc的转移关系，然后将它pack后输入LSTM，取出最后一个hidden state，融合入user的embedding，最后用softmax得到next poi的概率分布。\n\n其中的loc的neighbors得到的方法是使用基于meta path的随机游走模型得到的，将历史轨迹序列构成图，我这里的操作其实和pageRank的处理方法类似，搞一个邻接表，然后严格按照原路径的类型进行游走，将访问到的loc纳入path，也就是起点loc的neighbor。具体做法详见上篇blog。\n\n## 遇到的问题\n（1）在随机游走时，太慢了，虽然现在也不算快，但是一开始参照一位githuber的pageRank代码改造，是用dict代替多维list，连每一步的带权乘法都要自己用for循环写，很慢。后来想着可以把它搞成矩阵，然后转化为tensor，既能调用torh里的乘法，还可以放到GPU上运算，所以就这么做了，真的有很大的改进，但确实也不算快。\n\n（2）不会写attention，第一次复现嘛，一开始很傻，从tensor中一条一条数据遍历，然后找到对应的loc_id，再通过loc_id找到对应的neighbors的序列，然后再对neighbors embedding...太繁琐了，导致一个batch就要两三分钟。\n\n所以，我思考了一下，可不可以把neighbors的embedding也做成一个tensor，然后让二者去运算，这样是可以调用torch.matmul，方便加速的。但是问题在于不同的loc邻居的数量也不同，所以我采用的办法是取游走获得的path中出现次数最多的前n个邻居作为loc的neighbors，这样维度就统一了。\n\n假设batch_size为128，n是10，dim是100，那么一开始的loc_neighbor_emb就是(128, 464, 100, 10)，原本的loc_emb是(128, 464, 100)，为了方便相乘，unsqueeze一下为(128, 464, 1, 100)，这样二者的batch就统一了，为128*464，因为torch.matmul规定四维tensor的运算前两维为batch，前者转置一下，相乘后softmax就是相似度的矩阵了，大小为(128, 464, 1, 10)。这个大小一看就很对，对于每个loc都有10个neighbors对应的weight。最后再将其和neighbors的embedding相乘，得到最终的结果。\n\n然后用这样的方法再尝试，果然快了很多，一个batch就20s左右。时间估计都用在attention前loc_neighbor_emb的构建上了。\n\n（3）每个epoch的最后一个batch不足batch_size，一开始我还想着continue过去，但是想想会影响到测试集到验证的，所以就查了查，发现有解决办法，在DataLoader中设置参数drop_last = True，其实也是drop掉了，不过在一开始就去掉了，不会产生影响。\n\n## 总结\n总结一下，最困难的部分也就是核心模块的编写和随机游走的编写了，对于框架代码的编写其实没有涉足过多，毕竟是拿别人的代码改动的，下次有机会还是尝试一下自己从0开始，体会应该更深刻一些吧。另外就是一些torch的函数，了解的还是太少，还是应该多分析分析别人的代码。\n\n## 最后\n代码：[ARNN-master](https://github.com/hhy-huang/ARNN-master)\n\n我学识鄙陋，有问题一定要告诉我！\n","slug":"ARNN-md","published":1,"updated":"2024-09-14T17:33:45.187Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cm12ffx9u0001kvoefk9dd7y0","content":"<p>因为找遍了一二三四作，都没有能得到An Attentional Recurrent Neural Networkfor Personalized Next Location Recommendation这篇论文的代码，一作没反应，二三四都让我找一作…麻了，所以硬下头皮准备复现。</p>\n<p>其实任务量还好，最幸运的是这篇论文的模型架构与另外一篇DeepMove的模型十分相似，都是先embedding序列后，对序列元素进行attention的思路，不过也是有很多不同的。</p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>这篇论文的思路很清晰，先将check in序列处理好，得到用户的历史轨迹，这个历史轨迹包括loc、time和word的序列，分别把它们用相应的维度embedding后，在第三个维度拼接起来得到tensor：x(batch_size, num, dim)，这样这个轨迹序列的元素就融合了地点、时间、语义上的含义。</p>\n<p>之后对于历史轨迹的每一个loc，与该地点的所有neighbors求相似度，然后加权进行一次attention，得到targer，也就是与之最相似的loc向量，结果为ck。</p>\n<p>然后将x和ck同样在第三个维度拼接起来得到一个新的tensor，让每一个位置的元素融合入与其它loc的转移关系，然后将它pack后输入LSTM，取出最后一个hidden state，融合入user的embedding，最后用softmax得到next poi的概率分布。</p>\n<p>其中的loc的neighbors得到的方法是使用基于meta path的随机游走模型得到的，将历史轨迹序列构成图，我这里的操作其实和pageRank的处理方法类似，搞一个邻接表，然后严格按照原路径的类型进行游走，将访问到的loc纳入path，也就是起点loc的neighbor。具体做法详见上篇blog。</p>\n<h2 id=\"遇到的问题\"><a href=\"#遇到的问题\" class=\"headerlink\" title=\"遇到的问题\"></a>遇到的问题</h2><p>（1）在随机游走时，太慢了，虽然现在也不算快，但是一开始参照一位githuber的pageRank代码改造，是用dict代替多维list，连每一步的带权乘法都要自己用for循环写，很慢。后来想着可以把它搞成矩阵，然后转化为tensor，既能调用torh里的乘法，还可以放到GPU上运算，所以就这么做了，真的有很大的改进，但确实也不算快。</p>\n<p>（2）不会写attention，第一次复现嘛，一开始很傻，从tensor中一条一条数据遍历，然后找到对应的loc_id，再通过loc_id找到对应的neighbors的序列，然后再对neighbors embedding…太繁琐了，导致一个batch就要两三分钟。</p>\n<p>所以，我思考了一下，可不可以把neighbors的embedding也做成一个tensor，然后让二者去运算，这样是可以调用torch.matmul，方便加速的。但是问题在于不同的loc邻居的数量也不同，所以我采用的办法是取游走获得的path中出现次数最多的前n个邻居作为loc的neighbors，这样维度就统一了。</p>\n<p>假设batch_size为128，n是10，dim是100，那么一开始的loc_neighbor_emb就是(128, 464, 100, 10)，原本的loc_emb是(128, 464, 100)，为了方便相乘，unsqueeze一下为(128, 464, 1, 100)，这样二者的batch就统一了，为128*464，因为torch.matmul规定四维tensor的运算前两维为batch，前者转置一下，相乘后softmax就是相似度的矩阵了，大小为(128, 464, 1, 10)。这个大小一看就很对，对于每个loc都有10个neighbors对应的weight。最后再将其和neighbors的embedding相乘，得到最终的结果。</p>\n<p>然后用这样的方法再尝试，果然快了很多，一个batch就20s左右。时间估计都用在attention前loc_neighbor_emb的构建上了。</p>\n<p>（3）每个epoch的最后一个batch不足batch_size，一开始我还想着continue过去，但是想想会影响到测试集到验证的，所以就查了查，发现有解决办法，在DataLoader中设置参数drop_last = True，其实也是drop掉了，不过在一开始就去掉了，不会产生影响。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>总结一下，最困难的部分也就是核心模块的编写和随机游走的编写了，对于框架代码的编写其实没有涉足过多，毕竟是拿别人的代码改动的，下次有机会还是尝试一下自己从0开始，体会应该更深刻一些吧。另外就是一些torch的函数，了解的还是太少，还是应该多分析分析别人的代码。</p>\n<h2 id=\"最后\"><a href=\"#最后\" class=\"headerlink\" title=\"最后\"></a>最后</h2><p>代码：<a href=\"https://github.com/hhy-huang/ARNN-master\">ARNN-master</a></p>\n<p>我学识鄙陋，有问题一定要告诉我！</p>\n","site":{"data":{}},"excerpt":"","more":"<p>因为找遍了一二三四作，都没有能得到An Attentional Recurrent Neural Networkfor Personalized Next Location Recommendation这篇论文的代码，一作没反应，二三四都让我找一作…麻了，所以硬下头皮准备复现。</p>\n<p>其实任务量还好，最幸运的是这篇论文的模型架构与另外一篇DeepMove的模型十分相似，都是先embedding序列后，对序列元素进行attention的思路，不过也是有很多不同的。</p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>这篇论文的思路很清晰，先将check in序列处理好，得到用户的历史轨迹，这个历史轨迹包括loc、time和word的序列，分别把它们用相应的维度embedding后，在第三个维度拼接起来得到tensor：x(batch_size, num, dim)，这样这个轨迹序列的元素就融合了地点、时间、语义上的含义。</p>\n<p>之后对于历史轨迹的每一个loc，与该地点的所有neighbors求相似度，然后加权进行一次attention，得到targer，也就是与之最相似的loc向量，结果为ck。</p>\n<p>然后将x和ck同样在第三个维度拼接起来得到一个新的tensor，让每一个位置的元素融合入与其它loc的转移关系，然后将它pack后输入LSTM，取出最后一个hidden state，融合入user的embedding，最后用softmax得到next poi的概率分布。</p>\n<p>其中的loc的neighbors得到的方法是使用基于meta path的随机游走模型得到的，将历史轨迹序列构成图，我这里的操作其实和pageRank的处理方法类似，搞一个邻接表，然后严格按照原路径的类型进行游走，将访问到的loc纳入path，也就是起点loc的neighbor。具体做法详见上篇blog。</p>\n<h2 id=\"遇到的问题\"><a href=\"#遇到的问题\" class=\"headerlink\" title=\"遇到的问题\"></a>遇到的问题</h2><p>（1）在随机游走时，太慢了，虽然现在也不算快，但是一开始参照一位githuber的pageRank代码改造，是用dict代替多维list，连每一步的带权乘法都要自己用for循环写，很慢。后来想着可以把它搞成矩阵，然后转化为tensor，既能调用torh里的乘法，还可以放到GPU上运算，所以就这么做了，真的有很大的改进，但确实也不算快。</p>\n<p>（2）不会写attention，第一次复现嘛，一开始很傻，从tensor中一条一条数据遍历，然后找到对应的loc_id，再通过loc_id找到对应的neighbors的序列，然后再对neighbors embedding…太繁琐了，导致一个batch就要两三分钟。</p>\n<p>所以，我思考了一下，可不可以把neighbors的embedding也做成一个tensor，然后让二者去运算，这样是可以调用torch.matmul，方便加速的。但是问题在于不同的loc邻居的数量也不同，所以我采用的办法是取游走获得的path中出现次数最多的前n个邻居作为loc的neighbors，这样维度就统一了。</p>\n<p>假设batch_size为128，n是10，dim是100，那么一开始的loc_neighbor_emb就是(128, 464, 100, 10)，原本的loc_emb是(128, 464, 100)，为了方便相乘，unsqueeze一下为(128, 464, 1, 100)，这样二者的batch就统一了，为128*464，因为torch.matmul规定四维tensor的运算前两维为batch，前者转置一下，相乘后softmax就是相似度的矩阵了，大小为(128, 464, 1, 10)。这个大小一看就很对，对于每个loc都有10个neighbors对应的weight。最后再将其和neighbors的embedding相乘，得到最终的结果。</p>\n<p>然后用这样的方法再尝试，果然快了很多，一个batch就20s左右。时间估计都用在attention前loc_neighbor_emb的构建上了。</p>\n<p>（3）每个epoch的最后一个batch不足batch_size，一开始我还想着continue过去，但是想想会影响到测试集到验证的，所以就查了查，发现有解决办法，在DataLoader中设置参数drop_last = True，其实也是drop掉了，不过在一开始就去掉了，不会产生影响。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>总结一下，最困难的部分也就是核心模块的编写和随机游走的编写了，对于框架代码的编写其实没有涉足过多，毕竟是拿别人的代码改动的，下次有机会还是尝试一下自己从0开始，体会应该更深刻一些吧。另外就是一些torch的函数，了解的还是太少，还是应该多分析分析别人的代码。</p>\n<h2 id=\"最后\"><a href=\"#最后\" class=\"headerlink\" title=\"最后\"></a>最后</h2><p>代码：<a href=\"https://github.com/hhy-huang/ARNN-master\">ARNN-master</a></p>\n<p>我学识鄙陋，有问题一定要告诉我！</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"ckr2agn4l0000zuoehjwa0jva","tag_id":"ckr2bdm8v0000fgoeaffw5tof","_id":"ckr2bdm8x0001fgoe8usi7wm8"},{"post_id":"ckr31fffa0000wjoe66a24j5l","tag_id":"ckr31fffg0001wjoea22m2cn2","_id":"ckr31fffi0002wjoe8ltkgnfc"},{"post_id":"ckr3o95n200000woe8xima8wd","tag_id":"ckr31fffg0001wjoea22m2cn2","_id":"ckr3o95n700010woec2051gk9"},{"post_id":"ckr56actz0000xvoe1jyv1ae3","tag_id":"ckr31fffg0001wjoea22m2cn2","_id":"ckr56acu30001xvoe5dq580hq"},{"post_id":"ckr812mmq0000lkoeedif9tvc","tag_id":"ckr31fffg0001wjoea22m2cn2","_id":"ckr812mmu0001lkoe0h3f2b4f"},{"post_id":"ckravtm2q0000qwoe3vt9d9wm","tag_id":"ckravtm2x0002qwoefoe2ar9g","_id":"ckravtm300003qwoe2o7z4bgl"},{"post_id":"ckrh97fqd0000m8oee4j386k8","tag_id":"ckr2bdm8v0000fgoeaffw5tof","_id":"ckrh9lccl0002u3oec4cd7olv"},{"post_id":"ckrh9lcce0000u3oee4aa0cle","tag_id":"ckrh9lcch0001u3oe8r3z2ocr","_id":"ckrh9lccl0003u3oe3ign2pd1"},{"post_id":"ckrltyy1n000035oe7v4x8722","tag_id":"ckrltyy1s000135oe31ykgs2a","_id":"ckrltyy1w000235oeekh58xiv"},{"post_id":"ckrm02c0v000086oecbzc6w63","tag_id":"ckrltyy1s000135oe31ykgs2a","_id":"ckrm02c12000186oe3dqudt1q"},{"post_id":"cks0dw1520001gfoe3epf7p0s","tag_id":"ckr2bdm8v0000fgoeaffw5tof","_id":"cks0dxl6x0000kjoeducf6hmr"},{"post_id":"cksg44og80000l3oe92nefynw","tag_id":"ckr2bdm8v0000fgoeaffw5tof","_id":"cksg46b2i0000m5oe3auz411m"},{"post_id":"cktmo1qvu00000zoedl411g4n","tag_id":"cktmo1qw300010zoecz0w6whc","_id":"cktmo1qw800020zoe71578yhd"},{"post_id":"cktxte5ae0000d3oee8undkvv","tag_id":"cktmo1qw300010zoecz0w6whc","_id":"cktxte5ai0001d3oe26ln50pw"},{"post_id":"ckvm4wqog00000hoecx59gf45","tag_id":"ckvm4wqom00010hoehswzekf4","_id":"ckvm4wqoq00020hoe2rey3s59"},{"post_id":"ckvo0et3l0000t9oe2alhh4k5","tag_id":"ckr31fffg0001wjoea22m2cn2","_id":"ckvo0et3n0001t9oecgp1hxv8"},{"post_id":"ckvo10kj30000ymoe4gji7uw6","tag_id":"ckr31fffg0001wjoea22m2cn2","_id":"ckvo10kj70001ymoegmlwb6fj"},{"post_id":"ckvoqukk3000043oeh0htc0xa","tag_id":"ckr31fffg0001wjoea22m2cn2","_id":"ckvoqukka000143oe3ma451ea"},{"post_id":"ckvp5hm0z0000a9oecjff2kwp","tag_id":"ckr31fffg0001wjoea22m2cn2","_id":"ckvp5hm140001a9oe2p6w5g1o"},{"post_id":"ckvp8ovv100003coeel4dgcv8","tag_id":"ckr31fffg0001wjoea22m2cn2","_id":"ckvp8ovv700013coe4g66hxfb"},{"post_id":"ckvsbawxx0000r5oe89koez8m","tag_id":"cktmo1qw300010zoecz0w6whc","_id":"ckvsbawy10001r5oedsfxc49i"},{"post_id":"ckxez0owb0000ltoehais4djc","tag_id":"cktmo1qw300010zoecz0w6whc","_id":"ckxez0owh0001ltoedd4rgkxm"},{"post_id":"ckyct2g0u0000n2oe3qsb4ise","tag_id":"ckyct2g0y0001n2oe89jg5way","_id":"ckyct2g150002n2oe77t09zyc"},{"post_id":"ckzk2pxlu0000i3oeas710l53","tag_id":"ckzk2pxlz0002i3oefadm1h15","_id":"ckzk2pxm50003i3oe1sms3wts"},{"post_id":"ckzk2pxly0001i3oe27tmerkw","tag_id":"ckr2bdm8v0000fgoeaffw5tof","_id":"ckzk3g2si0000ztoe75l4eu3a"},{"post_id":"cm12ffx9u0001kvoefk9dd7y0","tag_id":"ckr2bdm8v0000fgoeaffw5tof","_id":"cm12ffx9w0002kvoe4ne2canf"}],"Tag":[{"name":"A","_id":"ckr2ati96000065oed04xdzzk"},{"name":"NLP的一些收获","_id":"ckr2bdm8v0000fgoeaffw5tof"},{"name":"ACM","_id":"ckr31fffg0001wjoea22m2cn2"},{"name":"QT","_id":"ckravtm2x0002qwoefoe2ar9g"},{"name":"NLP","_id":"ckrh97fqh0001m8oe22lkcasf"},{"name":"数值计算","_id":"ckrh9lcch0001u3oe8r3z2ocr"},{"name":"CV","_id":"ckrltyy1s000135oe31ykgs2a"},{"name":"机器学习","_id":"cktmo1qw300010zoecz0w6whc"},{"name":"AI Course","_id":"ckvm4wqom00010hoehswzekf4"},{"name":"高性能计算","_id":"ckyct2g0y0001n2oe89jg5way"},{"name":"HPC","_id":"ckzk2pxlz0002i3oefadm1h15"}]}}